{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "12cb4c0a-e099-446b-9097-6e31656f66e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Dict, Any\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# sklearn\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc3c2d9-2d9c-4cfb-8609-8348c91a79b5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Hands-on Introduction to Deep Learning (Lecture 1)\n",
    "The **prereqs** are kept (on purpose) to a minimum, i.e.:\n",
    "- Basic knowledge of <font color=\"blue\"><b>derivatives and the chain rule</b></font>.\n",
    "- Ability to perform <font color=\"blue\"><b>simple matrix operations</b></font> (dot product, multiplication, transpose).\n",
    "- Knowledge of <a href=\"https://www.python.org\"><b>Python</b></a> and <a href=\"https://www.numpy.org\"><b>NumPy</b></a>.<br>\n",
    "  CHPC provides courses on these topics. You can find them at:<br>\n",
    "  + <a href=\"https://github.com/chpc-uofu/python-lectures\"><b>Introduction to Python</b></a>\n",
    "  + <a href=\"https://github.com/chpc-uofu/intro-numpy\"><b>Introduction to NumPy & SciPy</b></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4944a1-b79a-4744-8060-ea4c235b088a",
   "metadata": {},
   "source": [
    "# 1.Artificial Intelligence (AI)\n",
    "<font color=\"green\"><b>Artificial intelligence (AI)</b></font> is the ability of machines to perform tasks that<br>normally <font color=\"green\"><b>require human intelligence</b></font>.\n",
    "Among these tasks we have, e.g.:\n",
    "* discern hand-written digits, addresses,...\n",
    "* facial recognition in order to admit people access to buildings\n",
    "* write essays\n",
    "* translate from one language into another one.\n",
    "* convert audio into transcripts\n",
    "* self-driving vehicles\n",
    "* $\\ldots$\n",
    "\n",
    "Traditionally AI has been performed using <font color=\"green\"><b>different approaches</b></font>:\n",
    "* <a href=\"https://pdfs.semanticscholar.org/4fb2/7b22f57442ef8dddfd5eec2e8ef17b901323.pdf\">**Rules-Based AI (Expert systems)**</a>:\n",
    "  - Domain experts extract rules and codify them.\n",
    "  - Money & time consuming.\n",
    "  - Rather rigid in order to incorporate new knowledge.\n",
    "* <a href=\"https://www.amazon.com/Logic-Based-Artificial-Intelligence-International-Engineering/dp/0792372247\">**Logic based AI**</a>: The use of symbolic logic to perform reasoning\n",
    "* <a href=\"https://epubs.siam.org/doi/book/10.1137/1.9781611977882\">**Machine Learning (ML)**</a>:\n",
    "  - A data-centric approach\n",
    "  - Computer systems are able to \"learn\" from data without explicit instructions\n",
    "  - Central underpinnings of ML: probability theory, statistics and mathematical optimization.\n",
    "\n",
    "In the last decade, ML has become the research focus of AI, due to the following reasons:\n",
    "* The enormous explosion of data creation (Internet, audio, video, audio,..)<br>\n",
    "  <font color=\"orangered\"><b>Data has become the new currency</b></font>: ML requires huge amounts of data to train its models.\n",
    "* The increase of parallel computational power:<br>\n",
    "  - the <font color=\"green\"><b>Graphical Processing Units (GPUs)</b></font> were originally developed for gaming\n",
    "  - General-purpose GPUs (GPGPUs) emerged for tasks beyond visualization such as Scientific Computing and ML.\n",
    "     \n",
    "## Types of Machine Learning (ML):\n",
    "* <font color=\"green\"><b>Supervised Learning:</b></font>\n",
    "  - <font color=\"blue\"><b>input data and (labeled) output data</b></font> are used for training\n",
    "  - common techniques: regression, classification\n",
    "* <font color=\"green\"><b>Unsupervised Learning:</b></font>\n",
    "  - <font color=\"blue\"><b>input data</b></font> are used for training.\n",
    "  - high dimensional input data are used and reduced in dimensionality\n",
    "  - common techniques: clustering, PCA, ..\n",
    "* <font color=\"green\"><b>Reinforcement Learning:</b></font>\n",
    "  - concept borrowed from behavorial psychology\n",
    "  - positive actions are rewarded; negative actions are penalized.\n",
    "\n",
    "## Current status  \n",
    "- Currently, an ML model is trained for <font color=\"green\"><b>one specific task</b></font>.\n",
    "  * it may outperform a human specialist in one particular task (e.g. the interpretation of medical images).\n",
    "  * but <font color=\"green\"><b>NOT as versatile</b></font> as the human brain/intelligence.\n",
    "- The versatile AI equivalent of the human mind is <font color=\"green\"><b>AGI (Artificial General Intelligence)</b></font>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d882f1-44cd-4c07-88f4-bbe916a2b81f",
   "metadata": {},
   "source": [
    "# 2. The concept of a neuron/perceptron\n",
    "A <font color=\"green\"><b>neuron</b></font> or nerve cell:\n",
    "- fundamental unit of the nervous system\n",
    "- fires electrical signals across a <font color=\"green\"><b>neural network</b></font>.\n",
    "- contains a nucleus and mitochondria\n",
    "- has additional structure:\n",
    "  - <font color=\"green\"><b>dendrites</b></font>: receive the incoming electric signal (<font color=\"green\"><b>input</b></font>)\n",
    "  - <font color=\"green\"><b>axon</b></font>: transmits the electrical signal away from the nerve cell body (<font color=\"green\"><b>output</b></font>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab3dd94-d3c6-47a2-ac0c-4e7f921c5738",
   "metadata": {},
   "source": [
    "<img src=\"neuron.jpg\" alt=\"neuron\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ee3c71-ace2-4278-b5e3-d7f5ff3ead60",
   "metadata": {},
   "source": [
    "*Source: <a href=\"https://www.ninds.nih.gov/health-information/public-education/brain-basics/brain-basics-life-and-death-neuron\">*Brain Basics: The Life and Death of a Neuron*</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc2f6eb-08c8-4af2-8a09-83e29a6759ec",
   "metadata": {},
   "source": [
    "The concept of a physical neuron:\n",
    "- gave rise to the concept of perceptron (<a href=\"https://bpb-us-e2.wpmucdn.com/websites.umass.edu/dist/a/27637/files/2016/03/rosenblatt-1957.pdf\"><b>Rosenblatt, 1957</b></a>)\n",
    "  \n",
    "In essence, a <font color=\"green\"><b>perceptron</b></font> is a **non-linear function** $f$ \n",
    "(<font color=\"green\"><b>activation function</b></font>)<br>\n",
    "which receives <font color=\"green\"><b>input</b></font> and returns <font color=\"green\"><b>output</b></font>.\n",
    "\n",
    "Or a little more formal, $\\textbf{y}=f(\\textbf{x})$, where:\n",
    "* $f$: non-linear/activation function\n",
    "* $\\textbf{x} \\in \\mathbb{R}^{n_1 \\times 1}$: input vector i.e. $\\textbf{x}:=(x_1,x_2,\\ldots, x_{n_1})^T$.\n",
    "* $\\textbf{y} \\in \\mathbb{R}^{n_2 \\times 1}$: output vector i.e. $\\textbf{y}:=(y_1,y_2,\\ldots, y_{n_2})^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870c794f-0f7f-41ad-97c7-e78183d50381",
   "metadata": {},
   "source": [
    "In what follows we will perform \n",
    "<font color=\"green\"><b>logistic regression</b></font> (<font color=\"orangered\"><b>the most simple (shallow) neural net possible</b></font>) on a simple data set. <br>This simple toy model/example will allow us:\n",
    "* to display the <b>(essential) features</b> of deep learning.\n",
    "* to easily transition to the <b>general case</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1120457c-3455-4232-b073-88535a6353e5",
   "metadata": {},
   "source": [
    "# 3.Logistic Regression (as the most simple neural net!)\n",
    "## 3.1. Goal/Task\n",
    "* To train a <font color=\"green\"><b>binary classifier</b></font> based on a given training set\n",
    "  <br>using <font color=\"green\"><b>one neuron</b></font> (<font color=\"green\"><b>one unit</b></font>) in just <font color=\"green\"><b>one layer</b></font>, i.e.\n",
    "  (**shallow network**)\n",
    "* To obtain the <font color=\"green\"><b>accuracy</b></font> of the trained model using a test set.\n",
    "* To <font color=\"green\"><b>predict</b></font> the outcome of some data (provided) using the model.\n",
    "* To get familiar with some DL net frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6706610c-9c90-4154-b68e-7bee80962520",
   "metadata": {},
   "source": [
    "<img src=\"perceptron.jpeg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8d94e4-d922-4933-a9ae-219f57bf2263",
   "metadata": {},
   "source": [
    "<font><i>Logistic regression as a (shallow neural network):</i></font><br>\n",
    "<font><i>the activation function operates on the $n$ inputs to generate $1$ output.</i></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006a768b-d5d1-408c-adce-55b77bea76d0",
   "metadata": {},
   "source": [
    "## 3.2.Training of the binary classifier\n",
    "The <font color=\"green\"><b>training</b></font> of a binary classifier (and <font color=\"orangered\"><b>in extenso a deep neural net</b></font>) \n",
    "consists of the following steps:\n",
    "* **Initialize** the <font color=\"green\"><b>parameters</b></font> (<font color=\"green\"><b>weight vector and bias</b></font>)\n",
    "* Perform an **iterative loop** over the following 2 substeps:\n",
    "\n",
    "  + <font color=\"green\"><b>Forward propagation</b></font>:<br>\n",
    "    Given a training set, and a set of <font color=\"green\"><b>parameters</b></font> (weight vector and bias)\n",
    "    we calculate the associated cost function,<br>\n",
    "    which is a measure how different the predicted data are from the true data.\n",
    "\n",
    "  + <font color=\"green\"><b>Back propagation</b></font>:<br>\n",
    "    Based on the cost function we perform an <font color=\"green\"><b>update</b></font> on the set of<br> parameters (weight vector and bias)\n",
    "    in order to <font color=\"green\"><b>lower</b></font> the cost.\n",
    " \n",
    "\n",
    "The <font color=\"green\"><b>training set</b></font> consists of $m_{\\mathrm{train}}$ data points</b>:<br> $(\\mathbf{x}_i,y_i)$, $i \\in \\{1,\\ldots,m_{\\mathrm{train}}\\}$<br>\n",
    "  where:\n",
    "  - $\\mathbf{x_i}$ is a column vector of length of $n$, i.e. $\\mathbf{x_i} \\in \\mathbb{R}^{n \\times 1}$.<br>\n",
    "    Each dimension of the vector $\\mathbf{x_i}$ represents a <font color=\"green\"><b>feature</b></font> ($\\Rightarrow$ n features).\n",
    "  - $y_i$ is either 0 ($\\texttt{False}$) or 1 ($\\texttt{True}$), i.e. $y_i \\in \\mathbb{R}$.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08513926-900d-4719-80da-0c954db6ef78",
   "metadata": {},
   "source": [
    "### 3.2.1. Forward Propagation\n",
    "* At the perceptron (only $\\mathbf{1}$!), each data point $i$ ($i \\in \\{1,\\ldots,m_{\\mathrm{train}}\\}$) will be<br>\n",
    "  subjected to the following $2$ transformations:\n",
    "  1. $\\texttt{affine}$ transformation:<br>\n",
    "       $\\begin{eqnarray}\n",
    "         z_i & = &   \\mathbf{w^T}.\\mathbf{x_i} +b \\;\\;,\\;\\; \\mathbf{x_i} \\in \\mathbb{R}^{n\\times 1} \\nonumber \\\\\n",
    "            &=  &  \\displaystyle{\\sum_{j=1}^n w_j x_{ji} +b} \\nonumber\n",
    "       \\end{eqnarray}$\n",
    "    \n",
    "     where:<br>\n",
    "     - $ \\mathbf{w}$ : <font color=\"green\"><b>weight</b></font> vector ($\\in \\mathbb{R}^{n \\times 1}$)<br>\n",
    "       Note: the weight vector has <font color=\"orangered\"><b>same number of dimensions as there are features</b></font> .\n",
    "    \n",
    "     - $ b$ : <font color=\"green\"><b>bias</b></font> (scalar $\\Rightarrow \\in \\mathbb{R}$).\n",
    "\n",
    "     - For each data point: the <font color=\"orangered\"><b> weight vector and bias used at this one perceptron are the same</b></font>.   \n",
    "  2. subsequent $\\texttt{non linear activation}$:<br>\n",
    "     $a_i =  \\sigma(z_i)$ , $a_i \\in \\mathbb{R}$<br>\n",
    "     where:<br>\n",
    "     - $\\sigma$ is known as the <font color=\"green\"><b>sigmoid</b></font> function.\n",
    "   \n",
    "     - $\\begin{equation}\n",
    "        \\sigma(z) = \\displaystyle \\frac{1}{1+e^{-z}} \n",
    "       \\end{equation}$\n",
    "    \n",
    "     - The activation of the <b>last layer</b> (in this case we only have one layer) is the same<br>\n",
    "       as the predicted value ($\\widehat{y_i})$. Thus, <br>\n",
    "    \n",
    "       $\\begin{equation}\n",
    "       \\widehat{y_i} := a_i \\nonumber\n",
    "       \\end{equation}$\n",
    "   \n",
    "* Calculate the <font color=\"green\"><b>cost function</b></font> ($\\mathcal{C}$).<br>\n",
    "  The cost function $\\mathcal{C}$ is defined as the mean of the <font color=\"green\"><b>loss functions</b></font> ($\\mathcal{L}^{(i)}$) over the $m_{\\mathrm{train}}$ data points:\n",
    "\n",
    "  $\\begin{eqnarray}\n",
    "       \\mathcal{C}       & := & \\displaystyle \\frac{1}{m_{\\mathrm{train}}} \\sum_{i=1}^{m_{\\mathrm{train}}}        \\mathcal{L}^{(i)} \n",
    "  \\end{eqnarray}$\n",
    "\n",
    "  In case of **binary classification**, the **loss function** $\\mathcal{L}^{(i)}$ for data point $i$ is given by:\n",
    "  \n",
    "  $\\begin{eqnarray}\n",
    "       \\mathcal{L}^{(i)} & = & - \\bigg [ y_i \\log(\\widehat{y_i}) + (1-y_i)\\log(1-\\widehat{y_i}) \\bigg ] \\\\\n",
    "                         & = & - \\bigg [ y_i \\log(a_i) + (1-y_i)\\log(1-a_i) \\bigg ] \\\\\n",
    "  \\end{eqnarray}$\n",
    "\n",
    "* <font color=\"red\"><b>Note on the determination of loss functions </b></font>\n",
    "  - The <font color=\"green\"><b>loss function</b></font> for a particular problem can be derived in different ways.<br>\n",
    "    The most common approach is through the\n",
    "    use of the <font color=\"green\"><b>Maximum Likelihood Estimator (MLE)</b></font>.<br>\n",
    "    Another way is through the <font color=\"green\"><b>Kullback-Leibler divergence</b></font><br> which is a measure of the difference between $2$ probability distributions.<br>\n",
    "    A third approach is via <font color=\"green\"><b>Information Theory</b></font>.<br>\n",
    "    Note that the aforementioned methods are all related.\n",
    "  - A function is <font color=\"green\"><b>convex</b></font> when a line connecting two points on its graph lies above the graph on on the graph itself.<br>\n",
    "    * Convexity induces some nice properties: the local minimum is the **GLOBAL** minimum.\n",
    "    * The loss function in logistic regression is convex.<br>Loss functions in deep learning are generally NOT convex.\n",
    "    \n",
    "\n",
    "* <font color=\"red\"><b>Computational Note:</b></font><br>\n",
    "  The vectorization feature in NumPy should be exploited.<br>\n",
    "  Instead of performing the $\\texttt{affine transformation}$ and $\\texttt{activation}$ on **one** element<br>\n",
    "  at the time, perform the operation on an $\\texttt{ndarray}$ of elements.\n",
    "\n",
    "  Provided that:\n",
    "  + $\\mathbf{x_i}$ is a column vector of length of $n$, i.e. $\\mathbf{x_i} \\in \\mathbb{R}^{n \\times 1}$.\n",
    "  + $y_i$ is either 0 ($\\texttt{False}$) or 1 ($\\texttt{True}$), i.e. $y_i \\in \\mathbb{R}$.\n",
    "  + The $\\mathrm{m}$ $\\mathbf{x_i}$ column vectors can be collected in the $n \\times m$ matrix $X$,<br>\n",
    "    given by:\n",
    "\n",
    "    $\\begin{eqnarray}\n",
    "         X & := & \\begin{pmatrix}\n",
    "                                \\vdots & \\cdots  &\\vdots \\\\\n",
    "                                 \\mathbf{x_1}   & \\vdots  & \\mathbf{x_{m}}  \\\\\n",
    "                                 \\vdots & \\cdots & \\vdots \n",
    "                 \\end{pmatrix}\n",
    "     \\end{eqnarray}$\n",
    "\n",
    "      \n",
    "  + The $\\mathrm{m}$ $y_i$ values can be collected in the row vector $Y$,<br>\n",
    "    given by:\n",
    "\n",
    "    $\\begin{eqnarray}\n",
    "       Y & := & \\begin{pmatrix} y_1 & y_2 & \\cdots & y_{\\mathrm{m}-1} & y_{\\mathrm{m}} \n",
    "                 \\end{pmatrix}\n",
    "      \\end{eqnarray}$\n",
    "\n",
    "  + Thus:\n",
    "    - $\\begin{eqnarray}\n",
    "         \\mathbf{Z} & = &   \\mathbf{w^T}.\\mathbf{X} + \\mathbf{b}  \\nonumber \n",
    "       \\end{eqnarray}$\n",
    "\n",
    "    - $\\begin{eqnarray}\n",
    "          \\mathbf{A} & = & \\sigma(\\mathbf{Z}) \\nonumber\n",
    "      \\end{eqnarray}$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b172780-7a9d-453c-afb5-d2ad93d5fecc",
   "metadata": {},
   "source": [
    "#### Some notes on the sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d2f438-4d4b-4c76-99d6-4c21cd0c6575",
   "metadata": {},
   "source": [
    "The sigmoid function $\\sigma(z)$ has the following properties:\n",
    "\n",
    "* $\n",
    "   \\begin{equation}\n",
    "  \\lim_{ z \\to -\\infty} \\sigma(z) = 0\n",
    "  \\end{equation}$\n",
    "\n",
    "* $\\begin{equation}\n",
    "   \\lim_{ z \\to +\\infty} \\sigma(z) = 1\n",
    "  \\end{equation}$\n",
    "\n",
    "* $\\begin{equation}\n",
    "   \\sigma(0) = \\frac{1}{2}\n",
    "  \\end{equation}$\n",
    "\n",
    "* $\\begin{equation}\n",
    "  \\displaystyle \\frac{d \\sigma(z)}{dz} = \\sigma(z)(1-\\sigma(z))\n",
    "  \\end{equation}$\n",
    "\n",
    "The range of the sigmoid is $[0,1]$ and can thus be interpreted as a <font color=\"green\"><b>probability</b></font>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb4aa30-ee02-4bda-9616-e22d1a0016b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-7.5, 7.5, 1501)\n",
    "y = 1.0/(1.0+np.exp(-x))\n",
    "plt.title(r\"Sigmoid function and decision boundary\")\n",
    "plt.xlabel(r\"$x$\")\n",
    "plt.ylabel(r\"$\\sigma(x)$\",rotation=0)\n",
    "plt.plot(x,y, label =r\"$\\sigma(x)$\" )\n",
    "plt.axvline(x=0,color='r',ymin=0.0, ymax=1.0, label=\"Decision boundary\")\n",
    "plt.plot(0.0,0.5,marker='o',color='orange')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b55643-d66f-4291-a423-2bf587c81955",
   "metadata": {},
   "source": [
    "#### Exercise 1: Tools for the forward propagation\n",
    "<font color=\"blue\"><b>Implement the following **3** functions:</b></color>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4fde55-d6b7-437e-bd9f-562754c323eb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "##### **Exercise 1.1**:\n",
    "`def init_param(n: int) -> Tuple[np.ndarray, float]:`<br>\n",
    "  * The <font color=\"green\"><b>weight</b></font> vector is a **column vector** having the <font color=\"red\"><b>(n,1)</b></font> shape and of type float.\n",
    "  * The <font color=\"green\"><b>bias</b></font> is a **scalar** of type float.\n",
    "  * All elements of the weight vector and bias can be initialized to $0.0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3019ca60-378d-4139-984e-68dd9a7ba0f9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Exercise 1.1:\n",
    "def init_param(n: int) -> Tuple[np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    Initialize the parameters (weight, bias) for the Binary Classifier.\n",
    "    \n",
    "    Args:\n",
    "        n (int): The number of features.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[np.ndarray, float]: A tuple containing the initialized weight (column vector)\n",
    "                                 & the initialized bias (scalar).\n",
    "    \"\"\"\n",
    "    # Here comes YOUR code to initialize the weight vector & bias.\n",
    "    # W =  <--- YOUR CODE: Weight (vector) to zero\n",
    "    # b =  <--- YOUR CODE: bias (float) to zero\n",
    "    return W,b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e637bb-9317-47b8-857c-0c26defa0b6e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "##### <font color=\"blue\"><b>Simple check of the function:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a69b67-0f57-4c19-a218-4c328d058d64",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %load tests/lec1/test_ex1.1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d186e1b2-eb5a-44be-b59c-359d4c37230d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Solution::\n",
    "# %load solutions/lec1/sol_ex1.1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7550844-d026-4f26-9ead-63dc01ac9904",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "##### **Exercise 1.2**\n",
    "`def sigmoid(Z: np.ndarray) -> np.ndarray:`<br>\n",
    "  * Perform $\\sigma(\\mathbf{Z})$ element-wise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa9cb4b-158c-424d-9bdc-522a784a3da0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Exercise 1.2:\n",
    "def sigmoid(Z: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute the sigmoid function.\n",
    "    \n",
    "    Args:\n",
    "        Z (np.ndarray): The input value(s).\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: The sigmoid of the input value(s).\n",
    "    \"\"\"\n",
    "    # Here comes YOUR code for the sigmoid function.\n",
    "    # return <-- YOUR CODE "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd4868f-04df-468b-bde5-7243b22e07e4",
   "metadata": {},
   "source": [
    "##### <font color=\"blue\"><b>Simple check of the function:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab8e146-5e4e-4f59-b490-ccab46c15a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tests/lec1/test_ex1.2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f91f00-29c3-42ad-b799-23d65eb5c91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution (in case of need)::\n",
    "# %load solutions/lec1/sol_ex1.2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9496359b-64b0-49ab-8dd3-57b44bdbbf39",
   "metadata": {},
   "source": [
    "##### **Exercise 1.3**\n",
    "`def forward(X: np.ndarray,Y:np.ndarray,`<br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`W: np.ndarray, b: float) ->`<br> \n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`Tuple[np.ndarray, float]:`<br>\n",
    " *  $ \\mathbf{Z} = \\mathbf{w^T}.\\mathbf{X} + \\mathbf{b} $\n",
    " *  $ \\mathbf{A} = \\sigma(\\mathbf{Z})$  ($\\texttt{Ex 1.2.}$)\n",
    " *  $ \\mathcal{C} = - \\frac{1}{m} \\displaystyle \\sum_{i=1}^m \\bigg [ y_i \\log(a_i) + (1-y_i)\\log(1-a_i) \\bigg ]$ <br>\n",
    "    The latter equation can be easily vectorized in NumPy using $\\mathbf{A}$ and $\\mathbf{Y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce49e55-b6fd-4530-804c-181700de11af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1.3:\n",
    "def forward(X: np.ndarray, Y: np.ndarray,\n",
    "            W: np.ndarray, b: float) -> Tuple[np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    Perform the forward pass of the binary classifier.\n",
    "    \n",
    "    Args:\n",
    "        X (np.ndarray): The training data (features)  -> shape(n, m) \n",
    "                        where n is #features & m is #samples.\n",
    "        Y (np.ndarray): The training labels (targets) -> shape(1, m)\n",
    "                        where m is #samples.\n",
    "        W (np.ndarray): The weight vector             -> shape(n,1 ).\n",
    "        b (float)     : The bias term                 -> float\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[np.ndarray, float]: \n",
    "          A tuple containing the activation matrix and the cost.\n",
    "    \"\"\"\n",
    "    # Here comes YOUR code for the forward propagation.\n",
    "    num_samples = X.shape[1]\n",
    "    # Z =     <-- YOUR CODE\n",
    "    # A =     <-- YOUR CODE\n",
    "    # cost =  <-- YOUR CODE\n",
    "    return A, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f423e1-4c64-4ed4-b274-bf0de1d6864e",
   "metadata": {},
   "source": [
    "##### <font color=\"blue\"><b>Simple check of the function:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed68248-3bce-4d92-97ad-366eaefd7c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tests/lec1/test_ex1.3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6023e400-5d3e-4ed0-87fa-d4cb6b9774c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution (in case of need)::\n",
    "# %load solutions/lec1/sol_ex1.3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697fd5b7-8d51-402b-8340-9f6688f6053a",
   "metadata": {},
   "source": [
    "### 3.2.2. Back Propagation\n",
    "Our ultimate goal (during the training phase of our model) is to find the <font color=\"green\"><b>optimal values</b></font><br> for the weight vector ($\\widehat{w}$), and the bias ($\\widehat{b}$).<br>\n",
    "  Thus:<br>\n",
    "\n",
    "  $\\begin{equation}\n",
    "     \\widehat{w}, \\widehat{b} = \\underset{w,b}{\\operatorname{argmin}} \\mathcal{C}\n",
    "  \\end{equation}$\n",
    "\n",
    "This goal can be reached iteratively:\n",
    "1. by calculating the <font color=\"green\"><b>direction/gradient</b></font> w.r.t weights and bias which leads to a minimum of the cost function $\\mathcal{C}$).\n",
    "2. by subsequently <font color=\"green\"><b>updating</b></font> the values of the weight vector and the bias along the aforementioned direction.\n",
    "\n",
    "1.Calculate the **gradient of cost function** w.r.t the weights and the bias:\n",
    "  \n",
    "  + $\\begin{eqnarray}\n",
    "      \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial a_i} & = & \\frac{\\partial}{\\partial a_i} \\bigg [ - \\big [ y_i \\log(a_i) + (1-y_i)\\log(1-a_i) \\big ] \\bigg ]\\\\\n",
    "                                                      & = & -\\frac{y_i}{a_i} + \\frac{(1-y_i)}{(1-a_i)}\n",
    "    \\end{eqnarray}$\n",
    " \n",
    "    \n",
    "  + $\\begin{eqnarray}\n",
    "        \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial z_i} & = & \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial a_i} \\frac{\\partial a_i}{\\partial z_i} \\\\\n",
    "         & = & \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial a_i} \\frac{\\partial \\sigma(z_i)}{\\partial z_i} \\\\\n",
    "         & =& a_i - y_i\n",
    "    \\end{eqnarray}$\n",
    " \n",
    "    \n",
    "  + $\\begin{eqnarray}\n",
    "       \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial b} & = & \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial a_i} \\frac{\\partial a_i}{\\partial z_i}\\frac{\\partial z_i}{\\partial b} \\\\\n",
    "                 & = & a_i - y_i\n",
    "    \\end{eqnarray}$\n",
    "  + $\\begin{eqnarray}\n",
    "      \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial w_j} & = & \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial a_i} \\frac{\\partial a_i}{\\partial z_i}\\frac{\\partial z_i}{\\partial w_j} \\\\\n",
    "                 & = & (a_i - y_i) x_{ji}\n",
    "    \\end{eqnarray}$\n",
    "    \n",
    "  Thus, <br>\n",
    "\n",
    "  + $\\begin{eqnarray}\n",
    "     \\frac{\\partial\\mathcal{C}}{\\partial b}  & =   &= & \\frac{1}{m_{\\mathrm{train}}}\\displaystyle \\sum_{i=1}^{m_{\\mathrm{train}}} \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial b}  \n",
    "                                             & = & \\frac{1}{m_{\\mathrm{train}}} \\displaystyle \\sum_{i=1}^{m_{\\mathrm{train}}} (a_i - y_i) \n",
    "     \\end{eqnarray}$\n",
    "\n",
    "  + $\\begin{eqnarray}\n",
    "     \\frac{\\partial\\mathcal{C}}{\\partial w_j}  &= & \\frac{1}{m_{\\mathrm{train}}}\\displaystyle \\sum_{i=1}^{m_{\\mathrm{train}}} \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial b} \n",
    "      & = & \\frac{1}{m_{\\mathrm{train}}} \\displaystyle \\sum_{i=1}^{m_{\\mathrm{train}}} (a_i - y_i) x_{ji} \\;\\;,\\;\\;\\forall \\, j \\in \\{1,\\ldots,n\\}\n",
    "     \\end{eqnarray}$\n",
    "\n",
    "2.**Update** the parameters (using <a href=\"https://en.wikipedia.org/wiki/Gradient_descent\">gradient descent</a>): \n",
    "\n",
    "  $\\begin{eqnarray}\n",
    "      b & = & b - \\alpha  \\frac{\\partial\\mathcal{C}}{\\partial b} \\\\\n",
    "      w_j & = & w_j - \\alpha  \\frac{\\partial\\mathcal{C}}{\\partial w_j} \\;\\;,\\;\\;\\forall \\, j \\in \\{1,\\ldots,n\\}\n",
    "    \\end{eqnarray}$\n",
    "\n",
    "  where $\\alpha$ is known as the <font color=\"green\"><b>learning rate </b></font> or the <font color=\"green\"><b>step size</b></font> (gradient descent).  \n",
    "\n",
    "<font color=\"red\"><b>Computational Note:</b></font><br>\n",
    "  The vectorization feature in NumPy should be exploited.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f151e433-1997-4608-898c-efa13f9f36b6",
   "metadata": {},
   "source": [
    "#### Exercise 2: Tools for the Back propagation\n",
    "<font color=\"blue\"><b>Implement the following functions:</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d3f635-e9b4-4c62-925a-e9b44e5065b0",
   "metadata": {},
   "source": [
    "##### **Exercise 2.1**\n",
    "`def calcgrad(X_train:np.ndarray, Y_train:np.ndarray, A:np.ndarray) ->\\`<br>\n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`Tuple[np.ndarray, float]:`\n",
    "\n",
    "These are the steps:  \n",
    "* $\\mathbf{dZ} = \\mathbf{A} - \\mathbf{Y}$\n",
    "* $\\mathbf{dW} = \\frac{1}{m} \\mathbf{X}.\\mathbf{dZ}^T$\n",
    "* $db = \\frac{1}{m} \\displaystyle \\sum_{i=1}^m \\mathbf{dZ}_i$\n",
    "* return $\\mathbf{dW},db$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a8ea4b-46d7-4055-9f26-2c524262c774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2.1.:\n",
    "def calcgrad(X: np.ndarray, Y: np.ndarray, \n",
    "             A:np.ndarray) ->  Tuple[np.ndarray, float]:\n",
    "    \"\"\"        \n",
    "    Computes the gradients of the cost function with respect to W and b.\n",
    "    Arg:\n",
    "        X (np.ndarray): Training data -> shape(n,m)\n",
    "        Y (np.ndarray): Training labels -> shape(1,m)\n",
    "        A (np.ndarray): Activation matrix -> shape(1,m) \n",
    "    Return:\n",
    "        A tuple containing the gradients with respect to W and b.\n",
    "    \"\"\" \n",
    "    # Here comes the calcgrad code\n",
    "    num_samples = X.shape[1]\n",
    "    # dZ = <--- YOUR CODE\n",
    "    # dW = <--- YOUR CODE\n",
    "    # db = <--- YOUR CODE\n",
    "    return dW,db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fde3186-39db-4c8a-9405-fcaf6b7a51da",
   "metadata": {},
   "source": [
    "##### <font color=\"blue\"><b>Simple check of the function:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773fbe03-df29-4674-87db-96f0c8d41b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tests/lec1/test_ex2.1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956dd7bb-ada5-4b77-94ce-7602c3fd1a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution (in case of need)::\n",
    "# %load solutions/lec1/sol_ex2.1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af79d3c-3038-4906-a240-9cb59eb9f8e2",
   "metadata": {},
   "source": [
    "##### **Exercise 2.2**\n",
    "`def update(Weights: np.ndarray, bias: float,\\`<br>\n",
    "   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`dWeight: np.ndarray, dbias:float, lr:float) -> Tuple[np.ndarray, float]:`<br>\n",
    "Here are the steps:\n",
    "* $\\mathbf{W} = \\mathbf{W} - \\alpha \\,\\mathbf{dW}$\n",
    "* $b = b - \\alpha \\,db$\n",
    "* return $\\mathbf{W},b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9f5310-6b88-42dc-ac32-ddbe97b897e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2.2.:\n",
    "def update(Weights: np.ndarray, bias: float,\n",
    "           dWeight: np.ndarray, dbias:float,\n",
    "           lr:float) -> Tuple[np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    Update the parameters using the gradients and learning rate.    \n",
    "\n",
    "    Args:\n",
    "        Weights (np.ndarray): The weight vector             -> shape(n, 1).\n",
    "        bias (float)        : The bias term                 -> float\n",
    "        dWeight (np.ndarray): The gradient of the cost w.r.t. W -> shape(n, 1).\n",
    "        dbias (float)       : The gradient of the cost w.r.t. b -> float\n",
    "        lr (float)          : The learning rate.    \n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, float]: \n",
    "          A tuple containing the updated weight and bias.\n",
    "    \"\"\"\n",
    "    # Here comes the code to update the weight vector and the bias.\n",
    "    # Weights = <--- YOUR CODE\n",
    "    # bias =    <--- YOUR CODE\n",
    "    return Weights, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d67b95b-6152-45f5-b44d-ca8ef74f9d9a",
   "metadata": {},
   "source": [
    "##### <font color=\"blue\"><b>Simple check of the function:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3f8655-0587-4aca-a0fd-208bac7c06d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tests/lec1/test_ex2.2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9427be-af25-4752-89eb-560e297cd72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution (in case of need)::\n",
    "# %load solutions/lec1/sol_ex2.2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952d38ec-55ef-40e7-9ace-57df9fe6efa2",
   "metadata": {},
   "source": [
    "#### Exercise 3: Training (complete)\n",
    "<font color=\"blue\"><b>Implement the following function:</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f58e61f-2afb-4eaf-8dbb-5d91c2f7520b",
   "metadata": {},
   "source": [
    "`def train_model(X: nd.array, Y: nd.array, \\`<br>\n",
    "     &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`num_epochs: int, lr: float) -> Tuple[List[float],np.ndarray, float]:`\n",
    "Here are steps:\n",
    "* initialize $\\mathbf{W},b$ to 0.0  \n",
    "* perform a loop/iteration:\n",
    "  - Calculate $\\mathbf{A}, \\mathcal{C}$ (cost) using the forward function\n",
    "  - Calculate the gradients $\\mathbf{dW}, db$\n",
    "  - Update $\\mathbf{W}, b$ using the gradients $\\mathbf{dW}, db$\n",
    "* return lst(cost), $\\mathbf{W},b$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc6c97f-41d4-4cbc-87d7-1f0f710d3475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.1.\n",
    "def train_model(X: np.ndarray, Y: np.ndarray,\n",
    "                num_epochs: int, lr: float) -> Tuple[List[float],np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    Train the binary classifier using gradient descent.\n",
    "    \n",
    "    Args:\n",
    "        X (np.ndarray): The training data (features)  -> shape(n, m) \n",
    "                        where n is #features & m is #samples.\n",
    "        Y (np.ndarray): The training labels (targets) -> shape(1, m)\n",
    "                        where m is #samples.\n",
    "        num_epochs (int): The number of epochs to train.\n",
    "        lr (float)    : The learning rate.    \n",
    "    \n",
    "    Returns:\n",
    "        Tuple[np.ndarray, float]: \n",
    "          A tuple containing the final weight and bias after training.\n",
    "    \"\"\"\n",
    "    # Here comes YOUR code to train the model.\n",
    "    lstCost = []\n",
    "    # W,b =         <--- YOUR CODE : Initialize to 0.0 using previous function\n",
    "    for i in range(num_epochs):\n",
    "        # A, cost = <--- YOUR CODE : Use the forward function\n",
    "        lstCost.append(cost)\n",
    "        # dW, db =  <--- YOUR CODE : Calc. the gradient\n",
    "        # W, b   =  <--- YOUR CODE : Perform the update\n",
    "    return lstCost, W, b    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2737be94-aff0-4944-a03a-6b14d7e1183b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution (in case of need)::\n",
    "# %load solutions/lec1/sol_ex3.1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9450608b-7af0-4d8e-89da-54b3170b8bc8",
   "metadata": {},
   "source": [
    "## 4. Testing of the binary classifier \n",
    "Once our neural net has been trained, i.e. the <font color=\"green\"><b>optimal values</b></font><br> \n",
    "for the parameters $\\mathbf{w}$ and $b$ (i.e. $\\mathbf{\\widehat{w}}$ and $\\widehat{b}$) are known. <br> \n",
    "We can now test our neural net model using the <font color=\"green\"><b>test data set</b></font>.\n",
    "\n",
    "* Apply the <font color=\"green\"><b> predict </b></font> function the **test data set**.<br>\n",
    "  The **predict** function has $2$ components:\n",
    "  + Apply the **forward** propagation to the test data set but use $\\mathbf{\\widehat{w}}$ and $\\widehat{b}$.\n",
    "  + Map the obtained activation to either $0$ or $1$\n",
    "* Calculate the number of correct predictions vs. total predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eea731e-a660-46a0-b498-4d6fcc99b53a",
   "metadata": {},
   "source": [
    "### 4.1.Prediction function\n",
    "* The <font color=\"green\"><b>test set</b></font> consists of $m_{\\mathrm{test}}$ test data points</b>:<br> $(\\mathbf{x}_i,y_i)$, $i \\in \\{1,\\ldots,m_{\\mathrm{test}}\\}$<br>\n",
    "  where:\n",
    "  - $\\mathbf{x_i}$ is a column vector of length of $n$, i.e. $\\mathbf{x_i} \\in \\mathbb{R}^{n \\times 1}$.\n",
    "  - $y_i$ is either 0 ($\\texttt{False}$) or 1 ($\\texttt{True}$), i.e. $y_i \\in \\mathbb{R}$.\n",
    "  - The $m_{\\mathrm{test}}$ $\\mathbf{x_i}$ column vectors can be collected in the matrix $X$,<br>\n",
    "    given by:\n",
    "\n",
    "    $\\begin{eqnarray}\n",
    "         X & := & \\begin{pmatrix}\n",
    "                                \\vdots & \\cdots  &\\vdots \\\\\n",
    "                                 \\mathbf{x_1}   & \\vdots  & \\mathbf{x_{\\mathrm{m_{test}}}}  \\\\\n",
    "                                 \\vdots & \\cdots & \\vdots \n",
    "                 \\end{pmatrix}\n",
    "     \\end{eqnarray}$\n",
    "  - The $m_{\\mathrm{test}}$ $y_i$ values can be collected in the $2D$ row vector $Y$,<br>\n",
    "    given by:\n",
    "\n",
    "    $\\begin{eqnarray}\n",
    "         Y & := & \\begin{pmatrix} y_1 & y_2 & \\cdots & y_{\\mathrm{m_{test}-1}} & y_{\\mathrm{m_{test}}} \n",
    "                 \\end{pmatrix}\n",
    "     \\end{eqnarray}$  \n",
    "    \n",
    "* Apply <font color=\"green\"><b>forward propagation</b></font> in (matrix) form (efficiency reasons):\n",
    "  - $\\begin{eqnarray}\n",
    "        \\mathbf{z^T} & = & \\mathbf{\\widehat{w}^T}. \\mathbf{X} + \\mathbf{\\widehat{b}^T} \n",
    "     \\end{eqnarray}$\n",
    "     where:\n",
    "     + $\\mathbf{z^T}$ is a row vector of length $m_{\\mathrm{test}}$\n",
    "     + $\\mathbf{\\widehat{w}^T}$ is a row vector (containing $n$ el.) with the optimal weights (obtained from the training)\n",
    "     + $\\mathbf{X}$ is $n \\times m_{\\mathrm{test}}$ matrix.\n",
    "     + $\\mathbf{\\widehat{b}^T}$ is a row vector of length $m_{\\mathrm{test}}$ (each element is $\\widehat{b}$).\n",
    "  - $\\begin{eqnarray}\n",
    "        \\mathbf{a} & = & \\sigma(\\mathbf{{z}}) \n",
    "     \\end{eqnarray}$\n",
    "    where $\\sigma$ operates element-wise on the row-vector $\\mathbf{z}$<br>\n",
    "    to generate the row vector $\\mathbf{a}$.\n",
    "* The elements that are calculated ($\\widehat{\\mathbf{y}}:=\\mathbf{a}$) are in the interval $[0,1]$.<br>\n",
    "  - In order to compare them with the <font color=\"green\"><b>test labels</b></font>, we must <font color=\"green\"><b>map</b></font> the elements of $\\mathbf{a}$ into $\\{0,1\\}$.\n",
    "  - $\\widetilde{y_i} = F(\\widehat{\\mathbf{y_i}})$ where\n",
    "    $F(\\widehat{\\mathbf{y_i}})$ is defined in the following way:<br>\n",
    "     $\\begin{equation}\n",
    "      \\widetilde{y_i}= \n",
    "\\begin{cases}\n",
    "    1,& \\text{if } a_i \\geq 0.5\\\\\n",
    "    0,              & \\text{otherwise}\n",
    "\\end{cases}\n",
    "     \\end{equation}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dcefeb-2a74-4f91-8e1f-3bdddfbe9b14",
   "metadata": {},
   "source": [
    "#### Exercise 4: prediction function + accuracy\n",
    "<font color=\"blue\"><b>Implement the following functions:</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604a83b3-a03c-44c3-8f40-852fc2f0d51e",
   "metadata": {},
   "source": [
    "##### **Exercise 4.1:**\n",
    "`def predict_labels(X: np.ndarray, Weights: np.ndarray,`<br>\n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`bias:float) -> np.ndarray:`<br>\n",
    "Steps:\n",
    "* Calculate $\\mathbf{A}$ using $\\mathbf{W}$ and $b$ (obtained from training)\n",
    "* Map all the elements of A to either $1$ or $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140dc38f-cb02-414d-bdb5-d43cd94b2136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.1:\n",
    "def predict_labels(X: np.ndarray, W: np.ndarray, b: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Make predictions using the trained model.\n",
    "    \n",
    "    Args:\n",
    "        X (np.ndarray): The input data (features)  -> shape(n, m) \n",
    "                        where n is #features & m is #samples.\n",
    "        W (np.ndarray): The weight vector             -> shape(n, 1).\n",
    "        b (float)     : The bias term                 -> float\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: The predicted labels (0 or 1).\n",
    "    \"\"\"\n",
    "    # Here comes the code to predict the labels (either 0 or 1\n",
    "    # A = <--- YOUR CODE \n",
    "    return np.where(A >= 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd683a7f-7393-4eeb-895b-fcd6395690f5",
   "metadata": {},
   "source": [
    "##### <font color=\"blue\"><b>Simple check of the function:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb0cb1a-f2ff-4f38-a39a-a6e97ff504c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tests/lec1/test_ex4.1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f66b393-11d4-487e-8336-35dc4feadbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution (in case of need)::\n",
    "# %load solutions/lec1/sol_ex4.1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5366eb4e-98bb-4545-9fba-220faa8ef82e",
   "metadata": {},
   "source": [
    "##### **Exercise 4.2:**\n",
    "`def accuracy(Y_true: np.ndarray,`<br>\n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`Y_pred: np.ndarray) -> float<br>\n",
    "Step:\n",
    "* return ratio (number matches/total number) * 100. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a227c11c-be2f-493f-9fd2-bf17d105bc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.2:\n",
    "def accuracy(Y_true: np.ndarray, Y_pred: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the accuracy of the predictions.\n",
    "    \n",
    "    Args:\n",
    "        Y_true (np.ndarray): The true labels.\n",
    "        Y_pred (np.ndarray): The predicted labels.\n",
    "    \n",
    "    Returns:\n",
    "        float: The accuracy as a percentage.\n",
    "    \"\"\"\n",
    "    # return <--- YOUR CODE: Here comes the ratio * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f1257c-4066-4875-b477-742aa549a4e0",
   "metadata": {},
   "source": [
    "##### <font color=\"blue\"><b>Simple check of the function:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a1a1cf-f345-4387-a5c3-8fb16b3331a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tests/lec1/test_ex4.2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61924d2-6e3a-4a62-be53-628620518209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution (in case of need)::\n",
    "# %load solutions/lec1/sol_ex4.2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2549172-cdb5-4ea7-b3d1-fc2d71d0a394",
   "metadata": {},
   "source": [
    "## 5. Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b06f9eb-1298-405b-a8fa-3d780901b2c5",
   "metadata": {},
   "source": [
    "### 5.1.Data Generation\n",
    "* The python library <a href=\"https://scikit-learn.org/stable/\"><b>scikit-learn</b></a> (based on NumPy & SciPy) is used to generate a synthetic data set.\n",
    "* In order to facilitate the visualization we will only choose 2 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385310a0-281b-4853-9cba-2ff4379d5b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to generate a data set\n",
    "X, y = make_moons(n_samples=500, noise=0.25, random_state=42)\n",
    "print(f\"  X.shape:{X.shape}\")\n",
    "print(f\"  y.shape:{y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9691241-f75f-4180-8d72-da6203f56632",
   "metadata": {},
   "source": [
    "### 5.2.Splitting the data.\n",
    "The data set will be split (using scikit-learn) into:\n",
    "- <font color=\"green\"><b>training set</b></font> \n",
    "- <font color=\"green\"><b>test set</b></font>\n",
    "\n",
    "<font color=\"orangered\"><b>Note:</b></font>\n",
    "* Normally, we will also create a <font color=\"green\"><b>dev/validation</b></font> set.\n",
    "* We only $1$ have one <font color=\"green\"><b>hyperparameter</b></font>, i.e. $\\alpha$ (learning rate)\n",
    "* The cost function ($\\mathcal{C}$) is <font color=\"green\"><b>convex</b></font>.<br>\n",
    "  So, our validation set is in essence the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dc3e22-caa8-4e60-9dea-a18a0448c6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to split the data in training and a test set.\n",
    "test_ratio = 0.30\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_ratio, random_state=42)\n",
    "print(f\"Splitting the data set ...\")\n",
    "print(f\"  Test ratio:{test_ratio}\")\n",
    "print(f\"  Training Data Set:\")\n",
    "print(f\"    X_train.shape :: {X_train.shape}\")\n",
    "print(f\"    y_train.shape :: {y_train.shape}\")\n",
    "print(f\"  Test Data Set:\")\n",
    "print(f\"    X_test.shape  :: {X_test.shape}\")\n",
    "print(f\"    y_test.shape  :: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4dfcc3-124c-4fb5-a1fc-a3296b383e1a",
   "metadata": {},
   "source": [
    "### 5.3. Visualization of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2059038-8d9f-4c47-b33e-21bc35d24d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the training set\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='RdBu')  \n",
    "plt.title(\"Synthetic (Training) Data (2 Features)\")\n",
    "plt.xlabel(r\"$x_1$\")\n",
    "plt.ylabel(r\"$x_2$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fde4bf0-078d-4352-9beb-c622fe27fe3e",
   "metadata": {},
   "source": [
    "### 5.4. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896a9615-a9dc-4657-a527-80d19ed635cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstCost, W, b = train_model(X=X_train.T, Y=y_train[np.newaxis,:], num_epochs=20000, lr=0.05)\n",
    "# If you have errors load the following module\n",
    "#import binclas as bc\n",
    "#lstCost, W,b = bc.train_model(X=X_train.T, Y=y_train[np.newaxis,:], num_epochs=20000, lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1863e73a-9f0e-4705-867f-11fc1dda1cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in lstCost[-10:]:\n",
    "    print(f\"  cost:{item}\")\n",
    "print(f\"  Weight:\\n{W}\")\n",
    "print(f\"  bias:{b}\")\n",
    "\n",
    "# Predict Labels\n",
    "y_pred = predict_labels(X_train.T, W,b)\n",
    "acc = accuracy(y_pred, y_train)\n",
    "print(f\"  accuracy:{acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd402d31-58e1-43d8-9028-23143d90e422",
   "metadata": {},
   "source": [
    "#### Comparison with LogisticRegression (from sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a418df2d-3198-4181-8b99-7aa7062fecb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model using different versions of sckit-learn\n",
    "# High-accuracy without L2 (term) \n",
    "model1 = LogisticRegression(penalty=None, max_iter=100000, tol=1.E-10).fit(X_train,y_train)\n",
    "print(f\"  LogisticRegression (sklearn) without L2 (Training Set) ::\")\n",
    "print(f\"    coef:{model1.coef_}\")\n",
    "print(f\"    intercept:{model1.intercept_}\")\n",
    "print(f\"    score:{model1.score(X_train,y_train):8.4f}\")\n",
    "\n",
    "model2 = LogisticRegression(max_iter=100000, tol=1.E-10).fit(X_train,y_train)\n",
    "print(f\"  LogisticRegression (sklearn) (Training Set) ::\")\n",
    "print(f\"    coef:{model2.coef_}\")\n",
    "print(f\"    intercept:{model2.intercept_}\")\n",
    "print(f\"    score:{model2.score(X_train,y_train):8.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ef5a9f-e78c-4805-bd2c-2ee2e4140799",
   "metadata": {},
   "source": [
    "### 5.5.Plot the training data and the decision boundary\n",
    "The point where the <font color=\"green\"><b>decision boundary</b></font> can be found, is $\\sigma(z) = \\frac{1}{2}$.<br>\n",
    "Thus:\n",
    "\n",
    "$\\begin{eqnarray}\n",
    "  \\sigma(z)    = & \\frac{1}{2} \\nonumber \\\\\n",
    "  \\Rightarrow \\frac{1}{1+e^{-(\\widehat{w}^T x + \\widehat{b})}} = & \\frac{1}{2} \\nonumber \\\\\n",
    "  \\Rightarrow x_2  = &  \\frac{-(\\widehat{w_1} x_1 + \\widehat{b})}{\\widehat{w_2}} \\nonumber\n",
    "\\end{eqnarray}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69abd47-6bc9-4f0f-99e7-26c91ee939a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the training set + decision boundary\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='RdBu')  \n",
    "plt.title(\"Train Data (2 Features) + decision boundary\")\n",
    "plt.xlabel(r\"$x_1$\")\n",
    "plt.ylabel(r\"$x_2$\")\n",
    "(x1_min, x1_max) = np.min(X_train[:,0]), np.max(X_train[:,0])\n",
    "delta = 0.1\n",
    "x1 = np.linspace(x1_min-delta,x1_max+delta,501)\n",
    "x2 = -(W[0]*x1 + b)/W[1]\n",
    "plt.plot(x1, x2,'g', label=r\"$x_2 = - \\frac{\\widehat{w_1}\\,x_1+\\widehat{b}}{\\widehat{w_2}}$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57d129a-8b5f-4278-b11a-6e2b54c919d3",
   "metadata": {},
   "source": [
    "### 5.6.Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97d04ee-0005-4d6b-86cd-1e9ba93a4d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here comes your code to test the model\n",
    "# Predict Labels\n",
    "y_pred = bc.predict_labels(X_test.T, W,b)\n",
    "acc = bc.accuracy(y_pred, y_test)\n",
    "print(f\"  Accuracy:{acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3345a44-6d6a-4846-89b2-eb2ab1a35aab",
   "metadata": {},
   "source": [
    "#### Comparison with LogisticRegression (from sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a50736a-dd27-433e-8857-b52bd76504f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"  LogisticRegression (sklearn) without L2 (Test Data Set) ::\")\n",
    "print(f\"    score:{model1.score(X_test,y_test):8.4f}\")\n",
    "print(f\"  LogisticRegression (sklearn) (Test Set) ::\")\n",
    "print(f\"    score:{model2.score(X_test,y_test):8.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06040257-1449-476b-a1ff-5854ef2dfb0d",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "Congratulations!<br>\n",
    "* You have implemented the <font color=\"orangered\"><b>simplest (shallow) neural net</b></font> from scratch.\n",
    "* You have learned that <font color=\"orangered\"><b>training of a neural net</b></font> is (in general) an iterative process \n",
    "  consisting of the following steps:\n",
    "  + <font color=\"orangered\"><b>forward propagation</b></font>.\n",
    "  + <font color=\"orangered\"><b>backward propagation</b></font>.\n",
    "  + <font color=\"orangered\"><b>calculation of the gradients w.r.t. loss function</b></font>.\n",
    "  + <font color=\"orangered\"><b>updating parameters</b></font> using gradient descent.\n",
    "* Once the training is finished, you can <font color=\"orangered\"><b>test/validate your model</b></font>.\n",
    "* After the validation process your apply your model on new data (from the same distribution)/inference</b></font>.\n",
    "\n",
    "The above algorithm is the <font color=\"red\"><b>basic algorithm</b></font> for **all** neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f7bb8d-ed30-4f4a-a576-e5f977fe4888",
   "metadata": {},
   "source": [
    "## 7.Perform the previous task using PyTorch\n",
    "* For the simplest (shallow) neural net we discussed:<br>\n",
    "  - we examined its components in detail.\n",
    "  - we developed the corresponding code.\n",
    "* For <font color=\"green\"><b>more advanced networks</b></font> most AI/deep learing practitioners:\n",
    "  - Do not derive the underlying equations.\n",
    "  - Nor do they implement them from scratch.<br>\n",
    "  Yet, the <font color=\"green\"><b>core mechanism remains the same</b></font> - and it's not particularly complex.\n",
    "* However, implementing <font color=\"orangered\"><b>highly performant code</b></font>\n",
    "  for the general case is significantly more <font color=\"red\"><b>demanding and time-consuming</b></font>.<br>It requires:\n",
    "  - Mastery of a <font color=\"red\"><b>compiled language</b></font> (e.g., `C++`)\n",
    "  - Deep understanfing of <font color=\"red\"><b>parallel computing</b></font> (multi-node, multi-GPU setups).\n",
    "  - A solid foundation in <font color=\"red\"><b>algorithms and numerical analysis</b></font>.\n",
    "* To address similar challenges, humanity adopted a\n",
    "  <a href=\"https://www.marxists.org/reference/archive/smith-adam/works/wealth-of-nations/book01/ch01.htm\"><b>division of labour</b></a>.   \n",
    "* Instead of building everything from scratch, practitioners rely on <font color=\"green\"><b>frameworks</b></font>.<br>The most commonly used include:\n",
    "  - <a href=\"https://pytorch.org/\"><b>PyTorch</b></a>\n",
    "  - <a href=\"https://www.tensorflow.org/\"><b>TensorFlow</b></a>\n",
    "  - <a href=\"https://docs.jax.dev/en/latest/\"><b>Jax</b></a>\n",
    "  - <a href=\"https://keras.io/\"><b>Keras</b></a>\n",
    "  - <a href=\"https://www.microsoft.com/en-us/cognitive-toolkit/\"><b>Microsoft CNTK</b></a>\n",
    "  \n",
    "Our goal is to implement the same model using PyTorch.<br>\n",
    "In general, this involves several key steps:\n",
    "* A. <font color=\"green\"><b>Load</b></font> the data\n",
    "* B. Define the <font color=\"green\"><b>Deep Neural Net (DNN) model/architecture</b></font>\n",
    "* C. Specify the <font color=\"green\"><b>loss/objective function</b></font>\n",
    "* D. Choose the <font color=\"green\"><b>optimization algorithm</b></font>\n",
    "* E. Assemble the components \\{A,B,C,D\\} to:\n",
    "     + Train the model\n",
    "     + Save/load the model to & from disk\n",
    "     + Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2ba59b-d299-49d1-9f0e-ee546657913c",
   "metadata": {},
   "source": [
    "### A.Loading the data\n",
    "* We have 2 PyTorch data classes (to be discussed later):\n",
    "  + data.Dataset : to load/create data in a class<br>\n",
    "    requires: \\_\\_init\\_\\_(), \\_\\_len\\_\\_(), \\_\\_getitem\\_\\_()\n",
    "  + data.DataLoader:: to load data in batches\n",
    "* For the time being (conversion to PyTorch Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138b0abe-2f46-4349-a7ab-7d530a673784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the PyTorch Tensors from the NumPy Data\n",
    "# Note: default torch.float32\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1,1)  # 2D \n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1,1)    # 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593a282d-7154-4cfc-8af1-021cd7e8f626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Conversion from NumPy to PyTorch Tensor\n",
    "START, END= 0, 2\n",
    "print(f\"NumPy data::\")\n",
    "print(f\"  X_train: {X_train.shape}\\n{X_train[START:END]}\")\n",
    "print(f\"  y_train: {y_train.shape}\\n{y_train[START:END]}\")\n",
    "print(f\"\\nPyTorch data::\")\n",
    "print(f\"  X_train_tensor: {X_train_tensor.shape}\\n{X_train_tensor[START:END]}\")\n",
    "print(f\"  y_train_tensor: {y_train_tensor.shape}\\n{y_train_tensor[START:END]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc197963-abb7-4e21-a87a-1313a5c318a6",
   "metadata": {},
   "source": [
    "### B.The Deep Neural Net (DNN) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872f0b4c-90c6-42bd-8999-39f4c3bdb1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logistic Regression module using PyTorch\n",
    "class LogisticRegressionModel1(nn.Module):\n",
    "\n",
    "    def __init__(self, num_inputs):\n",
    "\n",
    "        # The class inherits from the class nn.Module\n",
    "        super(LogisticRegressionModel1,self).__init__()\n",
    "\n",
    "        # Define a Single LAYER object which connects \n",
    "        #     the input with 1 single output \n",
    "        self.linear = nn.Linear(num_inputs, 1)\n",
    "\n",
    "        # Create the ACTIVATION (object) for the Single Layer\n",
    "        self.act_fn = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Applies the forward propagation\n",
    "        z = self.linear(x)\n",
    "        a = self.act_fn(z)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33762e7-caa2-41c5-85bb-d3af7dc7e35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = LogisticRegressionModel1(num_inputs=2)\n",
    "print(f\"  Logistic Model:{model1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f833ee-138a-4c71-abe3-82b1e5aec1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In PyTorch, you can use either the parameters() function\n",
    "# or the names_parameters() function\n",
    "for name, param in model1.named_parameters():\n",
    "    print(f\"Name:{name:20s} -> param:{param.shape}\")\n",
    "    print(f\"{param.data}\\n\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e37927-445f-4aac-9c32-ab9145f1cbfb",
   "metadata": {},
   "source": [
    "### C.Loss/Objective function\n",
    "* In order to find the <font color=\"green\"><b>optimal parameters</b></font> for the weights and bias, we need\n",
    "  to have an <font color=\"green\"><b>objective function</b></font> (a.k.a Loss function)\n",
    "* There are several options:\n",
    "  + <a href=\"https://docs.pytorch.org/docs/stable/generated/torch.nn.BCELoss.html#bceloss\"><b>nn.BCELoss()</b></a>: Binary Cross entropy => inputs need to be $[0,1]$\n",
    "  + <a href=\"https://docs.pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss\"><b>nn.BCEWithLogitLoss()</b></a>: Numerically more stable because of the combination of sigmoid and loss function at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17cb990-6c6d-4b2d-8726-f5e8ef3623d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates an instance of the Binary Cross Entropy Criterion\n",
    "loss_fn1 = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24295432-ce38-4ebd-9557-3b0ff81d049a",
   "metadata": {},
   "source": [
    "### D.Optimization\n",
    "* There are several methods to <font color=\"green\"><b>optimize</b></font> the Loss function/Objective function.<br>\n",
    "  - In this example we will use the <a href=\"https://docs.pytorch.org/docs/stable/generated/torch.optim.SGD.html#sgd\"><b>Stochastic Gradient Descent (SGD)</b></a> method<br>\n",
    "    (<a href=\"https://docs.pytorch.org/docs/stable/optim.html\"><b>torch.optim module</b></a>).\n",
    "  - Later on, we will describe more powerful <font color=\"green\"><b>optimization algorithms</b></font> (Adam, ...).\n",
    "* Useful methods:\n",
    "  + step(): method update parameters\n",
    "  + zero_grad() : sets the gradients of ALL optimized parameters to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00445b61-2484-4c71-8f76-d8c54758efc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim1 = optim.SGD(model1.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78eacc1-9656-402f-8b53-3bb86b03881c",
   "metadata": {},
   "source": [
    "### E.Assembling the components \\{A,B,C,D\\}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd6747b-5400-4995-90e1-718fc0dcd6e2",
   "metadata": {},
   "source": [
    "#### 1.Training\n",
    "* Goal: Obtain the optimized parameters i.e. <font color=\"green\"><b>weight matrix</b></font> and <font color=\"green\"><b>bias</b></font>\n",
    "* If the data set is **small**, then we will use **all** the training data at **once**.\n",
    "* Terminology:\n",
    "  - **One** complete iteration over **all** training data: <font color=\"green\"><b>epoch</b></font>\n",
    "  - For **larger** training data sets, each <font color=\"green\"><b>epoch</b></font> is split into <font color=\"green\"><b>batches</b></font>.<br>\n",
    "    * The gradient and the parameters are <font color=\"blue\"><b>updated</b></font> after every batch (points are selected <font color=\"blue\"><b>randomly</b></font>):<br>\n",
    "      <font color=\"green\"><b>stochastic gradient descent (SGD)</b></font>\n",
    "    * The batch size is a <font color=\"green\"><b>hyperparameter</b></font>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f75a1e4-75c4-47e7-9715-9f03657a6fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train_tensor, y_train_tensor, model, loss_fn, optim, num_epochs=100000 , delta_print=10000):\n",
    "    \"\"\"\"\n",
    "    Function which trains the model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set model to train mode\n",
    "    # Strictly not necessary for our case \n",
    "    model.train()\n",
    "\n",
    "    # Loop over the epochs\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # PART A: FORWARD PROPAGATION ( => )\n",
    "        # Step 1: Generate the output (activation of the linear layer)\n",
    "        output = model(X_train_tensor)\n",
    "\n",
    "        # Step 2: Use the activation of the last layer & the labels\n",
    "        #         to calculate the loss.\n",
    "        loss = loss_fn(output, y_train_tensor)\n",
    "\n",
    "        # Step B: BACK PROPAGATION ( <= )\n",
    "        # Step 3: Calculate the gradients of the parameters\n",
    "        optim.zero_grad()   # Init. the gradients to ZERO!!\n",
    "        loss.backward()     # Calc. grad. of param.\n",
    "\n",
    "        # Step 4: Adjust the parameters \n",
    "        optim.step()\n",
    "\n",
    "        if (epoch+1)%delta_print == 0 or epoch==0:\n",
    "           print(f\"  Epoch {epoch+1}/{num_epochs}  Loss:{loss.item():.6f}\")\n",
    "                \n",
    "    return loss.item()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4deac211-6416-43c3-a296-ff7066de3eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "final_loss1 = train_model(X_train_tensor, y_train_tensor, model1, loss_fn1, optim1)\n",
    "print(f\"Loss in the last step:{final_loss1:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a76000b-3ef8-4003-9358-5dd881c31f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOW TO Check the final parameters\n",
    "#   Method 1:\n",
    "print(f\"METHOD 1::\")\n",
    "print(f\"Weights::\\n{model1.linear.weight}\\n\")\n",
    "print(f\"Bias   ::\\n{model1.linear.bias}\")\n",
    "\n",
    "print(f\"\\nMETHOD 2::\")\n",
    "for name, param in model1.state_dict().items():\n",
    "    print(f\"{name} -> {param.shape}\")\n",
    "    print(f\"  {param.data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2b6fc7-23f5-4431-9e98-56386149effd",
   "metadata": {},
   "source": [
    "#### 2.Save/load the model to & from disk\n",
    "* To <font color=\"green\"><b>save</b></font> an object to disk, use <a href=\"https://docs.pytorch.org/docs/stable/generated/torch.save.html#torch-save\"><b>torch.save()</b></a>\n",
    "* To <font color=\"green\"><b>load</b></font> an object from disk, use <a href=\"https://docs.pytorch.org/docs/stable/generated/torch.load.html#torch-load\"><b>torch.load()</b></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e1621c-87c0-4487-8336-b27519031894",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='linreg1.pth'\n",
    "torch.save(model, filename)\n",
    "newmodel = torch.load(filename, weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8a0f91-2301-4012-a13d-dfc560be46e2",
   "metadata": {},
   "source": [
    "#### 3.Obtain predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9c9247-bcb5-4291-86b3-9b6fb0f5d660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(X_tensor, y_tensor, model):\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        res_tensor = model(X_tensor)\n",
    "        ypred_tensor =(res_tensor>0.5).float()\n",
    "    return ypred_tensor    \n",
    "\n",
    "def get_accuracy(y_pred, y):\n",
    "    num_ok = float((y_pred == y).sum())\n",
    "    return (num_ok / y_pred.shape[0]) * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f80995-21d5-4917-a824-0194a47ee851",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trainpred_tensor = test_model(X_train_tensor, y_train_tensor, newmodel)\n",
    "acc_train = get_accuracy(y_trainpred_tensor, y_train_tensor)\n",
    "print(f\"Accuracy train:{acc_train:8.4f}\")\n",
    "\n",
    "y_testpred_tensor = test_model(X_test_tensor, y_test_tensor, newmodel)\n",
    "acc_test = get_accuracy(y_testpred_tensor, y_test_tensor)\n",
    "print(f\"Accuracy test:{acc_test:8.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba7f7f9-cc94-40d5-8716-1a078bd98a8d",
   "metadata": {},
   "source": [
    "## 8.Alternative implementation within PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13365877-aa9a-4cb1-9ae2-f449f2e530fe",
   "metadata": {},
   "source": [
    "In the previous section, we implemented the \n",
    "* <font color=\"green\"><b>activation function</b></font> $a_i$, i.e.<br>\n",
    "  $\\begin{eqnarray}\n",
    "     a_i & = & \\sigma(z_i)\\\\\n",
    "         & = & \\frac{1}{1+e^{-z_i}} \\\\\n",
    "  \\end{eqnarray}$\n",
    "* loss function $\\mathcal{L}^{(i)}$, i.e.<br>\n",
    "  $\\begin{eqnarray}\n",
    "       \\mathcal{L}^{(i)} & = & - \\bigg [ y_i \\log(a_i) + (1-y_i)\\log(1-a_i) \\bigg ] \n",
    "  \\end{eqnarray}$<br>\n",
    "separately.\n",
    "\n",
    "To render the <font color=\"green\"><b>optimization numerically more stable</b></font> the <font color=\"green\"><b>activation and the loss function</b></font> can be combined into **one** function.<br>\n",
    "The corresponding loss function bears the name <a href=\"https://docs.pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss\"><b>BCEWithLogitsLoss</b></a>\n",
    "and is given by:\n",
    "\n",
    "$\\begin{eqnarray}\n",
    "  \\mathcal{L}^{(i)} & = & -\\bigg [ y_i \\log(a_i) + (1-y_i)\\log(1-a_i) \\bigg ]\\\\ \n",
    "                    & = & z_i(1-y_i) + \\log(1+e^{-z_i}) \\\\\n",
    "\\end{eqnarray}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6546feea-cd34-46a6-8b13-801452734d44",
   "metadata": {},
   "source": [
    "##### **Exercise 5.1:**\n",
    "* Implement the `class LogisticRegressionModel2(nn.Module)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9695a53a-2235-4737-9d00-96b908d4991c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logistic Regression module using PyTorch \n",
    "class LogisticRegressionModel2(nn.Module):\n",
    "\n",
    "    def __init__(self, num_inputs):\n",
    "\n",
    "        # The class inherits from the class nn.Module\n",
    "        super(LogisticRegressionModel2,self).__init__()\n",
    "\n",
    "        # <--- YOUR CODE\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Applies the forward propagation\n",
    "        # <--- YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d025b52c-ef20-474f-8d78-924764afaee2",
   "metadata": {},
   "source": [
    "Check the model <font color=\"blue\"><b>(ante optimization)</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0eaa0a-6a45-48e3-9e78-47028c2f58d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = LogisticRegressionModel2(num_inputs=2)\n",
    "print(f\"  Logistic Model:{model2}\")\n",
    "\n",
    "for name, param in model2.named_parameters():\n",
    "    print(f\"Name:{name:20s} -> param:{param.shape}\")\n",
    "    print(f\"{param.data}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0477716-cae5-4436-bb13-26bfa14d63f2",
   "metadata": {},
   "source": [
    "##### **Exercise 5.2:**\n",
    "* Implement the `loss_fn2` using BCWWithLogitsLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588ea3a8-0d0f-4ccd-8a35-695ba436daf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here comes your code to define the BCEWithLogitsLoss\n",
    "loss_fn2 = # <--- Here comes your code\n",
    "loss_fn2 = nn.BCEWithLogitsLoss() # <----- WRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f7506d-ffd3-4f42-9e5c-d61dee98da2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim2 = optim.SGD(model2.parameters(), lr=0.005)\n",
    "final_loss2 = train_model(X_train_tensor, y_train_tensor, model2, loss_fn2, optim2)\n",
    "print(f\"Loss in the last step:{final_loss2:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d09f515-1065-4855-b232-b7948dedddf1",
   "metadata": {},
   "source": [
    "Check the model <font color=\"blue\"><b>(post optimization)</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07ae834-2ca9-47b8-b33b-eb7effda5f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model2.named_parameters():\n",
    "    print(f\"Name:{name:20s} -> param:{param.shape}\")\n",
    "    print(f\"{param.data}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
