{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cb4c0a-e099-446b-9097-6e31656f66e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Dict, Any\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# sklearn dependencies\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc3c2d9-2d9c-4cfb-8609-8348c91a79b5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Hands-on Introduction to Deep Learning (Lecture 1)\n",
    "The **prereqs** are kept (on purpose) to a minimum, i.e.:\n",
    "- Basic knowledge of <font color=\"blue\"><b>(partial) derivatives and the chain rule</b></font>.\n",
    "- Ability to perform <font color=\"blue\"><b>simple matrix operations</b></font> (dot product, multiplication, transpose).\n",
    "- Knowledge of <a href=\"https://www.python.org\"><b>Python</b></a> and <a href=\"https://www.numpy.org\"><b>NumPy</b></a>.<br>\n",
    "  CHPC provides courses on these topics. You can find them at:<br>\n",
    "  + <a href=\"https://github.com/chpc-uofu/python-lectures\"><b>Introduction to Python</b></a>\n",
    "  + <a href=\"https://github.com/chpc-uofu/intro-numpy\"><b>Introduction to NumPy & SciPy</b></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4944a1-b79a-4744-8060-ea4c235b088a",
   "metadata": {},
   "source": [
    "# 1.Artificial Intelligence (AI)\n",
    "<font color=\"green\"><b>Artificial intelligence (AI)</b></font> is the ability of machines to perform tasks that<br>normally <font color=\"green\"><b>require human intelligence</b></font>.\n",
    "Among these tasks we have, e.g.:\n",
    "* discern hand-written digits, addresses,...\n",
    "* facial recognition in order to admit people access to buildings\n",
    "* write essays\n",
    "* translate from one language into another one.\n",
    "* convert audio into transcripts\n",
    "* self-driving vehicles\n",
    "* $\\ldots$\n",
    "\n",
    "Traditionally AI has been performed using <font color=\"green\"><b>different approaches</b></font>:\n",
    "* <a href=\"https://pdfs.semanticscholar.org/4fb2/7b22f57442ef8dddfd5eec2e8ef17b901323.pdf\">**Rules-Based AI (Expert systems)**</a>:\n",
    "  - Domain experts extract rules and codify them.\n",
    "  - Money & time consuming.\n",
    "  - Rather rigid in order to incorporate new knowledge.\n",
    "* <a href=\"https://www.amazon.com/Logic-Based-Artificial-Intelligence-International-Engineering/dp/0792372247\">**Logic based AI**</a>: The use of symbolic logic to perform reasoning\n",
    "* <a href=\"https://epubs.siam.org/doi/book/10.1137/1.9781611977882\">**Machine Learning (ML)**</a>:\n",
    "  - A data-centric approach\n",
    "  - Computer systems are able to \"learn\" from data without explicit instructions\n",
    "  - Central underpinnings of ML: probability theory, statistics and mathematical optimization.\n",
    "\n",
    "In the last decade, ML has become the research focus of AI, due to the following reasons:\n",
    "* The enormous explosion of data creation (Internet, audio, video, audio,..)<br>\n",
    "  <font color=\"orangered\"><b>Data has become the new currency</b></font>: ML requires huge amounts of data to train its models.\n",
    "* The increase of parallel computational power:<br>\n",
    "  - the <font color=\"green\"><b>Graphical Processing Units (GPUs)</b></font> were originally developed for gaming\n",
    "  - General-purpose GPUs (GPGPUs) emerged for tasks beyond visualization such as Scientific Computing and ML.\n",
    "     \n",
    "## Types of Machine Learning (ML):\n",
    "* <font color=\"green\"><b>Supervised Learning:</b></font>\n",
    "  - <font color=\"blue\"><b>input data and (labeled) output data</b></font> are used for training\n",
    "  - common techniques: regression, classification\n",
    "* <font color=\"green\"><b>Unsupervised Learning:</b></font>\n",
    "  - <font color=\"blue\"><b>input data</b></font> are used for training.\n",
    "  - high dimensional input data are used and reduced in dimensionality\n",
    "  - common techniques: clustering, PCA, ..\n",
    "* <font color=\"green\"><b>Reinforcement Learning:</b></font>\n",
    "  - concept borrowed from behavorial psychology\n",
    "  - positive actions are rewarded; negative actions are penalized.\n",
    "\n",
    "## Current status  \n",
    "- Currently, an ML model is trained for <font color=\"green\"><b>one specific task</b></font>.\n",
    "  * it may outperform a human specialist in one particular task (e.g. the interpretation of medical images).\n",
    "  * but <font color=\"green\"><b>NOT as versatile</b></font> as the human brain/intelligence.\n",
    "- The versatile AI equivalent of the human mind is <font color=\"green\"><b>AGI (Artificial General Intelligence)</b></font>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d882f1-44cd-4c07-88f4-bbe916a2b81f",
   "metadata": {},
   "source": [
    "# 2.The concept of a neuron/perceptron\n",
    "A <font color=\"green\"><b>neuron</b></font> or nerve cell:\n",
    "- fundamental unit of the nervous system\n",
    "- fires electrical signals across a <font color=\"green\"><b>neural network</b></font>.\n",
    "- contains a nucleus and mitochondria\n",
    "- has additional structure:\n",
    "  - <font color=\"green\"><b>dendrites</b></font>: receive the incoming electric signal (<font color=\"green\"><b>input</b></font>)\n",
    "  - <font color=\"green\"><b>axon</b></font>: transmits the electrical signal away from the nerve cell body (<font color=\"green\"><b>output</b></font>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab3dd94-d3c6-47a2-ac0c-4e7f921c5738",
   "metadata": {},
   "source": [
    "<img src=\"neuron.jpg\" alt=\"neuron\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ee3c71-ace2-4278-b5e3-d7f5ff3ead60",
   "metadata": {},
   "source": [
    "Source: <a href=\"https://www.ninds.nih.gov/health-information/public-education/brain-basics/brain-basics-life-and-death-neuron\"><b>Brain Basics: The Life and Death of a Neuron<b></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc2f6eb-08c8-4af2-8a09-83e29a6759ec",
   "metadata": {},
   "source": [
    "The concept of a physical neuron:\n",
    "- gave rise to the concept of perceptron (<a href=\"https://bpb-us-e2.wpmucdn.com/websites.umass.edu/dist/a/27637/files/2016/03/rosenblatt-1957.pdf\"><b>Rosenblatt, 1957</b></a>)\n",
    "  \n",
    "In essence, a <font color=\"green\"><b>perceptron</b></font> is a **non-linear function** $f$ \n",
    "(<font color=\"green\"><b>activation function</b></font>)<br>\n",
    "which operates <font color=\"green\"><b>input</b></font> and returns <font color=\"green\"><b>output</b></font>.\n",
    "\n",
    "Or a little more formal, $\\textbf{y}=\\widehat{h}(\\textbf{x})$, where:\n",
    "* $\\widehat{h}$: non-linear/activation operator\n",
    "* $\\textbf{x} \\in \\mathbb{R}^{n_1 \\times 1}$: input vector i.e. $\\textbf{x}:=(x_1,x_2,\\ldots, x_{n_1})^T$.\n",
    "* $\\textbf{y} \\in \\mathbb{R}^{n_2 \\times 1}$: output vector i.e. $\\textbf{y}:=(y_1,y_2,\\ldots, y_{n_2})^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870c794f-0f7f-41ad-97c7-e78183d50381",
   "metadata": {},
   "source": [
    "In what follows we will perform \n",
    "<font color=\"green\"><b>logistic regression</b></font> (<font color=\"orangered\"><b>the most simple (shallow) neural net possible</b></font>) on a simple data set. <br>This simple toy model/example will allow us:\n",
    "* to display the <b>(essential) features</b> of deep learning.\n",
    "* to easily transition to the <b>general case</b> (see <a href=\"./lecture2.ipynb\"><b>Lecture 2</b></a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1120457c-3455-4232-b073-88535a6353e5",
   "metadata": {},
   "source": [
    "# 3.Logistic Regression (LR) - a bird's view\n",
    "## 3.1.Goals/Tasks\n",
    "* To train a <font color=\"green\"><b>binary classifier</b></font> based on a given data set\n",
    "  <br>using <font color=\"red\"><b>one neuron</b></font> in just <font color=\"red\"><b>one layer</b></font>, i.e.\n",
    "  (**shallow network**)\n",
    "* To obtain the <font color=\"green\"><b>accuracy</b></font> of the trained model using a test set.\n",
    "* To <font color=\"green\"><b>predict</b></font> the outcome of some data (provided) using the model.\n",
    "* To get a start with PyTorch/Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6706610c-9c90-4154-b68e-7bee80962520",
   "metadata": {},
   "source": [
    "<img src=\"./perceptron.jpeg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8d94e4-d922-4933-a9ae-219f57bf2263",
   "metadata": {},
   "source": [
    "<font><i>Logistic regression as a (shallow neural network):</i></font>\n",
    "* <font color=\"green\"><b>Input</b></font> ($l=0$): vector $x_i$ with $5$ <font color=\"green\"><b>features</b></font>.\n",
    "* <font color=\"red\"><b>First layer</b></font> ($l=1$) with <font color=\"red\"><b>one unit/node</b></font>:\n",
    "  + creation of $z_i:=\\displaystyle \\sum_{j=1}^5 x_{ij} w_j + b$  (i.e. `affine transformation`)\n",
    "  + $a_i := \\widehat{h}^{[1]}(z_i)$ (i.e. `non linear activation`)\n",
    "* Output ($a_i$):\n",
    "  + if `training`: to be used for loss function\n",
    "  + elif `test`/`inference`: to be used to determine the label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5eb75e-cf52-4f7c-8607-f0f93353d425",
   "metadata": {},
   "source": [
    "## 3.1.Initialization of the parameters\n",
    "\n",
    "Before starting the optimization process for the <font color=\"green\"><b>parameters</b></font> (weight vector and bias),<br>\n",
    "those parameters need to be <font color=\"green\"><b>initialized</b></font>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006a768b-d5d1-408c-adce-55b77bea76d0",
   "metadata": {},
   "source": [
    "## 3.2.Training of the binary classifier\n",
    "The <font color=\"green\"><b>training</b></font> of a binary classifier (and <font color=\"orangered\"><b>in extenso any deep neural net</b></font>) \n",
    "<br>consists of an **iterative loop** of the following $3$ tasks:\n",
    "\n",
    "1. <font color=\"green\"><b>Forward propagation</b></font>:<br>\n",
    "   Given a training set, and a set of <font color=\"green\"><b>parameters</b></font> \n",
    "   we calculate the associated cost function,<br>\n",
    "   which is a measure how different the predicted data are from the true data.\n",
    "\n",
    "2. <font color=\"green\"><b>Back propagation</b></font>:<br>\n",
    "   Based on the cost function we calculate the <font color=\"green\"><b>gradients of the parameters</b>   </font>.\n",
    " \n",
    "3. <font color=\"green\"><b>Update of the parameters</b></font>:<br>\n",
    "   The parameters are <font color=\"green\"><b>updated</b></font> using <font color=\"green\"><b>gradient descent</b></font>.\n",
    "\n",
    "The <font color=\"green\"><b>training set</b></font> consists of $m_{\\mathrm{train}}$ data points</b> $(\\mathbf{x}_i,y_i)$, $i \\in \\{1,\\ldots,m_{\\mathrm{train}}\\}$<br>\n",
    "  where:\n",
    "  - $\\mathbf{x_i}$ is a column vector of length of $n$, i.e. $\\mathbf{x_i} \\in \\mathbb{R}^{n \\times 1}$.<br>\n",
    "    Each dimension of the vector $\\mathbf{x_i}$ represents a <font color=\"green\"><b>feature</b></font>.\n",
    "  - $y_i$ is either 0 ($\\texttt{False}$) or 1 ($\\texttt{True}$), i.e. $y_i \\in \\mathbb{R}$.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e425ac0e-381c-4422-ae57-1067fa92ef8b",
   "metadata": {},
   "source": [
    "## 3.3.Testing of the binary classifier\n",
    "In the <font color=\"green\"><b>testing phase</b></font> the accuracy of the model will be determined.\n",
    "\n",
    "In the next sections, we will describe the details of each of these steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce5795f-b43c-40eb-a5b5-dd79d803e86f",
   "metadata": {},
   "source": [
    "# 4.LR: Initialization\n",
    "\n",
    "During the <font color=\"green\"><b>initialization</b></font> all parameters (weights and bias) are set to zero.\n",
    "- <font color=\"green\"><b>weight vector</b></font>: $  w \\in \\mathbb{R}^{n \\times 1}$ \n",
    "- <font color=\"green\"><b>b</b></font>: $b \\in \\mathbb{R}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b55643-d66f-4291-a423-2bf587c81955",
   "metadata": {},
   "source": [
    "#### **Exercise 1**: Initialize the parameters\n",
    "\n",
    "`def init_param(n: int) -> Tuple[np.ndarray, float]:`<br>\n",
    "  * The <font color=\"green\"><b>weight</b></font> vector is a **column vector** of length $n$ (#features).\n",
    "  * The <font color=\"green\"><b>bias</b></font> is a **scalar** of type float.\n",
    "  * All elements of the weight vector and bias can be initialized to $0.0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5177fa4f-5fe5-4bf3-b2bc-45583e0680f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1:\n",
    "def init_param(n: int) -> Tuple[np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    Initialize the parameters (weight, bias) for the Binary Classifier.\n",
    "    \n",
    "    Args:\n",
    "        n (int): The number of features/dim. of the input vector\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[np.ndarray, float]: A tuple containing \n",
    "           - initialized weight: shape (n,1)\n",
    "           - initialized bias (scalar).\n",
    "    \"\"\"\n",
    "    # Here comes YOUR code to initialize the weight vector & bias.\n",
    "    # W =  <--- YOUR CODE: Weight (vector) to zero\n",
    "    # b =  <--- YOUR CODE: bias (float) to zero\n",
    "    return W,b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641e08d5-55ac-4718-8ecd-b25dfda1fdab",
   "metadata": {},
   "source": [
    "##### <font color=\"blue\"><b>Simple check of the function:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2b8652-86ed-4e04-9676-4c63266e6135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tests/lec1/test_ex1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d25af6-9011-4050-b340-5c244432f43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/lec1/sol_ex1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08513926-900d-4719-80da-0c954db6ef78",
   "metadata": {},
   "source": [
    "# 5.LR: Forward Propagation\n",
    "* At the perceptron/node, each instance $i$ ($i \\in \\{1,\\ldots,m_{\\mathrm{train}}\\}$) will be<br>\n",
    "  subjected to the following $2$ <font color=\"green\"><b>transformations</b></font>:\n",
    "  1. `affine` transformation:<br>\n",
    "       $\\begin{eqnarray}\n",
    "         z_i & = &   \\mathbf{x_i^T}.\\mathbf{w} +b  \\nonumber \\\\\n",
    "            &=  &  \\displaystyle{\\sum_{j=1}^n x_{ij} w_j +b} \\nonumber\n",
    "       \\end{eqnarray}$\n",
    "    \n",
    "     where:<br>\n",
    "     - $ \\mathbf{x_i}$: <font color=\"green\"><b>input vector</b></font> for example $i$ ($\\in \\mathbb{R}^{n\\times 1}$)\n",
    "  \n",
    "     - $ \\mathbf{w}$ : <font color=\"green\"><b>weight</b></font> vector ($\\in \\mathbb{R}^{n \\times 1}$)<br>\n",
    "       Note: the weight vector has <font color=\"orangered\"><b>same number of dimensions as there are features</b></font>.\n",
    "    \n",
    "     - $ b$ : <font color=\"green\"><b>bias</b></font> ($\\in \\mathbb{R}$).\n",
    "\n",
    "     - Each data instance uses the <font color=\"orangered\"><b>same weight vector and bias</b></font>.   \n",
    "  1. `non linear activation`:<br>\n",
    "     $a_i =  \\sigma(z_i)$ , $a_i \\in \\mathbb{R}$<br>\n",
    "     \n",
    "     - $\\sigma$ is known as the `sigmoid` function:<br>\n",
    "   \n",
    "       * $\\begin{equation}\n",
    "         \\sigma(z) = \\displaystyle \\frac{1}{1+e^{-z}}  \\nonumber \\;\\;,\\;\\; 0 \\leq \\sigma(z) \\leq 1\n",
    "         \\end{equation}$\n",
    "    \n",
    "     - The activation of the <b>last layer</b> (in this case we only have one layer) is the same<br>\n",
    "       as the predicted value ($\\widehat{y_i})$. Thus, <br>\n",
    "    \n",
    "       $\\begin{equation}\n",
    "       \\widehat{y_i} := a_i \\nonumber\n",
    "       \\end{equation}$\n",
    "   \n",
    "* Calculate the <font color=\"green\"><b>cost function</b></font> ($\\mathcal{C}$).<br>\n",
    "  The cost function $\\mathcal{C}$ is defined as the mean of the <font color=\"green\"><b>loss functions</b></font> ($\\mathcal{L}^{(i)}$) over the $m_{\\mathrm{train}}$ data points:\n",
    "\n",
    "  $\\begin{eqnarray}\n",
    "       \\mathcal{C}       & := & \\displaystyle \\frac{1}{m_{\\mathrm{train}}} \\sum_{i=1}^{m_{\\mathrm{train}}}        \\mathcal{L}^{(i)} \\nonumber\n",
    "  \\end{eqnarray}$\n",
    "\n",
    "  In case of <font color=\"green\"><b>binary classification</b></font>, the **loss function** $\\mathcal{L}^{(i)}$ for data point $i$ is given by:\n",
    "  \n",
    "  $\\begin{eqnarray}\n",
    "       \\mathcal{L}^{(i)} & = & - \\bigg [ y_i \\log(\\widehat{y_i}) + (1-y_i)\\log(1-\\widehat{y_i}) \\bigg ] \\nonumber\\\\\n",
    "                         & = & - \\bigg [ y_i \\log(a_i) + (1-y_i)\\log(1-a_i) \\bigg ]  \\nonumber\n",
    "  \\end{eqnarray}$\n",
    "\n",
    "* <font color=\"red\"><b>Note:</b></font><br>\n",
    "  - The loss function in logistic regression is <font color=\"green\"><b>convex</b></font>.<br>Loss functions in deep learning are\n",
    "    generally <font color=\"red\"><b>NOT</b></font> convex.\n",
    "  - <a href=\"./convexity.ipynb\"><b>Convexity</b></a> has some nice properties: e.g. the local minimum is the **GLOBAL** minimum.\n",
    "    \n",
    "\n",
    "* <font color=\"green\"><b>Vectorization:</b></font><br>\n",
    "  In case of NumPy vectorization speeds up code significantly.\n",
    "\n",
    "  Given that:\n",
    "  + $\\mathbf{x_i}$ is a column vector, i.e. $\\mathbf{x_i} \\in \\mathbb{R}^{n \\times 1}$.\n",
    "  + $y_i \\in \\{0,1\\}$.\n",
    "  + The $\\mathrm{m}$ column vectors $\\mathbf{x_i}$ can be stacked in an matrix $X \\in \\mathbb{R}^{m \\times n}$,<br>\n",
    "    given by:\n",
    "\n",
    "    $\\begin{eqnarray}\n",
    "         X & := & \\begin{pmatrix}  \\mathbf{x^T_1}\\\\\n",
    "                                   \\mathbf{x^T_2} \\\\\n",
    "                                   \\vdots \\\\\n",
    "                                   \\mathbf{x^T_{m}}\n",
    "                 \\end{pmatrix}\\nonumber\n",
    "    \\end{eqnarray}$\n",
    "\n",
    "      \n",
    "  + The $\\mathrm{m}$ $y_i$ values can be collected in a column vector $Y \\in \\mathbb{R}^{m \\times 1}$,<br>\n",
    "    given by:\n",
    "\n",
    "    $\\begin{eqnarray}\n",
    "       Y & := & \\begin{pmatrix} y_1 & y_2 & \\cdots & y_{\\mathrm{m}-1} & y_{\\mathrm{m}} \n",
    "                 \\end{pmatrix}^T \\nonumber\n",
    "      \\end{eqnarray}$\n",
    "\n",
    "  we get:\n",
    "  - $\\begin{eqnarray}\n",
    "         \\mathbf{Z} & = &   \\mathbf{X}.\\mathbf{w} + b\\mathbf{1}  \\nonumber \n",
    "       \\end{eqnarray}$<br>\n",
    "    where $\\mathbf{1}$ is the unity vector $\\in \\mathbb{R}^{m \\times 1}$\n",
    "\n",
    "  - $\\begin{eqnarray}\n",
    "          \\mathbf{A} & = & \\sigma(\\mathbf{Z}) \\nonumber\n",
    "    \\end{eqnarray}$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d2f438-4d4b-4c76-99d6-4c21cd0c6575",
   "metadata": {},
   "source": [
    "<font color=\"red\"><b>Note: (for sake of completeness)</b></font><br>\n",
    "\n",
    "The `activation` function $\\sigma(z)$ has the following properties:\n",
    "\n",
    "$\n",
    "   \\begin{eqnarray}\n",
    "   \\lim_{ z \\to -\\infty} \\sigma(z) & =&0 \\nonumber \\\\\n",
    "   \\lim_{ z \\to +\\infty} \\sigma(z) & = & 1 \\nonumber \\\\\n",
    "   \\sigma(0) & = &\\frac{1}{2} \\nonumber \\\\\n",
    "  \\displaystyle \\frac{d \\sigma(z)}{dz} & = & \\sigma(z)(1-\\sigma(z)) \\nonumber\n",
    "  \\end{eqnarray}$\n",
    "\n",
    "The range of the sigmoid is $[0,1]$ and can thus be interpreted as a <font color=\"green\"><b>probability</b></font>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb4aa30-ee02-4bda-9616-e22d1a0016b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-7.5, 7.5, 1501)\n",
    "y = 1.0/(1.0+np.exp(-x))\n",
    "plt.title(r\"Sigmoid function and decision boundary\")\n",
    "plt.xlabel(r\"$x$\")\n",
    "plt.ylabel(r\"$\\sigma(x)$\",rotation=0)\n",
    "plt.plot(x,y, label =r\"$\\sigma(x)$\" )\n",
    "plt.axvline(x=0,color='r',ymin=0.0, ymax=1.0, label=\"Decision boundary\")\n",
    "plt.plot(0.0,0.5,marker='o',color='orange')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7550844-d026-4f26-9ead-63dc01ac9904",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### **Exercise 2**: Implementation of the sigmoid activation function\n",
    "Perform $\\sigma(\\mathbf{Z})$ element-wise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa9cb4b-158c-424d-9bdc-522a784a3da0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Exercise 2:\n",
    "def sigmoid(Z: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute the sigmoid function.\n",
    "    \n",
    "    Args:\n",
    "        Z (np.ndarray): The input value(s).\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: The sigmoid of the input value(s).\n",
    "    \"\"\"\n",
    "    # Here comes YOUR code for the sigmoid function.\n",
    "    # return <-- YOUR CODE "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd4868f-04df-468b-bde5-7243b22e07e4",
   "metadata": {},
   "source": [
    "##### <font color=\"blue\"><b>Simple check of the function:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab8e146-5e4e-4f59-b490-ccab46c15a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tests/lec1/test_ex2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f91f00-29c3-42ad-b799-23d65eb5c91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/lec1/sol_ex2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9496359b-64b0-49ab-8dd3-57b44bdbbf39",
   "metadata": {},
   "source": [
    "#### **Exercise 3** : Implementation of the forward propagation\n",
    "*  $ \\mathbf{Z} = \\mathbf{X} \\mathbf{w} + \\mathbf{b} $\n",
    "*  $ \\mathbf{A} = \\sigma(\\mathbf{Z})$  ($\\texttt{Ex. 2}$)\n",
    "*  $ \\mathcal{C} = - \\frac{1}{m} \\displaystyle \\sum_{i=1}^m \\bigg [ y_i \\log(a_i) + (1-y_i)\\log(1-a_i) \\bigg ]$ <br>\n",
    "The latter equation can be easily vectorized (and should) in NumPy using $\\mathbf{A}$ and $\\mathbf{Y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce49e55-b6fd-4530-804c-181700de11af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1.3:\n",
    "def forward(X: np.ndarray, Y: np.ndarray,\n",
    "            W: np.ndarray, b: float) -> Tuple[np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    Perform the forward pass of the binary classifier.\n",
    "    \n",
    "    Args:\n",
    "        X (np.ndarray): The training data -> shape(m, n) \n",
    "                        where m is #samples & n is #features.\n",
    "        Y (np.ndarray): The training labels (targets) -> shape(m,1) \n",
    "                        where m is #samples.\n",
    "        W (np.ndarray): The weight vector             -> shape(n,1)\n",
    "        b (float)     : The bias term                 -> float\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[np.ndarray, float]: \n",
    "          A tuple containing the activation matrix and the cost.\n",
    "    \"\"\"\n",
    "    # Here comes YOUR code for the forward propagation.\n",
    "    num_samples = X.shape[0]\n",
    "    # Z = <--- YOUR CODE\n",
    "    # A = <--- YOUR CODE  \n",
    "    # cost = <--- YOUR CODE\n",
    "    return A, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f423e1-4c64-4ed4-b274-bf0de1d6864e",
   "metadata": {},
   "source": [
    "##### <font color=\"blue\"><b>Simple check of the function:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed68248-3bce-4d92-97ad-366eaefd7c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tests/lec1/test_ex3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6023e400-5d3e-4ed0-87fa-d4cb6b9774c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/lec1/sol_ex3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697fd5b7-8d51-402b-8340-9f6688f6053a",
   "metadata": {},
   "source": [
    "# 6.LR: Back Propagation\n",
    "\n",
    "In the <font color=\"green\"><b>back propagation</b></font> we calculate the **gradient of the cost function** w.r.t the weights and the bias:\n",
    "\n",
    "+ $\\begin{eqnarray}\n",
    "      \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial a_i} & = & \\frac{\\partial}{\\partial a_i} \\bigg [ - \\big [ y_i \\log(a_i) + (1-y_i)\\log(1-a_i) \\big ] \\bigg ] \\nonumber \\\\\n",
    "                                                      & = & -\\frac{y_i}{a_i} + \\frac{(1-y_i)}{(1-a_i)} \\nonumber\n",
    "    \\end{eqnarray}$\n",
    " \n",
    "    \n",
    "+ $\\begin{eqnarray}\n",
    "        \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial z_i} & = & \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial a_i} \\frac{\\partial a_i}{\\partial z_i} \\nonumber \\\\\n",
    "         & = & \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial a_i} \\frac{\\partial \\sigma(z_i)}{\\partial z_i}  \\nonumber \\\\\n",
    "         & =& a_i - y_i \\nonumber\n",
    "    \\end{eqnarray}$\n",
    " \n",
    "    \n",
    "+ $\\begin{eqnarray}\n",
    "       \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial b} & = & \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial a_i} \\frac{\\partial a_i}{\\partial z_i}\\frac{\\partial z_i}{\\partial b} \\nonumber \\\\\n",
    "                 & = & a_i - y_i \\nonumber\n",
    "    \\end{eqnarray}$\n",
    "\n",
    "  \n",
    "+ $\\begin{eqnarray}\n",
    "      \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial w_j} & = & \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial a_i} \\frac{\\partial a_i}{\\partial z_i}\\frac{\\partial z_i}{\\partial w_j} \\nonumber \\\\\n",
    "                 & = & (a_i - y_i) x_{ij} \\nonumber\n",
    "    \\end{eqnarray}$\n",
    "    \n",
    "Thus, <br>\n",
    "\n",
    "+ $\\begin{eqnarray}\n",
    "     \\frac{\\partial\\mathcal{C}}{\\partial b}  & =   &= & \\frac{1}{m_{\\mathrm{train}}}\\displaystyle \\sum_{i=1}^{m_{\\mathrm{train}}} \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial b}  \n",
    "                                             & = & \\frac{1}{m_{\\mathrm{train}}} \\displaystyle \\sum_{i=1}^{m_{\\mathrm{train}}} (a_i - y_i) \\nonumber\n",
    "     \\end{eqnarray}$\n",
    "\n",
    "+ $\\begin{eqnarray}\n",
    "     \\frac{\\partial\\mathcal{C}}{\\partial w_j}  &= & \\frac{1}{m_{\\mathrm{train}}}\\displaystyle \\sum_{i=1}^{m_{\\mathrm{train}}} \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial w_j} \n",
    "      & = & \\frac{1}{m_{\\mathrm{train}}} \\displaystyle \\sum_{i=1}^{m_{\\mathrm{train}}} (a_i - y_i) x_{ij} \\;\\;,\\;\\;\\forall \\, j \\in \\{1,\\ldots,n\\} \\nonumber\n",
    "     \\end{eqnarray}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d3f635-e9b4-4c62-925a-e9b44e5065b0",
   "metadata": {},
   "source": [
    "#### **Exercise 4**: Back propagation\n",
    "\n",
    "These are the steps:  \n",
    "* $\\mathbf{dZ} = \\mathbf{A} - \\mathbf{Y}$\n",
    "* $\\mathbf{dW} = \\frac{1}{m} \\mathbf{X}^T.\\mathbf{dZ}$\n",
    "* $db = \\frac{1}{m} \\displaystyle \\sum_{i=1}^m \\mathbf{dZ}_i$\n",
    "* return ($\\mathbf{dW},db$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a8ea4b-46d7-4055-9f26-2c524262c774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4:\n",
    "def calcgrad(X: np.ndarray, Y: np.ndarray, \n",
    "             A:np.ndarray) ->  Tuple[np.ndarray, float]:\n",
    "    \"\"\"        \n",
    "    Computes the gradients of the cost function with respect to W and b.\n",
    "    Arg:\n",
    "        X (np.ndarray): Training data     -> shape(m,n)\n",
    "        Y (np.ndarray): Training labels   -> shape(m,1)\n",
    "        A (np.ndarray): Activation matrix -> shape(m,1) \n",
    "    Return:\n",
    "        A tuple containing the gradients with respect to W and b.\n",
    "    \"\"\" \n",
    "    # Here comes the calcgrad code\n",
    "    num_samples = X.shape[0]\n",
    "    # dZ = <--- YOUR CODE\n",
    "    # dW = <--- YOUR CODE\n",
    "    # db = <--- YOUR CODE\n",
    "    return dW,db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fde3186-39db-4c8a-9405-fcaf6b7a51da",
   "metadata": {},
   "source": [
    "##### <font color=\"blue\"><b>Simple check of the function:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773fbe03-df29-4674-87db-96f0c8d41b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tests/lec1/test_ex4.py\n",
    "# Test Ex.4 : Checking the calcgrad function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956dd7bb-ada5-4b77-94ce-7602c3fd1a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/lec1/sol_ex4.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f986a34b-87b1-45af-8a83-3a98431ac829",
   "metadata": {},
   "source": [
    "# 7.LR: Update the parameters (with gradient descent)\n",
    "\n",
    "In this step, the parameters will be updated using <a href=\"https://en.wikipedia.org/wiki/Gradient_descent\"><b>gradient descent</b></a>: \n",
    "\n",
    "  $\\begin{eqnarray}\n",
    "      b & = & b - \\alpha  \\frac{\\partial\\mathcal{C}}{\\partial b} \\nonumber \\\\\n",
    "      w_j & = & w_j - \\alpha  \\frac{\\partial\\mathcal{C}}{\\partial w_j} \\;\\;,\\;\\;\\forall \\, j \\in \\{1,\\ldots,n\\} \\nonumber\n",
    "    \\end{eqnarray}$\n",
    "\n",
    "  where $\\alpha$ is known as the <font color=\"green\"><b>learning rate </b></font> or the <font color=\"green\"><b>step size</b></font>.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9df759-0807-415d-af63-36d92de6098b",
   "metadata": {},
   "source": [
    "#### **Exercise 5**: Update of the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9f5310-6b88-42dc-ac32-ddbe97b897e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(Weights: np.ndarray, bias: float,\n",
    "           dWeight: np.ndarray, dbias:float,\n",
    "           lr:float) -> Tuple[np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    Update the parameters using the gradients and learning rate.    \n",
    "\n",
    "    Args:\n",
    "        Weights (np.ndarray): The weight vector              -> shape(n, 1).\n",
    "        bias (float)        : The bias term                  -> float\n",
    "        dWeight (np.ndarray): The grad. of the cost w.r.t. W -> shape(n, 1).\n",
    "        dbias (float)       : The grad. of the cost w.r.t. b -> float\n",
    "        lr (float)          : The learning rate.    \n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, float]: \n",
    "          A tuple containing the updated weight and bias.\n",
    "    \"\"\"\n",
    "    # Here comes the code to update the weight vector and the bias.\n",
    "    # Weights = <--- YOUR CODE\n",
    "    # bias =    <--- YOUR CODE\n",
    "    return Weights, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d67b95b-6152-45f5-b44d-ca8ef74f9d9a",
   "metadata": {},
   "source": [
    "##### <font color=\"blue\"><b>Simple check of the function:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3f8655-0587-4aca-a0fd-208bac7c06d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tests/lec1/test_ex5.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9427be-af25-4752-89eb-560e297cd72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/lec1/sol_ex5.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fee4b7-fa87-43db-a18b-d7e24c6bc5b3",
   "metadata": {},
   "source": [
    "# 8.LR: Training: All components combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f58e61f-2afb-4eaf-8dbb-5d91c2f7520b",
   "metadata": {},
   "source": [
    "During **training**, the following steps are to be performed:\n",
    "\n",
    "* initialize $\\mathbf{W},b$ to 0.0  \n",
    "* perform the loop over all epochs\n",
    "  - Calculate $\\mathbf{A}, \\mathcal{C}$ (cost) using the forward function\n",
    "  - Calculate the gradients $\\mathbf{dW}, db$\n",
    "  - Update $\\mathbf{W}, b$ using the gradients $\\mathbf{dW}, db$\n",
    "* return lst(cost), $\\mathbf{W},b$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952d38ec-55ef-40e7-9ace-57df9fe6efa2",
   "metadata": {},
   "source": [
    "#### **Exercise 6**: Training (complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc6c97f-41d4-4cbc-87d7-1f0f710d3475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 6\n",
    "def train_model(X: np.ndarray, Y: np.ndarray,\n",
    "                num_epochs: int, lr: float) -> Tuple[List[float],np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    Train the binary classifier using gradient descent.\n",
    "    \n",
    "    Args:\n",
    "        X (np.ndarray): The training data (features)  -> shape(m,n) \n",
    "                        where m is #samples & n is #features.\n",
    "        Y (np.ndarray): The training labels (targets) -> shape(m,1)\n",
    "                        where m is #samples.\n",
    "        W (np.ndarray): The weight vector             -> shape(n,1).\n",
    "        b (float)     : The bias term                 -> float\n",
    "        num_epochs (int): The number of epochs to train.\n",
    "        lr (float)    : The learning rate.    \n",
    "    \n",
    "    Returns:\n",
    "        Tuple[np.ndarray, float]: \n",
    "          A tuple containing the final weight and bias after training.\n",
    "    \"\"\"\n",
    "    # Here comes YOUR code to train the model.\n",
    "    lstCost = []\n",
    "    # W,b =         <--- YOUR CODE : Initialize to 0.0 using previous function\n",
    "    for i in range(num_epochs):\n",
    "        # A, cost = <--- YOUR CODE : Use the forward function\n",
    "        lstCost.append(cost)\n",
    "        # dW, db =  <--- YOUR CODE : Calc. the gradient\n",
    "        # W, b   =  <--- YOUR CODE : Perform the update\n",
    "    return lstCost, W, b    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2737be94-aff0-4944-a03a-6b14d7e1183b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/lec1/sol_ex6.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b0e6f8-3a4e-42d8-8aeb-9c27068cc832",
   "metadata": {},
   "source": [
    "# 9.LR: Testing of the binary classifier \n",
    "Once our neural net has been trained, the <font color=\"green\"><b>optimal values</b></font><br> \n",
    "for the parameters $\\mathbf{w}$ and $b$ (i.e. $\\mathbf{\\widehat{w}}$ and $\\widehat{b}$) are known. <br> \n",
    "We are now ready to <font color=\"green\"><b>test</b></font> our neural net model.\n",
    "\n",
    "* Apply the <font color=\"green\"><b> predict </b></font> function to the **test data set**.<br>\n",
    "  The **predict** function has $2$ components:\n",
    "  + Apply the <font color=\"green\"><b>forward propagation</b></font> to the test data set (use $\\mathbf{\\widehat{w}}$ and $\\widehat{b}$).\n",
    "  + <font color=\"green\"><b>Map</b></font> the obtained activations to either $0$ or $1$\n",
    "* Calculate the <font color=\"green\"><b>accuracy</b></font> i.e. the ratio of the number of correct predictions over total predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b1c7d9-b189-44f8-925d-19ab66db3af1",
   "metadata": {},
   "source": [
    "## 9.1.Prediction function\n",
    "* The <font color=\"green\"><b>test set</b></font> consists of $m_{\\mathrm{test}}$ test data points</b>:<br> $(\\mathbf{x}_i,y_i)$, $i \\in \\{1,\\ldots,m_{\\mathrm{test}}\\}$<br>\n",
    "  where:\n",
    "  - $\\mathbf{x_i}$ is a column vector of length of $n$, i.e. $\\mathbf{x_i} \\in \\mathbb{R}^{n \\times 1}$.\n",
    "  - $y_i$ is either 0 ($\\texttt{False}$) or 1 ($\\texttt{True}$), i.e. $y_i \\in \\mathbb{R}$.\n",
    "  - The $m_{\\mathrm{test}}$ $\\mathbf{x_i}$ column vectors can be collected in the matrix $X \\in \\mathbb{R}^{ \\mathrm{m_{test}}\\times n}$,<br>\n",
    "    given by:\n",
    "\n",
    "    $\\begin{eqnarray}\n",
    "         X & := & \\begin{pmatrix}  \\mathbf{x^T_1}\\\\\n",
    "                                   \\mathbf{x^T_2} \\\\\n",
    "                                   \\vdots \\\\\n",
    "                                   \\mathbf{x^T_{\\mathrm{m_{test}}}}\n",
    "                 \\end{pmatrix} \\nonumber\n",
    "    \\end{eqnarray}$\n",
    "\n",
    "  - The $m_{\\mathrm{test}}$ $y_i$ values can be collected in the column vector $Y$,<br>\n",
    "    given by:\n",
    "\n",
    "    $\\begin{eqnarray}\n",
    "         Y & := & \\begin{pmatrix} y_1 & y_2 & \\cdots & y_{\\mathrm{m_{test}-1}} & y_{\\mathrm{m_{test}}} \n",
    "                 \\end{pmatrix}^T \\nonumber\n",
    "     \\end{eqnarray}$\n",
    "\n",
    "* Apply <font color=\"green\"><b>forward propagation</b></font> in (matrix) form (efficiency reasons):\n",
    "  - $\\begin{eqnarray}\n",
    "        \\mathbf{z} & = & \\mathbf{X.\\widehat{w}} + \\mathbf{\\widehat{b}} \\nonumber\n",
    "     \\end{eqnarray}$<br>\n",
    "     where:\n",
    "     + $\\mathbf{X} \\in \\mathbb{R}^{m_{\\mathrm{test}} \\times n}$\n",
    "     + $\\mathbf{\\widehat{w}}$ is a column vector with the <font color=\"green\"><b>optimized weights</b></font> ($ \\in \\mathbb{R}^{n \\times 1}$)  \n",
    "     + $\\mathbf{\\widehat{b}}$ is a column vector with the <font color=\"green\"><b>optimized bias</b>\n",
    "     </font> ($\\widehat{b}$) ($\\widehat{b}\\,\\mathbf{1} \\in \\mathbb{R}^{n \\times 1}$)\n",
    "     + $\\mathbf{z} \\in \\mathbb{R}^{m_{\\mathrm{test}} \\times 1}$\n",
    "  - $\\begin{eqnarray}\n",
    "        \\mathbf{a} & = & \\sigma(\\mathbf{{z}}) \\nonumber \n",
    "     \\end{eqnarray} \\;,\\;\\mathbf{a} \\in \\mathbb{R}^{m_{\\mathrm{test}} \\times 1}$    \n",
    "* The elements that are calculated ($\\widehat{\\mathbf{y}}:=\\mathbf{a}$) are in the interval $[0,1]$.<br>\n",
    "  - In order to compare them with the <font color=\"green\"><b>test labels</b></font>, we must <font color=\"green\"><b>map/discretize</b></font> the elements of $\\mathbf{a}$ into $\\{0,1\\}$.\n",
    "  - $\\widetilde{y_i} = F(\\widehat{\\mathbf{y_i}})$ where\n",
    "    $F(\\widehat{\\mathbf{y_i}})$ is defined in the following way:<br>\n",
    "     $\\begin{equation}\n",
    "      \\widetilde{y_i}= \n",
    "\\begin{cases}\n",
    "    1,& \\text{if } a_i \\geq 0.5\\\\\n",
    "    0,              & \\text{otherwise}\n",
    "\\end{cases} \\nonumber\n",
    "     \\end{equation}$    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544c316e-26f2-42c8-8b25-cf6e469c6bdf",
   "metadata": {},
   "source": [
    "#### **Exercise 7**: predict labels\n",
    "\n",
    "Steps:\n",
    "* Calculate $\\mathbf{A}$ using $\\mathbf{W}$ and $b$ (obtained from training)\n",
    "* Map all the elements of A to either $0$ or $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0e3685-4002-4c75-850a-dfafe459d77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 7:\n",
    "def predict_labels(X: np.ndarray, W: np.ndarray, b: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Make predictions using the trained model.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): The input data (features)  -> shape(m,n) \n",
    "                        where m is #samples & n is #features.\n",
    "        W (np.ndarray): The weight vector          -> shape(n,1).\n",
    "        b (float)     : The bias term              -> float\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: The predicted labels (0 or 1).\n",
    "    \"\"\"\n",
    "    # Here comes the code to predict the labels (either 0 or 1)\n",
    "    # A = <--- YOUR CODE \n",
    "    return np.where(A >= 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2f63b0-7393-45bd-bc2b-84e5e2fd57aa",
   "metadata": {},
   "source": [
    "##### <font color=\"blue\"><b>Simple check of the function:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0195cecd-8d89-49cd-a3cb-b0fac34b7026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tests/lec1/test_ex7.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3860a623-06d5-44a6-a103-bc8767774ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/lec1/sol_ex7.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f318f7ac-6cb0-4028-9947-1a3fbe3b705b",
   "metadata": {},
   "source": [
    "## 9.2.Accuracy\n",
    "\n",
    "<font color=\"green\"><b>Accuracy</b></font> is defined as the <font color=\"green\"><b>ratio</b></font> of:\n",
    "- the number of correct classifications to \n",
    "- the total number of classifications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8be1233-a266-4004-9c5e-48b76703fdb7",
   "metadata": {},
   "source": [
    "#### **Exercise 8**: Accuracy\n",
    "\n",
    "Step:\n",
    "* return ratio (number matches/total number) * 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136f7279-6325-44da-bba2-584ea0e92fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 8:\n",
    "def accuracy(Y_true: np.ndarray, Y_pred: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the accuracy of the predictions.\n",
    "    \n",
    "    Args:\n",
    "        Y_true (np.ndarray): The true labels      -> shape(m,1)\n",
    "        Y_pred (np.ndarray): The predicted labels -> shape(m,1)\n",
    "    \n",
    "    Returns:\n",
    "        float: The accuracy as a percentage.\n",
    "    \"\"\"\n",
    "    # return <--- YOUR CODE: Here comes the ratio * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879fca3c-1924-42ec-9d6f-0adead232db5",
   "metadata": {},
   "source": [
    "##### <font color=\"blue\"><b>Simple check of the function:</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f2734d-3970-42fc-9941-9b59a2bdf8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tests/lec1/test_ex8.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5623ee01-0dc4-4cbc-be12-785a17f8fb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/lec1/sol_ex8.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b207e9-21c9-4769-bb9c-702e8ba9e64c",
   "metadata": {},
   "source": [
    "# 10.In praxi: Train a binary classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363d351d-bce3-4c14-b6ef-0ddb16b4e27d",
   "metadata": {},
   "source": [
    "## Generation of a synthetic data set\n",
    "* The python library <a href=\"https://scikit-learn.org/stable/\"><b>scikit-learn</b></a> (based on NumPy & SciPy) is used to generate a <font color=\"green\"><b>synthetic data set</b></font>.\n",
    "* In order to facilitate the visualization we will only choose 2 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621efdde-f676-490a-bcbf-00767fc00f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to generate a data set\n",
    "X, y = make_moons(n_samples=500, noise=0.25, random_state=42)\n",
    "print(f\"  X.shape:{X.shape}\")\n",
    "print(f\"  y.shape:{y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff183569-7aee-44ae-83ba-ca1c21783ecf",
   "metadata": {},
   "source": [
    "## 10.1.Splitting the data set\n",
    "The data set will be split (using scikit-learn) into:\n",
    "- <font color=\"green\"><b>training set</b></font> \n",
    "- <font color=\"green\"><b>test set</b></font>\n",
    "\n",
    "<font color=\"orangered\"><b>Note:</b></font>\n",
    "* Normally, we will also create a <font color=\"green\"><b>dev/validation</b></font> set. In <a href=\"./lecture2.ipynb\"><b>Lecture 2</b></a>, we will elaborate on the use of a validation set.\n",
    "* Make sure that the training, validation and test sets belong to the <font color=\"green\"><b>same distribution</b></font>.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2803a3b-2fe2-496c-8a8a-4fbc047edfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to split the data in training and a test set.\n",
    "test_ratio = 0.30\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_ratio, random_state=42)\n",
    "print(f\"Splitting the data set ...\")\n",
    "print(f\"  Test ratio:{test_ratio}\")\n",
    "print(f\"  Training Data Set:\")\n",
    "print(f\"    X_train.shape :: {X_train.shape}\")\n",
    "print(f\"    y_train.shape :: {y_train.shape}\")\n",
    "print(f\"  Test Data Set:\")\n",
    "print(f\"    X_test.shape  :: {X_test.shape}\")\n",
    "print(f\"    y_test.shape  :: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb85cdcf-5852-4646-ba74-40443ae78935",
   "metadata": {},
   "source": [
    "## 10.2. Visualization of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af698f16-337b-4698-9dcc-31b45f5b54f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the training set\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='RdBu')  \n",
    "plt.title(\"Synthetic (Training) Data (2 Features)\")\n",
    "plt.xlabel(r\"$x_1$\")\n",
    "plt.ylabel(r\"$x_2$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d91b58c-8dcd-4bfa-8a41-16ecc125d64d",
   "metadata": {},
   "source": [
    "## 10.3. Train the model using your code\n",
    "\n",
    "* The cost function ($\\mathcal{C}$) is <a href=\"convexity.ipynb\"><b>convex</b></a> which implies that the local minimum is also the global minimum.\n",
    "* We have only <font color=\"green\"><b>one</b></font> hyperparameter (the learning rate $\\alpha$). So, how to choose it?\n",
    "  + a <font color=\"red\"><b>too large</b></font> value of $\\alpha$ will lead to moving around the minimum but not reaching it.\n",
    "  + a <font color=\"red\"><b>too small</b></font> value of $\\alpha$ will lead to extemely small step sizes and will make the convergency process very long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d12c7a7-6445-47df-b7f8-b8406512a3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstCost, W, b = train_model(X=X_train, Y=y_train[:,np.newaxis], num_epochs=20000, lr=0.05)\n",
    "# If you have errors load the following module\n",
    "#import binclas as bc\n",
    "#lstCost, W,b = bc.train_model(X=X_train, Y=y_train[:,np.newaxis], num_epochs=20000, lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f645b27-b984-4bce-a3c5-ebecc902d7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Last el. of Cost array:\")\n",
    "for item in lstCost[-10:]:\n",
    "    print(f\"  Cost:{item:20.14f}\")\n",
    "print(f\"\\nWeight (optimized): {W.ravel()}\")\n",
    "print(f\"Bias (optimized)  : {b}\")\n",
    "\n",
    "# Predict Labels\n",
    "y_pred = predict_labels(X_train, W,b)\n",
    "acc = accuracy(y_pred, y_train[:,np.newaxis])\n",
    "print(f\"Accuracy          : {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2059038-8d9f-4c47-b33e-21bc35d24d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the training set\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='RdBu')  \n",
    "plt.title(\"Synthetic (Training) Data (2 Features)\")\n",
    "plt.xlabel(r\"$x_1$\")\n",
    "plt.ylabel(r\"$x_2$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd402d31-58e1-43d8-9028-23143d90e422",
   "metadata": {},
   "source": [
    "## 10.4. Training: comparison with LogisticRegression (from sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a418df2d-3198-4181-8b99-7aa7062fecb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model using different versions of sckit-learn\n",
    "# High-accuracy without L2 (term) \n",
    "model1 = LogisticRegression(penalty=None, max_iter=100000, tol=1.E-12).fit(X_train,y_train)\n",
    "print(f\"  LogisticRegression (sklearn) without L2 (Training Set) ::\")\n",
    "print(f\"    coef:{model1.coef_}\")\n",
    "print(f\"    intercept:{model1.intercept_}\")\n",
    "print(f\"    score:{model1.score(X_train,y_train):8.4f}\")\n",
    "\n",
    "model2 = LogisticRegression(max_iter=100000, tol=1.E-12).fit(X_train,y_train)\n",
    "print(f\"  LogisticRegression (sklearn) with L2 Reg. (default) (Training Set) ::\")\n",
    "print(f\"    coef:{model2.coef_}\")\n",
    "print(f\"    intercept:{model2.intercept_}\")\n",
    "print(f\"    score:{model2.score(X_train,y_train):8.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ef5a9f-e78c-4805-bd2c-2ee2e4140799",
   "metadata": {},
   "source": [
    "## 10.5. Plot training data & the decision boundary\n",
    "The point where the <font color=\"green\"><b>decision boundary</b></font> can be found, is $\\sigma(z) = \\frac{1}{2}$.<br>\n",
    "Solving for $x_2$ leads to:\n",
    "\n",
    "$\\begin{eqnarray}\n",
    "   x_2  = &  \\frac{-(\\widehat{w_1} x_1 + \\widehat{b})}{\\widehat{w_2}} \\nonumber\n",
    "\\end{eqnarray}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69abd47-6bc9-4f0f-99e7-26c91ee939a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the training set + decision boundary\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='RdBu')  \n",
    "plt.title(\"Train Data (2 Features) + decision boundary\")\n",
    "plt.xlabel(r\"$x_1$\")\n",
    "plt.ylabel(r\"$x_2$\")\n",
    "(x1_min, x1_max) = np.min(X_train[:,0]), np.max(X_train[:,0])\n",
    "delta = 0.1\n",
    "x1 = np.linspace(x1_min-delta,x1_max+delta,501)\n",
    "x2 = -(W[0]*x1 + b)/W[1]\n",
    "plt.plot(x1, x2,'g', label=r\"$x_2 = - \\frac{\\widehat{w_1}\\,x_1+\\widehat{b}}{\\widehat{w_2}}$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57d129a-8b5f-4278-b11a-6e2b54c919d3",
   "metadata": {},
   "source": [
    "## 10.6. Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97d04ee-0005-4d6b-86cd-1e9ba93a4d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here comes your code to test the model\n",
    "# Predict Labels\n",
    "y_pred = predict_labels(X_test, W,b)\n",
    "acc = accuracy(y_pred, y_test[:,np.newaxis])\n",
    "print(f\"  Accuracy:{acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3345a44-6d6a-4846-89b2-eb2ab1a35aab",
   "metadata": {},
   "source": [
    "#### Comparison with LogisticRegression (from sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a50736a-dd27-433e-8857-b52bd76504f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"  LogisticRegression (sklearn) without L2 (Test Data Set) ::\")\n",
    "print(f\"    score:{model1.score(X_test,y_test):8.4f}\")\n",
    "print(f\"  LogisticRegression (sklearn) with L2 reg. (Test Set) ::\")\n",
    "print(f\"    score:{model2.score(X_test,y_test):8.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06040257-1449-476b-a1ff-5854ef2dfb0d",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "* You have implemented the <font color=\"orangered\"><b>simplest (shallow) neural net</b></font> from scratch.\n",
    "* You have learned that <font color=\"orangered\"><b>training of a neural net</b></font> is (in general)  <br> an iterative process consisting of the following <font color=\"orangered\"><b>components</b></font>:\n",
    "  + <font color=\"orangered\"><b>forward propagation</b></font>.\n",
    "  + <font color=\"orangered\"><b>backward propagation</b></font>.\n",
    "  + <font color=\"orangered\"><b>updating parameters</b></font> using gradient descent.\n",
    "* Once the training is finished, you can <font color=\"orangered\"><b>test/validate your model</b></font>.\n",
    "* After <font color=\"orangered\"><b>testing</b></font> you can apply your model on new data (from the same distribution)/inference</b></font>.\n",
    "\n",
    "The above algorithm is the <font color=\"green\"><b>basic algorithm</b></font> for <font color=\"green\"><b>all</b></font> neural networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
