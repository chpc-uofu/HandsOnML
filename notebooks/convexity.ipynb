{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b7874c-baca-44b9-b0dd-53d66a63d342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Callable, Tuple, List\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae9df26-ce38-41d9-b2a7-4b4333d79e38",
   "metadata": {},
   "source": [
    "# Note on convex functions\n",
    "\n",
    "* <font color=\"teal\"><b>author: Wim R.M. Cardoen</b></font>\n",
    "* <font color=\"teal\"><b>e-mail: wcardoen [\\at] gmail.com</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7a356e-74b0-49b2-be09-257afe319339",
   "metadata": {},
   "source": [
    "## Topics to be discussed:\n",
    "* What is convexity?\n",
    "* How to determine convexity & properties\n",
    "* Example\n",
    "* Further reading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2b9253-3eda-45b2-b62c-61d3e8de4420",
   "metadata": {},
   "source": [
    "## Convexity\n",
    "- a function $f(\\mathbf{x})$ is <font color=\"green\"><b>convex</b></font> if and only if:<br>\n",
    "$\\begin{equation}\n",
    "      f(\\lambda \\mathbf{x_1} + (1-\\lambda) \\mathbf{x_2}) <= \\lambda f(\\mathbf{x_1}) + (1-\\lambda) f(\\mathbf{x_2})\n",
    "\\end{equation}$<br>\n",
    "  where:<br>\n",
    "    + $\\lambda \\in [0,1]$\n",
    "    + $\\mathbf{x_1}, \\mathbf{x_2} \\in \\mathbb{R}^d$\n",
    "\n",
    "- This can be easily visualized in 1D:<br>\n",
    "  $\\begin{equation}\n",
    "      f(\\lambda x_1 + (1-\\lambda) x_2) <= \\lambda f(x_1) + (1-\\lambda) f(x_2)\n",
    "\\end{equation}$\n",
    "\n",
    "i.e. when you take two points ($x_1,x_2$) belonging to a convex set, then<br> all function values $f(y)$ for the points $y$ $\\in (x_1,x_2)$ are<br>**below the straight line** spanned by the points $f(x_1)$ and $f(x_2)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6336d75b-3338-4f5c-88d0-a9198e442744",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**2 + x +2\n",
    "x = np.linspace(-3,2.5,100)\n",
    "p1 = -2.0, f(-2.0)\n",
    "p2 =  2.0, f(2.0)\n",
    "m = (p2[1]-p1[1])/(p2[0]-p1[0])\n",
    "q = p1[1] - m*p1[0]\n",
    "w = np.linspace(-3,2.5,101)\n",
    "plt.plot(x, f(x))\n",
    "plt.plot(w, m*w+q)\n",
    "plt.plot(-1.0, f(-1.0),'or')\n",
    "plt.plot( 1.0, f(1.0), 'or')\n",
    "plt.plot(-1.0, -1.0*m +q , 'og')\n",
    "plt.plot( 1.0, 1.0*m +q , 'og')\n",
    "plt.plot(p1[0], p1[1],'og')\n",
    "plt.plot(p2[0], p2[1],'og')\n",
    "\n",
    "\n",
    "plt.title(r\"Convex function $f(x)=x^2+x+2$\")\n",
    "plt.text(-2.1, 0.25, r\"$x_1$\")\n",
    "plt.text(1.9, 0.25, r\"$x_2$\")\n",
    "plt.text(-3.95, 3.85, r\"$f(x_1)$\")\n",
    "plt.text(-3.95, 7.85, r\"$f(x_2)$\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56de8818-5cc3-428b-96e5-20af1b11cce0",
   "metadata": {},
   "source": [
    "Further:\n",
    "- If $f$ is twice continuously differentiable ($C^{2}$) over an open convex set $\\mathcal{D}$,<br>\n",
    "  then $f$ is **convex** over $\\mathcal{D}$ iff $\\nabla^2 f(x) $ is **positive semi-definite** for any $x$ in $\\mathcal{D}$.\n",
    "- If a function $f$ is convex, its **local minimum** is the **global minimum**.<br>\n",
    "  <font color=\"green\"><b>Thus, convexity makes an optimization problem easy!</b></font>\n",
    "- In deep learning most problems are **NOT** convex. We have the chance to end up in a <font color=\"red\"><b>local mininum</b></font>.\n",
    "  + <font color=\"red\"><b>From a mathematical perspective this is a serious issue.</b></font>\n",
    "  + DL community: as long as people are able to generalize/learn, things are \"fine\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfca910-d460-477e-8c4e-62177f92f430",
   "metadata": {},
   "source": [
    "## Example\n",
    "$\n",
    "f(x,y):= x^2 + 2xy + 3y^2 + 2x -3y + e^x$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0ba4d4-f941-4738-93c5-5c29b7d763ef",
   "metadata": {},
   "source": [
    "The gradient is given by:<br>\n",
    "$\\nabla f := \\begin{bmatrix} \n",
    "               2x + 2y + 2 + e^x \\\\\n",
    "               2x + 6y -3\n",
    "            \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed6fc97-f3ad-43f3-b1e6-98ba7a4cfdb1",
   "metadata": {},
   "source": [
    "The Hessian is given by:<br>\n",
    "$\\nabla^2 f := \\begin{bmatrix} 2 + e^x  &  2 \\\\\n",
    "                               2        &  6 \n",
    "               \\end{bmatrix}$  \n",
    "\n",
    "If we set $e^x:=\\alpha > 0$, we get:<br>\n",
    "$\\nabla^2 f := \\begin{bmatrix} 2 + \\alpha  &  2 \\\\\n",
    "                               2        &  6 \n",
    "               \\end{bmatrix}$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f9edf1-42b1-4007-8c18-20f54592d493",
   "metadata": {},
   "source": [
    "We will prove that $\\nabla^2 f(x,y)$ is positive semidefinite over $\\mathbb{R}^2$, i.e.,\n",
    "the **eigenvalues** of $\\nabla^2 f(x,y) $ are positive.<br>\n",
    "\n",
    "- To obtain the eigenvalues of $\\nabla^2 f(x,y)$ , we need to solve the following characteristic equation for $\\lambda$:<br>\n",
    "  $ (2 + \\alpha - \\lambda)(6-\\lambda) -4 = 0$\n",
    "- After a simple rearrangement, we obtain the following roots:<br>\n",
    "  $ \\lambda_{1,2} = \\frac{1}{2}\\Big[(8+\\alpha)\\pm \\sqrt{(\\alpha-4)^2 +16} \\Big]$\n",
    "\n",
    "- Because $\\alpha\\ge 0$<br>\n",
    "  + $ \\lambda_1 = \\frac{1}{2}\\Big[(8+\\alpha)+ \\sqrt{(\\alpha-4)^2 +16} \\Big]$ is strictly positive\n",
    "  + $(8+\\alpha)^2 > (\\alpha-4)^2 +16$ therefore,<br>\n",
    "    $ \\lambda_2 = \\frac{1}{2}\\Big[(8+\\alpha)- \\sqrt{(\\alpha-4)^2 +16} \\Big]$ is strictly positive\n",
    "    \n",
    "- Therefore, the function $f(x,y)$ is convex over $\\mathbb{R}^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abb62ff-a67d-4eb1-8472-a61d25c709b3",
   "metadata": {},
   "source": [
    "### Locating the minimum using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea05e772-858e-4455-9622-ec6b8f281434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x: float,y: float) -> float:\n",
    "    \"\"\"\n",
    "    Computes the mathematical expression:\n",
    "        f(x,y):= x**2 + 2*x*y + 3*y**2 + 2*x - 3*y + exp(x)\n",
    "\n",
    "    Parameters:\n",
    "    x (float): The first coord.\n",
    "    y (float): The second coord.\n",
    "\n",
    "    Returns:\n",
    "    float: The result of the evaluated expression.\n",
    "    \"\"\"\n",
    "    return x**2 + 2*x*y +3*y**2 +2*x -3*y + math.exp(x)\n",
    "\n",
    "def fder(x: float, y:float) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Computes the partial derivatives of a specific multivariable function with respect to x and y.\n",
    "\n",
    "    The function returns a tuple representing the gradient vector:\n",
    "        (∂f/∂x, ∂f/∂y)\n",
    "    where:\n",
    "        ∂f/∂x = 2*x + 2*y + 2 + exp(x)\n",
    "        ∂f/∂y = 2*x + 6*y - 3\n",
    "    \n",
    "    Parameters:\n",
    "        x (float): The x-coordinate input.\n",
    "        y (float): The y-coordinate input.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[float, float]: A tuple containing the partial derivatives with respect to x and y.\n",
    "    \"\"\"\n",
    "    return (2*x + 2*y+2 + math.exp(x), 2*x + 6*y -3)\n",
    "\n",
    "def findmin( f: Callable[[float, float], float],\n",
    "             fder: Callable[[float, float], Tuple[float, float]],\n",
    "             start: Tuple[float, float],\n",
    "             ALPHA: float = 0.005,\n",
    "             CONV: float = 1.0E-10,\n",
    "             INTER: int = 50\n",
    "           ) -> Tuple[List[List[float]], List[float], List[List[float]]]:\n",
    "    \n",
    "    \"\"\"\n",
    "    Performs gradient descent to find the minimum of a two-variable function.\n",
    "\n",
    "    Parameters:\n",
    "      f (Callable[[float, float], float]): The function to minimize. It should take two floats (x, y) and return a float.\n",
    "      fder (Callable[[float, float], Tuple[float, float]]): The gradient (partial derivatives) of the function.\n",
    "      start (Tuple[float, float]): The starting coordinates (x, y) for the descent.\n",
    "      ALPHA (float, optional): The learning rate or step size. Default is 0.005.\n",
    "      CONV (float, optional): The convergence threshold for the change in function value. Default is 1.0E-10.\n",
    "      INTER (int, optional): The interval at which to print progress updates. Default is 50.\n",
    "\n",
    "    Returns:\n",
    "      Tuple[List[List[float]], List[float], List[List[float]]]: \n",
    "        - A list of coordinate pairs visited during descent.\n",
    "        - A list of function values at those coordinates.\n",
    "        - A list of gradient vectors at those coordinates.\n",
    "    \"\"\"\n",
    "    lstCoord, lstFunc, lstFuncder = [], [], []\n",
    "    it, isConverged = 0, False\n",
    "    \n",
    "    xs, ys = start\n",
    "    f_old = f(xs,ys) \n",
    "    fder_x, fder_y = fder(xs,ys)\n",
    "    \n",
    "    lstCoord.append([xs,ys])\n",
    "    lstFunc.append(f_old)\n",
    "    lstFuncder.append([fder_x, fder_y])\n",
    "    print(f\"Iter:{it:4d} x:{xs:12.8f}  y:{ys:12.8f} f:{f_old:14.10f}\")\n",
    "    \n",
    "    while(not(isConverged)):\n",
    "\n",
    "       # Update coord, fval, fder\n",
    "       it +=1\n",
    "       xs -= ALPHA*fder_x\n",
    "       ys -= ALPHA*fder_y\n",
    "       f_new = f(xs,ys)\n",
    "       fder_x, fder_y = fder(xs,ys)\n",
    "\n",
    "       # Add the new points to list\n",
    "       lstCoord.append([xs,ys])\n",
    "       lstFunc.append(f_new)\n",
    "       lstFuncder.append([fder_x, fder_y])\n",
    "        \n",
    "       if it%INTER == 0 :\n",
    "           print(f\"Iter:{it:4d} x:{xs:12.8f}  y:{ys:12.8f} f:{f_new:14.10f}\") \n",
    "       if math.fabs(f_new-f_old)<CONV:\n",
    "           isConverged = True\n",
    "           print(f\"Iter:{it:4d} x:{xs:12.8f}  y:{ys:12.8f} f:{f_old:14.10f} => CONVERGED\")\n",
    "           return (lstCoord, lstFunc, lstFuncder)\n",
    "       f_old = f_new    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed40241-e667-4171-a0ea-91d2bd845f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find. the minimum\n",
    "(lstCoord, lstFunc, lstFuncder) = findmin(f,fder,(0.,0))\n",
    "coordArr = np.array(lstCoord)\n",
    "fvalArr = np.array(lstFunc)\n",
    "fderArr = np.array(lstFuncder)\n",
    "\n",
    "# Extract a slice of the Coord, Fval and the Gradients\n",
    "SZ = 25\n",
    "coordRArr = np.vstack((coordArr[0::SZ,:], coordArr[-1,:]))\n",
    "fvalRArr = np.hstack((fvalArr[0::SZ], fvalArr[-1]))\n",
    "fderRArr = np.vstack((fderArr[0::SZ,:], fderArr[-1,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99826a6f-2f33-4bb2-a5d7-36a0d85b5b53",
   "metadata": {},
   "source": [
    "### Plotting the path to the minimum (including the gradient vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93dbbef-6739-457a-bcd0-a368465434fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "(XMIN, XMAX) = -2.5, 1.0\n",
    "(YMIN, YMAX) = -1.0, 2.5\n",
    "x = np.linspace(XMIN, XMAX, 25)\n",
    "y = np.linspace(YMIN, YMAX, 25)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = X**2 + 2*X*Y + 3*Y**2 + 2*X -3*Y + np.exp(X)\n",
    "\n",
    "# Compute gradients\n",
    "dZ_dx, dZ_dy = np.gradient(Z, x, y)\n",
    "\n",
    "# Create the contour plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "contours = plt.contour(X, Y, Z, levels=25, cmap='viridis')\n",
    "plt.clabel(contours, inline=True, fontsize=8)\n",
    "\n",
    "# Overlay gradient vectors\n",
    "plt.quiver(coordRArr[:,0], coordRArr[:,1], fderRArr[:,0], fderRArr[:,1], color='red', scale=50)\n",
    "\n",
    "plt.plot(coordRArr[0,0],coordRArr[0,1],'ok')\n",
    "plt.plot(coordRArr[:,0],coordRArr[:,1],'x--g')\n",
    "plt.plot(coordRArr[-1,0],coordRArr[-1,1],'ob')\n",
    "\n",
    "plt.title(f\"Contour Plot with Gradient Vectors\\n\" + f\"Start: ({coordRArr[0,0]:6.4},{coordRArr[0,1]:6.4})\\n\" +\n",
    "          f\"Min.: ({coordRArr[-1,0]:6.4},{coordRArr[-1,1]:6.4})\")\n",
    "plt.xlabel(r\"$x$\")\n",
    "plt.ylabel(r\"$y$\",rotation=0)\n",
    "plt.axis([XMIN,XMAX,YMIN,YMAX])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a33f1d9-1738-4e41-9fec-997acf1628ed",
   "metadata": {},
   "source": [
    "Note: from the above plot we can see that:\n",
    "- the gradient is the biggest in the beginning and goes to $0$ at the minimum\n",
    "- the \"marching\" direction is **opposite** to the direction of the gradient.\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
