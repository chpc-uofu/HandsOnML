{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cb4c0a-e099-446b-9097-6e31656f66e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Tuple, List, Dict, Any\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sklearn\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "# Keras (Torch) as backend\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"   # IF NOT SPECIFIED, TensorFlow will be used as BACKEND\n",
    "import keras\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23c4867-854a-458e-bed4-c18e4caaf354",
   "metadata": {},
   "source": [
    "# Logistic regression using Keras & PyTorch\n",
    "* <font color=\"teal\"><b>author: Wim R.M. Cardoen</b></font>\n",
    "* <font color=\"teal\"><b>e-mail: wcardoen [\\at] gmail.com</b></font>\n",
    "\n",
    "* For <font color=\"green\"><b>more advanced networks</b></font> most AI/deep learing practitioners:\n",
    "  - neither derive the underlying equations.\n",
    "  - nor implement these equations from scratch.\n",
    "* Implementing <font color=\"green\"><b>highly performant code</b></font>\n",
    "  for the general case is significantly more <font color=\"green\"><b>demanding and time-consuming</b></font>.<br>It requires:\n",
    "  - mastery of a <font color=\"red\"><b>compiled language</b></font> (e.g., `C++`)\n",
    "  - understanding of <font color=\"red\"><b>parallel computing</b></font> (multi-node, multi-GPU setups).\n",
    "  - a solid foundation in <font color=\"red\"><b>algorithms and numerical analysis</b></font>.\n",
    "* To address these kind of challenges, humanity adopted over time a\n",
    "  <a href=\"https://www.marxists.org/reference/archive/smith-adam/works/wealth-of-nations/book01/ch01.htm\"><b>division of labour</b></a>.   \n",
    "* Instead of building everything from scratch, practitioners rely on <font color=\"green\"><b>frameworks</b></font>.<br>The most commonly used frameworks are currently:\n",
    "  - <a href=\"https://pytorch.org/\"><b>PyTorch</b></a>\n",
    "  - <a href=\"https://www.tensorflow.org/\"><b>TensorFlow</b></a>\n",
    "  - <a href=\"https://docs.jax.dev/en/latest/\"><b>Jax</b></a>\n",
    "  - <a href=\"https://keras.io/\"><b>Keras</b></a>\n",
    "  \n",
    "Our goal is to implement the logistic regression model (<a href=\"./lecture1.ipynb\"><b>Lecture 1</b></a>) using Keras & PyTorch.\n",
    "\n",
    "We will proceed in two different ways:\n",
    "1. by using <font color=\"green\"><b>Keras</b></font> (PyTorch as backend) : more user-friendly \n",
    "2. by using <font color=\"green\"><b>PyTorch</b></font> as such: low-level but versatile.\n",
    "\n",
    "In <a href=\"./lecture2.ipynb\"><b>Lecture 2</b></a> we will use Keras but will also provide its lower level counter part (as addendum)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeac246-3cb3-4ca6-bed2-976bc3d82e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a data set\n",
    "X, y = make_moons(n_samples=500, noise=0.25, random_state=42)\n",
    "print(f\"Generate the data set ...\")\n",
    "print(f\"  X.shape:{X.shape}\")\n",
    "print(f\"  y.shape:{y.shape}\")\n",
    "\n",
    "# Split the data in training and a test set.\n",
    "test_ratio = 0.30\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_ratio, random_state=42)\n",
    "print(f\"Splitting the data set (splitting ratio:{test_ratio})\")\n",
    "print(f\"  Training Data Set:\")\n",
    "print(f\"    X_train.shape : {X_train.shape}\")\n",
    "print(f\"    y_train.shape : {y_train.shape}\")\n",
    "print(f\"  Test Data Set:\")\n",
    "print(f\"    X_test.shape  : {X_test.shape}\")\n",
    "print(f\"    y_test.shape  : {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5963890e-8640-428e-b3f9-c4885cd489d5",
   "metadata": {},
   "source": [
    "# 1.Keras (with PyTorch backend)\n",
    "Phases:<br>\n",
    "- A. set up the model: specify the layers and units\n",
    "- B. `compile` the model: specify the optimizer, loss, metrics\n",
    "- C. `fit` the model: training\n",
    "- D. `evaluate` the model: testing\n",
    "- E. `predict`: use the model to predict outcomes for input samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bbf9fe-28ad-47b3-bebe-cbd3eb211a5e",
   "metadata": {},
   "source": [
    "### A.Set up the model\n",
    "consists mainly of:\n",
    "- providing the layout of the neural net (layers and #units/layer)\n",
    "- specifying the activation functions for each layer\n",
    "  \n",
    "`Sequential class`: groups a linear stack of layers into a Model (For more info on the API, see <a href=\"https://keras.io/api/models/sequential/\"><b>here</b></a>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9fe847-3467-425e-ae0c-31fe76d5deb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the model\n",
    "model1 =  keras.Sequential([\n",
    "                keras.layers.Input(shape=(2,)),                      # Input layer: input vector (2 features)\n",
    "                keras.layers.Dense(units=1, activation='sigmoid')],  # Output layer: 1 Class\n",
    "                name=\"LogisticRegression\")       \n",
    "print(model1.summary())\n",
    "\n",
    "# Info on kernel_regularization, etc.\n",
    "print(f\"Info on the layers ...\")\n",
    "for layer in model1.layers:\n",
    "    print(f\"  Layer:'{layer.name}'\")\n",
    "    print(layer.get_config())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c6d5b4-eba9-40ea-8ffd-233ab0fe1207",
   "metadata": {},
   "source": [
    "### B.Compile the model\n",
    "\n",
    "The `compile` method's task it to provide the model with:\n",
    "- an <font color=\"teal\"><b>optimization method/optimizer</b></font> (e.g. `SGD`, `RMSprop`,`Adam`, `AdamW`, $\\ldots$)<br>\n",
    "  (For more info on the optimizers, see <a href=\"https://keras.io/api/optimizers/\"><b>here</b></a>)\n",
    "- a <font color=\"teal\"><b>loss</b></font> function (e.g. `binary_crossentropy`, `categorical_crossentropy`, `mean_squared_error` , $\\ldots$)<br>\n",
    "  (For more info on the loss function, see <a href=\"https://keras.io/api/losses/\"><b>here</b></a>)\n",
    "- <font color=\"teal\"><b>metrics</b></font> (e.g. `accuracy` $\\ldots$)<br>\n",
    "  (For more info on the metrics, see <a href=\"https://keras.io/api/metrics/\"><b>here</a></a>)\n",
    "\n",
    "`compile`: (For more info on the method, see <a href=\"https://keras.io/api/models/model_training_apis/#compile-method\"><b>here</b></a>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f8fd65-9f89-451e-a7d2-0a8e8ef41bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilation of the model\n",
    "optimizer = SGD(learning_rate=0.075)\n",
    "model1.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(f\"Compilation models\")\n",
    "print(f\"  Optimizer: {model1.optimizer}\")\n",
    "print(f\"  Optimizer Config: {model1.optimizer.get_config()}\")\n",
    "print(f\"  Loss Function: {model1.loss}\")\n",
    "print(f\"  Metrics: {model1.metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad52b256-5d8f-49e0-926f-d543b6529c7d",
   "metadata": {},
   "source": [
    "### C.Train the model\n",
    "- The `fit` method is used to train the model.<br>\n",
    "  (For more info on the method, see <a href=\"https://keras.io/api/models/model_training_apis/#fit-method\"><b>here</b></a>)\n",
    "- This method has important arguments, e.g.:<br>\n",
    "  * `epochs`: number of passes through the **complete** data set.\n",
    "  * `batch_size`: number of samples used per gradient update. (Default: $[32]$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feac98c2-de9f-44a0-92ee-c44bae91927b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "history = model1.fit(X_train, y_train, epochs=1000, verbose=0)\n",
    "#print(history.history)\n",
    "loss = history.history['loss']\n",
    "accuracy=history.history['accuracy']\n",
    "it = np.arange(len(loss))+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03202081-e0d0-46f2-8d61-33ebfb4af09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Loss of the training data\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(it,loss,\"-\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad15a525-a289-4c80-9d1f-befd5e2932b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final weights and bias\n",
    "print(f\"Parameters at the end of the training\")\n",
    "for layer in model1.layers:\n",
    "    weights, biases = layer.get_weights()\n",
    "    print(f\"  Layer   : {layer.name}\")\n",
    "    print(f\"  Weights : {np.ravel(weights)}\")\n",
    "    print(f\"  Bias    : {biases}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996cd386-c412-4b3a-b8a5-54b2c421fdd0",
   "metadata": {},
   "source": [
    "### D.Test the model\n",
    "\n",
    "The `evaluate` method returns the `loss` value & `metrics` values for the model in test mode.<br>\n",
    "(For more info on the method, see <a href=\"https://keras.io/api/models/model_training_apis/#evaluate-method\"><b>here</b></a>)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac992dde-fc44-4884-8e59-97b9ee777ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model1.evaluate(X_test, y_test)\n",
    "print(f\"Evaluation of the test set\")\n",
    "print(f\"  Accuracy : {test_accuracy:8.4f}\")\n",
    "print(f\"  Loss     : {test_loss:8.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a68544-7138-47f4-8d3f-a8811575b554",
   "metadata": {},
   "source": [
    "### E.Predict outcomes\n",
    "\n",
    "The `predict` method generates output predictions for the input samples.<br>\n",
    "(For more info on the method, see <a href=\"https://keras.io/api/models/model_training_apis/#predict-method\"><b>here</b></a>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d650dd-5598-45dd-b4b7-afbe7a9339a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a data set\n",
    "print(f\"Generate a sample of X values ...\")\n",
    "X_sample, y_sample = make_moons(n_samples=100, noise=0.25, random_state=12)\n",
    "print(f\"  X.shape:{X_sample.shape}\")\n",
    "y_pred = model1.predict(X_sample,verbose=0)\n",
    "y_pred = np.where(y_pred>0.5,1,0).squeeze()\n",
    "print(f\"  #same  :{np.sum(y_pred ==y_sample)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3a4da2-c8cb-457d-b510-2d557769b175",
   "metadata": {},
   "source": [
    "# 2.PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2ba59b-d299-49d1-9f0e-ee546657913c",
   "metadata": {},
   "source": [
    "### Data conversion/Data Loaders\n",
    "* We have 2 PyTorch data classes (to be discussed later):\n",
    "  + data.Dataset : to load/create data in a class<br>\n",
    "    requires: \\_\\_init\\_\\_(), \\_\\_len\\_\\_(), \\_\\_getitem\\_\\_()\n",
    "  + data.DataLoader:: to load data in batches\n",
    "* For the time being (conversion to PyTorch Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138b0abe-2f46-4349-a7ab-7d530a673784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the PyTorch Tensors from the NumPy Data\n",
    "# Note: default torch.float32\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1,1)  # 2D \n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1,1)    # 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593a282d-7154-4cfc-8af1-021cd7e8f626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Conversion from NumPy to PyTorch Tensor\n",
    "START, END= 0, 2\n",
    "print(f\"NumPy data::\")\n",
    "print(f\"  X_train: {X_train.shape}\\n{X_train[START:END]}\")\n",
    "print(f\"  y_train: {y_train.shape}\\n{y_train[START:END]}\")\n",
    "print(f\"\\nPyTorch data::\")\n",
    "print(f\"  X_train_tensor: {X_train_tensor.shape}\\n{X_train_tensor[START:END]}\")\n",
    "print(f\"  y_train_tensor: {y_train_tensor.shape}\\n{y_train_tensor[START:END]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc197963-abb7-4e21-a87a-1313a5c318a6",
   "metadata": {},
   "source": [
    "### A.Setup the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872f0b4c-90c6-42bd-8999-39f4c3bdb1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logistic Regression module using PyTorch\n",
    "class LogisticRegressionModel2(nn.Module):\n",
    "\n",
    "    def __init__(self, num_inputs):\n",
    "\n",
    "        # The class inherits from the class nn.Module\n",
    "        super(LogisticRegressionModel2,self).__init__()\n",
    "\n",
    "        # Define a Single LAYER object which connects \n",
    "        #     the input with 1 single output \n",
    "        self.linear = nn.Linear(num_inputs, 1)\n",
    "\n",
    "        # Create the ACTIVATION (object) for the Single Layer\n",
    "        self.act_fn = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Applies the forward propagation\n",
    "        z = self.linear(x)\n",
    "        a = self.act_fn(z)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce21371-e24d-4ad9-b175-7e5877c0ec59",
   "metadata": {},
   "source": [
    "<font color=\"red\"><b>Note</b></font>:\n",
    "* `nn.Linear(in_features, out_features)`:\n",
    "   <a href=\"https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html\"><b>affine transformation</b></a>\n",
    "* `nn.Sigmoid()`: <a href=\"https://docs.pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html#torch.nn.Sigmoid\"><b>sigmoid function</b></a>\n",
    "* $\\ldots$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33762e7-caa2-41c5-85bb-d3af7dc7e35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = LogisticRegressionModel2(num_inputs=2)\n",
    "print(f\"  Logistic Model:{model2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f833ee-138a-4c71-abe3-82b1e5aec1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In PyTorch, you can use either the parameters() function\n",
    "# or the names_parameters() function\n",
    "for name, param in model2.named_parameters():\n",
    "    print(f\"Name:{name:20s} -> param:{param.shape}\")\n",
    "    print(f\"{param.data}\\n\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2428beba-5dea-48ae-9d5c-ed0ab1401400",
   "metadata": {},
   "source": [
    "### B.Compiling the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e37927-445f-4aac-9c32-ab9145f1cbfb",
   "metadata": {},
   "source": [
    "#### B1.Loss/Objective function\n",
    "* In order to find the <font color=\"green\"><b>optimal parameters</b></font> for the weights and bias, we need\n",
    "  to have an <font color=\"green\"><b>objective function</b></font> (a.k.a Loss function)\n",
    "* There are several options:\n",
    "  + <a href=\"https://docs.pytorch.org/docs/stable/generated/torch.nn.BCELoss.html#bceloss\"><b>nn.BCELoss()</b></a>:<br>\n",
    "  **B**inary **C**ross **E**ntropy => inputs need to be $[0,1]$<br>\n",
    "  $\\begin{eqnarray}\n",
    "  \\mathcal{L}^{(i)} & = & -\\bigg [ y_i \\log(a_i) + (1-y_i)\\log(1-a_i) \\bigg ]\n",
    "  \\end{eqnarray}$\n",
    "  + <a href=\"https://docs.pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss\"><b>nn.BCEWithLogitLoss()</b></a>:<br> Numerically more stable because of the combination of sigmoid and loss function at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17cb990-6c6d-4b2d-8726-f5e8ef3623d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates an instance of the Binary Cross Entropy Criterion\n",
    "loss_fn2 = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24295432-ce38-4ebd-9557-3b0ff81d049a",
   "metadata": {},
   "source": [
    "#### B2.Optimization\n",
    "* There are several methods to <font color=\"green\"><b>optimize</b></font> the Loss function/Objective function.<br>\n",
    "  - In this example we will use the <a href=\"https://docs.pytorch.org/docs/stable/generated/torch.optim.SGD.html#sgd\"><b>Stochastic Gradient Descent (SGD)</b></a> method<br>\n",
    "    (<a href=\"https://docs.pytorch.org/docs/stable/optim.html\"><b>torch.optim module</b></a>).\n",
    "  - Later on, we will describe more powerful <font color=\"green\"><b>optimization algorithms</b></font> (`Adam`, `AdamW`, ...).\n",
    "* Useful methods:\n",
    "  + step(): method update parameters\n",
    "  + zero_grad() : sets the gradients of ALL optimized parameters to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00445b61-2484-4c71-8f76-d8c54758efc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim2 = optim.SGD(model2.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd6747b-5400-4995-90e1-718fc0dcd6e2",
   "metadata": {},
   "source": [
    "### C.Train the model\n",
    "* Goal: Obtain the optimized parameters i.e. <font color=\"green\"><b>weight matrix</b></font> and <font color=\"green\"><b>bias</b></font>\n",
    "* If the data set is **small**, then we will use **all** the training data at **once**.\n",
    "* Terminology:\n",
    "  - **One** complete iteration over **all** training data: <font color=\"green\"><b>epoch</b></font>\n",
    "  - For **larger** training data sets, each <font color=\"green\"><b>epoch</b></font> is split into <font color=\"green\"><b>batches</b></font>.<br>\n",
    "    * The gradient and the parameters are <font color=\"blue\"><b>updated</b></font> after every batch (points are selected <font color=\"blue\"><b>randomly</b></font>):<br>\n",
    "      <font color=\"green\"><b>stochastic gradient descent (SGD)</b></font>\n",
    "    * The batch size is a <font color=\"green\"><b>hyperparameter</b></font>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f75a1e4-75c4-47e7-9715-9f03657a6fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train_tensor, y_train_tensor, model, loss_fn, \n",
    "                optim, num_epochs=100000 , delta_print=10000):\n",
    "    \"\"\"\"\n",
    "    Function which trains the model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set model to train mode\n",
    "    # Strictly not necessary for our case \n",
    "    model.train()\n",
    "\n",
    "    # Loop over the epochs\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # PART A: FORWARD PROPAGATION ( => )\n",
    "        # Step 1: Generate the output (activation of the linear layer)\n",
    "        output = model(X_train_tensor)\n",
    "\n",
    "        # Step 2: Use the activation of the last layer & the labels\n",
    "        #         to calculate the loss.\n",
    "        loss = loss_fn(output, y_train_tensor)\n",
    "\n",
    "        # Step B: BACK PROPAGATION ( <= )\n",
    "        # Step 3: Calculate the gradients of the parameters\n",
    "        optim.zero_grad()   # Init. the gradients to ZERO!!\n",
    "        loss.backward()     # Calc. grad. of param.\n",
    "\n",
    "        # Step 4: Adjust the parameters \n",
    "        optim.step()\n",
    "\n",
    "        if (epoch+1)%delta_print == 0 or epoch==0:\n",
    "           print(f\"  Epoch {epoch+1}/{num_epochs}  Loss:{loss.item():.6f}\")\n",
    "                \n",
    "    return loss.item()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4deac211-6416-43c3-a296-ff7066de3eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "final_loss2 = train_model(X_train_tensor, y_train_tensor, model2, loss_fn2, optim2)\n",
    "print(f\"Loss in the last step:{final_loss2:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a76000b-3ef8-4003-9358-5dd881c31f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOW TO Check the final parameters\n",
    "#   Method 1:\n",
    "print(f\"METHOD 1::\")\n",
    "print(f\"Weights::\\n{model2.linear.weight}\\n\")\n",
    "print(f\"Bias   ::\\n{model2.linear.bias}\")\n",
    "\n",
    "print(f\"\\nMETHOD 2::\")\n",
    "for name, param in model2.state_dict().items():\n",
    "    print(f\"{name} -> {param.shape}\")\n",
    "    print(f\"  {param.data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2b6fc7-23f5-4431-9e98-56386149effd",
   "metadata": {},
   "source": [
    "#### Save/load the model to & from disk\n",
    "* To <font color=\"green\"><b>save</b></font> an object to disk, use <a href=\"https://docs.pytorch.org/docs/stable/generated/torch.save.html#torch-save\"><b>torch.save()</b></a>\n",
    "* To <font color=\"green\"><b>load</b></font> an object from disk, use <a href=\"https://docs.pytorch.org/docs/stable/generated/torch.load.html#torch-load\"><b>torch.load()</b></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb9f8f6-0fef-4da3-8159-fed99f5727eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='linreg2.pth'\n",
    "torch.save(model2.state_dict(), filename)\n",
    "\n",
    "newmodel = LogisticRegressionModel2(num_inputs=2)\n",
    "state = torch.load(filename, map_location=\"cpu\")\n",
    "newmodel.load_state_dict(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e11d92-5ea9-487a-b9bd-9c8d4278fa50",
   "metadata": {},
   "source": [
    "<font color=\"red\"><b>Important Notes:</b><font>\n",
    "* The line `output = model(X_train_tensor)` calls the **forward** method.<br>\n",
    "  How does this work?\n",
    "  - `model(X_train_tensor)` invokes the `__call__` method<br> of the base class i.e. $\\texttt{nn.Module}$\n",
    "  - The `__call__` method does 2 things:\n",
    "    + Handles hooks (e.g. for debugging)\n",
    "    + Calls the `forward` method.\n",
    "* `optim.zero_grad()`: forces to set the gradient vector to zero (accumulation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8a0f91-2301-4012-a13d-dfc560be46e2",
   "metadata": {},
   "source": [
    "### D.Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9c9247-bcb5-4291-86b3-9b6fb0f5d660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(X_tensor, y_tensor, model):\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        res_tensor = model(X_tensor)\n",
    "        ypred_tensor =(res_tensor>0.5).float()\n",
    "    return ypred_tensor    \n",
    "\n",
    "def get_accuracy(y_pred, y):\n",
    "    num_ok = float((y_pred == y).sum())\n",
    "    return (num_ok / y_pred.shape[0]) * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f80995-21d5-4917-a824-0194a47ee851",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trainpred_tensor = test_model(X_train_tensor, y_train_tensor, model2)\n",
    "acc_train = get_accuracy(y_trainpred_tensor, y_train_tensor)\n",
    "print(f\"Accuracy train:{acc_train:8.4f}\")\n",
    "\n",
    "y_testpred_tensor = test_model(X_test_tensor, y_test_tensor, model2)\n",
    "acc_test = get_accuracy(y_testpred_tensor, y_test_tensor)\n",
    "print(f\"Accuracy test:{acc_test:8.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba7f7f9-cc94-40d5-8716-1a078bd98a8d",
   "metadata": {},
   "source": [
    "### Alternative implementation within PyTorch (numerical stability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13365877-aa9a-4cb1-9ae2-f449f2e530fe",
   "metadata": {},
   "source": [
    "In the previous section, we implemented the \n",
    "* <font color=\"green\"><b>activation function</b></font> $a_i$, i.e.<br>\n",
    "  $\\begin{eqnarray}\n",
    "     a_i & = & \\sigma(z_i)\\\\\n",
    "         & = & \\frac{1}{1+e^{-z_i}} \\\\\n",
    "  \\end{eqnarray}$\n",
    "* loss function $\\mathcal{L}^{(i)}$, i.e.<br>\n",
    "  $\\begin{eqnarray}\n",
    "       \\mathcal{L}^{(i)} & = & - \\bigg [ y_i \\log(a_i) + (1-y_i)\\log(1-a_i) \\bigg ] \n",
    "  \\end{eqnarray}$<br>\n",
    "separately.\n",
    "\n",
    "To render the <font color=\"green\"><b>optimization numerically more stable</b></font> the <font color=\"green\"><b>activation and the loss function</b></font> can be combined into **one** function.<br>\n",
    "The corresponding loss function bears the name <a href=\"https://docs.pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss\"><b>BCEWithLogitsLoss</b></a>\n",
    "and is given by:\n",
    "\n",
    "$\\begin{eqnarray}\n",
    "  \\mathcal{L}^{(i)} & = & -\\bigg [ y_i \\log(a_i) + (1-y_i)\\log(1-a_i) \\bigg ]\\\\ \n",
    "                    & = & z_i(1-y_i) + \\log(1+e^{-z_i}) \\\\\n",
    "\\end{eqnarray}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6546feea-cd34-46a6-8b13-801452734d44",
   "metadata": {},
   "source": [
    "#### **Exercise 1**:\n",
    "* Implement the `class LogisticRegressionModelEx(nn.Module)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9695a53a-2235-4737-9d00-96b908d4991c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logistic Regression module using PyTorch \n",
    "class LogisticRegressionModelEx(nn.Module):\n",
    "\n",
    "    def __init__(self, num_inputs):\n",
    "\n",
    "        # The class inherits from the class nn.Module\n",
    "        super(LogisticRegressionModelEx,self).__init__()\n",
    "\n",
    "        # <--- YOUR CODE\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Applies the forward propagation\n",
    "        # <--- YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7427e650-c454-4a32-9e11-bd846b0a57d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/kerastorch/sol_ex1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d025b52c-ef20-474f-8d78-924764afaee2",
   "metadata": {},
   "source": [
    "Check the model <font color=\"blue\"><b>(ante optimization)</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0eaa0a-6a45-48e3-9e78-47028c2f58d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelEx = LogisticRegressionModelEx(num_inputs=2)\n",
    "print(f\"  Logistic Model:{modelEx}\")\n",
    "\n",
    "for name, param in modelEx.named_parameters():\n",
    "    print(f\"Name:{name:20s} -> param:{param.shape}\")\n",
    "    print(f\"{param.data}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0477716-cae5-4436-bb13-26bfa14d63f2",
   "metadata": {},
   "source": [
    "#### **Exercise 2**:\n",
    "* Implement the `loss_fnEx` using BCEWithLogitsLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588ea3a8-0d0f-4ccd-8a35-695ba436daf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here comes your code to define the BCEWithLogitsLoss\n",
    "loss_fnEx = # <--- Here comes your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a9373f-328d-4fa1-9f06-9d645ceb0a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/kerastorch/sol_ex2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f7506d-ffd3-4f42-9e5c-d61dee98da2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimEx = optim.SGD(modelEx.parameters(), lr=0.005)\n",
    "final_lossEx = train_model(X_train_tensor, y_train_tensor, modelEx, loss_fnEx, optimEx)\n",
    "print(f\"Loss in the last step:{final_lossEx:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d09f515-1065-4855-b232-b7948dedddf1",
   "metadata": {},
   "source": [
    "Check the model <font color=\"blue\"><b>(post optimization)</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07ae834-2ca9-47b8-b33b-eb7effda5f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in modelEx.named_parameters():\n",
    "    print(f\"Name:{name:20s} -> param:{param.shape}\")\n",
    "    print(f\"{param.data}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
