{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bb65ce-6231-4d1f-8692-78d1f486eb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f72d733-2165-4398-975f-c81c4b066d34",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "In order to discuss the internals of optimization methods like `Adam`, $\\ldots$<br>\n",
    "we will first elaborate on the <font color=\"green\"><b>prerequisites</b></font>:\n",
    "+ <font color=\"green\"><b>Exponentially Weighted Moving Averages</b></font> (EWMA)\n",
    "+ <font color=\"green\"><b>Momentum based gradient descent</b></font>\n",
    "+ <font color=\"green\"><b>Root Mean Square Propagation</b></font> (RMSProp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ec5593-6e84-432d-a685-6905c4027b70",
   "metadata": {},
   "source": [
    "## 1.Exponentially Weighted Moving Averages\n",
    "The technique of moving averages is commonplace in time series analysis (see e.g., Brockwell).\n",
    "* Let ${x_t}$ be a time series indexed by $t$.\n",
    "* Let $\\beta \\in (0,1)$ be a fixed `smoothing factor`.\n",
    "\n",
    "The `Exponentially Weighted Moving Average` (EWMA) $S_t$ is then given by:\n",
    "\\begin{eqnarray}\n",
    "    m_t & = & \\beta \\, m_{t-1} + (1-\\beta)\\, x_{t}\n",
    "\\end{eqnarray}\n",
    "\n",
    "Using **recursion**, we can easily derive the following relation for $m_t$:\n",
    "\\begin{eqnarray}\n",
    "  m_t = \\beta^t\\, m_0 + (1-\\beta) \\sum_{j=0}^{t-1} \\beta^j \\, x_{t-j}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5acfd70-0db8-411d-9505-36c950406ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ewma(x, beta, mstart=0.0):\n",
    "    \"\"\"\n",
    "    Computes the Exponentially Weighted Moving Average (EWMA) of a sequence.\n",
    "\n",
    "    Parameters:\n",
    "        x (array-like): Input sequence of values (e.g., temperatures).\n",
    "        beta (float): Smoothing factor between 0 and 1. Higher values give more weight to past values.\n",
    "        mstart (float, optional): Initial value for the EWMA. Default is 0.0.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Array of the same shape as `temp` containing the EWMA values.\n",
    "    \"\"\" \n",
    "    m = np.zeros(x.shape,dtype=np.float64)\n",
    "    betac = 1.0 - beta\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        if i==0:\n",
    "            m[i] = beta*mstart + betac*x[i]\n",
    "        m[i] = beta*m[i-1] + betac*x[i]\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f91da0a-18b8-4afc-ab39-bad10ad50868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read temperatures at Salt Lake City International Airport (01/01/2024 -> 12/31/2024) \n",
    "# Source: https://www.ncei.noaa.gov/cdo-web/\n",
    "df = pd.read_csv(\"../data/4148860.csv\", usecols=['DATE','TMAX'])\n",
    "first_days = df[pd.to_datetime(df['DATE']).dt.day==1]['DATE']\n",
    "date_labels = pd.to_datetime(first_days).dt.strftime(\"%m/%d\")\n",
    "\n",
    "temp = df['TMAX'].to_numpy()\n",
    "temp_smoothed1 = ewma(temp, beta=0.9)\n",
    "temp_smoothed2 = ewma(temp, beta=0.95)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(df['DATE'], df['TMAX'], '.', label=\"Max. Daily Temp.\")\n",
    "ax.plot(df['DATE'], temp_smoothed1, label=r\"EWMA ($\\beta=0.9$)\", \n",
    "        linestyle='-', linewidth=2, color=\"red\")\n",
    "ax.plot(df['DATE'], temp_smoothed2, label=r\"EWMA ($\\beta=0.95$)\", \n",
    "        linestyle=\"-\", linewidth=2, color=\"green\")\n",
    "ax.set_ylabel(r\"$T(^{\\circ}F)$\")\n",
    "ax.set_xticks(first_days)\n",
    "ax.set_xticklabels(date_labels, rotation=90)\n",
    "ax.legend()\n",
    "plt.title(r\"Max. Temp. $T(^{\\circ}F)$ in 2024 at the SLC International Airport\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d15c978-8fae-49d2-9858-123e449de805",
   "metadata": {},
   "source": [
    "### 1.1.Rate of decay \n",
    "What value of $j$ do we need for $\\beta^j$ to be of the order of $e^{-1}$?\n",
    "* Given that $ \\displaystyle \\lim_{z \\to 0} \\,(1-z)^{-1/z} = e$\n",
    "* Therefore, for small values of $1 -\\beta$, we get:\n",
    "  \\begin{eqnarray}\n",
    "    (1 -(1-\\beta))^{\\frac{1}{1-\\beta}} & \\approx & e^{-1} \\\\\n",
    "            \\beta^{\\frac{1}{1-\\beta}} & \\approx & e^{-1}\n",
    "  \\end{eqnarray}<br>\n",
    "  or: $j \\approx \\frac{1}{1-\\beta}$.\n",
    "\n",
    "### 1.2.Bias Correction \n",
    "We previously assumed that $m_0:=0$.<br>\n",
    "For <font color=\"green\"><b>small</b></font> $t$, $m_t$ is significantly underestimating the true signal.<br>\n",
    "This can be remediated by a <font color=\"green\"><b>bias correction</b></font>.\n",
    "\n",
    "For $\\beta <1$, we have the following identity:\n",
    "\\begin{eqnarray}\n",
    "   1 & = &(1-\\beta) \\sum_{j=0}^{\\infty} \\beta^j  \\\\\n",
    "     & =  &(1-\\beta) \\sum_{j=0}^{t-1} \\beta^j \\,+\\, (1-\\beta) \\sum_{j=t}^{\\infty} \\beta^j \\\\                                         & = & (1-\\beta^t) + (1-\\beta) \\sum_{j=t}^{\\infty} \\beta^j \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "In the previous result, the second term was absent (we ended the recursion at $t=1$). We can remediate for its absence  \n",
    "by scaling the first term by a factor of $K$.  \n",
    "\\begin{eqnarray}\n",
    "   1 & = & K \\, (1-\\beta^t)  \n",
    "\\end{eqnarray}\n",
    "The scaling factor $K(t;\\beta)$ thus becomes $K(t;\\beta):=\\displaystyle \\frac{1}{1 -\\beta^t}$.\n",
    "\n",
    "The scaled/bias corrected version of $m_t$, i.e. $\\widehat{m_t}$ is thus:\n",
    "\\begin{eqnarray}\n",
    "   \\widehat{m_t} &:=& K(t;\\beta) \\, m_t \\\\\n",
    "                & = & \\frac{m_t}{1-\\beta^t}\n",
    "\\end{eqnarray}\n",
    "\n",
    "From the picture below and the aforementioned equation, we immediately notice:\n",
    "- the bias' correction **major influence** at the <font color=\"green\"><b>start</b></font> of the ECWA.\n",
    "- the **almost negligent correction** for <font color=\"green\"><b>larger values</b></font> of $t$, i.e.\n",
    "  \\begin{eqnarray}\n",
    "      \\displaystyle \\lim_{t \\to \\infty} K(t;\\beta) = 1\n",
    "  \\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8653795-4659-497c-9c53-08e06a37a231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read temperatures at Salt Lake City International Airport (01/01/2024 -> 12/31/2024) \n",
    "# Source: https://www.ncei.noaa.gov/cdo-web/\n",
    "df = pd.read_csv(\"../data/4148860.csv\", usecols=['DATE','TMAX'])\n",
    "first_days = df[pd.to_datetime(df['DATE']).dt.day==1]['DATE']\n",
    "date_labels = pd.to_datetime(first_days).dt.strftime(\"%m/%d\")\n",
    "\n",
    "temp = df['TMAX'].to_numpy()\n",
    "BETA = 0.9\n",
    "temp_smoothed1 = ewma(temp, beta=BETA)\n",
    "denom = 1.0 - BETA**np.arange(1,temp_smoothed1.shape[0]+1)\n",
    "temp_smoothed2 = temp_smoothed1/denom\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(df['DATE'], df['TMAX'], '.', label=\"Max. Daily Temp.\")\n",
    "ax.plot(df['DATE'], temp_smoothed1, label=r\"EWMA ($\\beta=0.9$)\", \n",
    "        linestyle='-', linewidth=2, color=\"red\")\n",
    "ax.plot(df['DATE'], temp_smoothed2, label=r\"EWMA ($\\beta=0.9$) bias-corrected\", \n",
    "        linestyle=\"-\", linewidth=2, color=\"orange\")\n",
    "ax.set_ylabel(r\"$T(^{\\circ}F)$\")\n",
    "ax.set_xticks(first_days)\n",
    "ax.set_xticklabels(date_labels, rotation=90)\n",
    "ax.legend()\n",
    "plt.title(r\"Max. Temp. $T(^{\\circ}F)$ in 2024 at the SLC International Airport\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96b9684-df79-4739-8a6d-7dfdcb50d61b",
   "metadata": {},
   "source": [
    "## 2.Momentum-based gradient descent\n",
    "\n",
    "* **Goal**<br>\n",
    "  To <font color=\"green\"><b>reduce oscillations</b></font> during the gradient descent minimization process<br>\n",
    "  and thereby <font color=\"green\"><b>accelerate convergence</b></font>.\n",
    " \n",
    "* **Approach**<br>\n",
    "  Apply <font color=\"green\"><b>Exponentially Weighted Moving Average</b></font> (EWMA) to smooth the gradients.\n",
    "\n",
    "* **Results**<br>\n",
    "  + Enables <font color=\"green\"><b>larger steps</b></font> in flat regions of the loss surface.\n",
    "  + Enables <font color=\"green\"><b>smaller steps</b></font> in highly curved regions, reducing oscillations.\n",
    "  \n",
    "* **Mathematical formulation**<br>\n",
    "  \\begin{eqnarray}\n",
    "    \\theta^{[l]}_t & = & \\beta \\, \\theta^{[l]}_{t-1} + (1-\\beta)\\, dW^{[l]}_{t} \\nonumber \\\\\n",
    "    \\xi^{[l]}_t & = & \\beta \\, \\xi^{[l]}_{t-1} + (1-\\beta)\\, db^{[l]}_{t} \\nonumber \\\\\n",
    "  \\end{eqnarray}\n",
    "  where:\n",
    "  + $l$: layer of the deep neural net ($ l \\in \\{1,2,\\ldots,L\\}$)\n",
    "  + $\\theta^{[l]}_t$: EWMA of $dW^{[l]}$\n",
    "  + $\\xi^{[l]}_t$: EWMA of $db^{[l]}_{t}$\n",
    "\n",
    "  <br>\n",
    "  The <font color=\"green\"><b>update of the parameters</b></font> then becomes:<br>\n",
    "  $\\texttt{for(l=1,...,L)}$<br>\n",
    "  $\\texttt{do}$<br>\n",
    "  $\\begin{eqnarray}\n",
    "      W^{[l]}_t & = & W^{[l]}_t - \\alpha \\,\\theta^{[l]}_t \\\\\n",
    "      b^{[l]}_t & = & b^{[l]}_t - \\alpha \\,\\xi^{[l]}_t \\\\\n",
    "  \\end{eqnarray}$<br>\n",
    "  $\\texttt{done}$\n",
    "\n",
    "  <br>\n",
    "\n",
    "* **Remarks**\n",
    "  + Instead of using $\\theta^{[l]}_t$ and $\\xi^{[l]}_t$, their <font color=\"green\"><b>bias-corrected</b></font> versions $\\widehat{\\theta}^{[l]}_t$ and $\\widehat{\\xi}^{[l]}_t$ can be used:\n",
    "  \\begin{eqnarray}\n",
    "     \\widehat{\\theta}^{[l]}_t = & \\frac{\\theta^{[l]}_t}{1- \\beta^t} \\\\\n",
    "     \\widehat{\\xi}^{[l]}_t   = & \\frac{\\xi^{[l]}_t}{1- \\beta^t}\n",
    "  \\end{eqnarray}\n",
    "  + This correction is mainly relevant during the early stages of optimization, as the number of iterations is typically large.\n",
    "  + The <font color=\"green\"><b>hyperparameter</b></font> $\\beta$ is often set to $0.9$.\n",
    "\n",
    "* **Physical Analogy**<br>\n",
    "  Momentum-based gradient descent is inspired by classical mechanics.<br>\n",
    "  Consider an object subjected to an external force $\\mathbf{F}$ in a medium with viscosity $\\eta$. Its motion is described by:\n",
    "  \\begin{eqnarray}\n",
    "       m\\frac{d^2 \\mathbf{x}}{dt^2} \\,+\\, K \\eta \\frac{ d\\mathbf{x}}{dt} & = & \\mathbf{F}  \\\\\n",
    "  \\end{eqnarray}\n",
    "    The factor $K$ is a coefficient dependent on the shape of the object.<br>\n",
    "    Rearranging this equation yields an expression analogous to momentum-based optimization.\n",
    "     \\begin{eqnarray}\n",
    "        \\mathbf{v} = \\frac{1}{K\\eta}\\mathbf{F} - \\frac{1}{K\\eta}\\frac{d \\mathbf{p}}{dt}\n",
    "     \\end{eqnarray}  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf756eab-47be-4c12-a3bc-6712217fd22b",
   "metadata": {},
   "source": [
    "## 3.RMSProp\n",
    "\n",
    "This algorithm was proposed by G. Hinton for a <a href=\"green\"><b>Coursera course</b></font>.<br>\n",
    "[see also here](https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)\n",
    "\n",
    "The <font color=\"green\"><b>Root Mean Square Propagation</b></font> (RMSProp) method has identical goals as the momentum-based optimization (i.e. reducing the oscillation in the convergency of the gradient descent). It proceeds mathematically in a slightly different fashion.\n",
    "\n",
    "* **Mathematical formulation**<br>\n",
    "  \\begin{eqnarray}\n",
    "    \\tau^{[l]}_t & = & \\widetilde{\\beta} \\, \\tau^{[l]}_{t-1} + (1-\\widetilde{\\beta})\\, dW^{[l]}_{t} \\odot dW^{[l]}_{t} \\nonumber \\\\\n",
    "    \\psi^{[l]}_t & = & \\widetilde{\\beta} \\, \\psi^{[l]}_{t-1} + (1- \\widetilde{\\beta})\\, db^{[l]}_{t} \\odot  db^{[l]}_{t}\\nonumber \\\\\n",
    "  \\end{eqnarray}\n",
    "  where:\n",
    "  + $l$: layer of the deep neural net ($ l \\in \\{1,2,\\ldots,L\\}$)\n",
    "  + $\\tau^{[l]}_t$: EWMA of $dW^{[l]}_{t} \\odot dW^{[l]}_{t}$ (element-wise square)\n",
    "  + $\\psi^{[l]}_t$: EWMA of $db^{[l]}_{t} \\odot db^{[l]}_{t}$ (element-wise square)\n",
    "\n",
    "  <br>\n",
    "  The <font color=\"green\"><b>update of the parameters</b></font> then becomes:<br>\n",
    "  $\\texttt{for(l=1,...,L)}$<br>\n",
    "  $\\texttt{do}$<br>\n",
    "  $\\begin{eqnarray}\n",
    "      W^{[l]}_t & = & W^{[l]}_t - \\alpha \\displaystyle \\frac{dW^{[l]}_t}{\\sqrt{\\tau^{[l]}_t + \\epsilon }} \\\\\n",
    "      b^{[l]}_t & = & b^{[l]}_t - \\alpha \\displaystyle \\frac{db^{[l]}_t}{\\sqrt{\\psi^{[l]}_t + \\epsilon }}\n",
    "  \\end{eqnarray}$<br>\n",
    "  $\\texttt{done}$\n",
    "\n",
    "  The value of $\\epsilon$ is set to very small values (e.g. $10^{-8}$) to prevent division by zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0beb1f8-d573-44df-9ffe-50d74ff9f157",
   "metadata": {},
   "source": [
    "## 4.Adaptive Moment Estimation (Adam)\n",
    "The <font color=\"green\"><b>Adaptive Moment Estimation (Adam)</b></font> is a combination of:\n",
    "+ Momentum-based gradient descent\n",
    "+ RMSProp\n",
    "\n",
    "This [**algorithm**](https://arxiv.org/pdf/1412.6980) was developed by **Diederik Kingma** and **Jimmy Ba**.  \n",
    "\n",
    "**Mathematical formulation**\n",
    "\n",
    "* <font color=\"green\"><b>Momentum-based gradient descent</b></font>\n",
    "  \\begin{eqnarray}\n",
    "    \\theta^{[l]}_t & = & \\beta_1 \\, \\theta^{[l]}_{t-1} + (1-\\beta_1)\\, dW^{[l]}_{t} \\nonumber \\\\\n",
    "    \\xi^{[l]}_t & = & \\beta_1 \\, \\xi^{[l]}_{t-1} + (1-\\beta_1)\\, db^{[l]}_{t} \\nonumber \\\\\n",
    "  \\end{eqnarray}\n",
    "  Further:\n",
    "  \\begin{eqnarray}\n",
    "     \\widehat{\\theta}^{[l]}_t = & \\frac{\\theta^{[l]}_t}{1- \\beta_1^t} \\\\\n",
    "     \\widehat{\\xi}^{[l]}_t   = & \\frac{\\xi^{[l]}_t}{1- \\beta_1^t}\n",
    "  \\end{eqnarray}\n",
    "\n",
    "* <font color=\"green\"><b>RMSProp</b></font>\n",
    "  \n",
    "  \\begin{eqnarray}\n",
    "    \\tau^{[l]}_t & = & \\beta_2 \\, \\tau^{[l]}_{t-1} + (1- \\beta_2)\\, dW^{[l]}_{t} \\odot dW^{[l]}_{t} \\nonumber \\\\\n",
    "    \\psi^{[l]}_t & = & \\beta_2 \\, \\psi^{[l]}_{t-1} + (1- \\beta_2)\\, db^{[l]}_{t} \\odot  db^{[l]}_{t}\\nonumber \\\\\n",
    "  \\end{eqnarray}\n",
    "  \n",
    "  Further:\n",
    "  \\begin{eqnarray}\n",
    "     \\widehat{\\tau}^{[l]}_t = & \\frac{\\tau^{[l]}_t}{1- \\beta_2^t} \\\\\n",
    "     \\widehat{\\psi}^{[l]}_t   = & \\frac{\\psi^{[l]}_t}{1- \\beta_2^t}\n",
    "  \\end{eqnarray}\n",
    "  \n",
    " \n",
    "  The <font color=\"green\"><b>update of the parameters</b></font> then becomes:<br>\n",
    "  $\\texttt{for(l=1,...,L)}$<br>\n",
    "  $\\texttt{do}$<br>\n",
    "  $\\begin{eqnarray}\n",
    "      W^{[l]}_t & = & W^{[l]}_t - \\alpha \\displaystyle \\frac{\\widehat{\\theta}^{[l]}_t}{\\sqrt{\\widehat{\\tau}^{[l]}_t} + \\epsilon } \\\\\n",
    "      b^{[l]}_t & = & b^{[l]}_t - \\alpha \\displaystyle \\frac{\\widehat{\\xi}^{[l]}_t}{\\sqrt{\\widehat{\\psi}^{[l]}_t} + \\epsilon }\n",
    "  \\end{eqnarray}$<br>\n",
    "  $\\texttt{done}$\n",
    "\n",
    "\n",
    "The <font color=\"green\"><b>hyperparameters</b></font> are set by a lot of packages to the following values:\n",
    "\\begin{eqnarray}\n",
    "    \\beta_1 & = & 0.9 \\\\\n",
    "    \\beta_2 & = & 0.999 \\\\\n",
    "    \\epsilon & = & 10^{-8}\n",
    "\\end{eqnarray}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1db9e29-e19b-4c29-bce5-3efba3d566c1",
   "metadata": {},
   "source": [
    "## 5.Decay of the learning rate\n",
    "Up till now we have assumed the learning rate ($\\alpha$) to be constant.<br>\n",
    "**Add something on these decay rate schemes.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d9882a-b2cc-49b1-83d5-1afc68b6b1e9",
   "metadata": {},
   "source": [
    "# Bibliography\n",
    "* [Peter J. Brockwell and Richard A. Davis, **Time Series: Theory and Methods**](https://www.amazon.com/Time-Methods-Peter-J-Brockwell/dp/0387974296/)\n",
    "* [Kingma and Ba, **Adam: A Method for Stochastic Optimization**](https://arxiv.org/abs/1412.6980)\n",
    "* [Bottou et al., **Optimization Methods for Large-Scale Machine Learning**](https://arxiv.org/pdf/1606.04838)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271fea55-8004-46f3-a7ec-1ef65e0d5a4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
