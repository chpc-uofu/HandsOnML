{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fb4769-d938-4c12-9a85-52317dad236f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as ss\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "import keras\n",
    "from keras.optimizers import SGD, Adam, AdamW\n",
    "import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb55a6c-7666-451c-8b06-8770cfdb9182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether GPU is available\n",
    "%run ./check_gpu.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b53a679-01ae-4772-a474-9d8e2dec5dac",
   "metadata": {},
   "source": [
    "# Hands-on Introduction to Deep Learning (Lecture 2)\n",
    "\n",
    "* <font color=\"teal\"><b>author: Wim R.M. Cardoen</b></font>\n",
    "* <font color=\"teal\"><b>e-mail: wcardoen [\\at] gmail.com</b></font>\n",
    "\n",
    "## Synopsis  \n",
    "In <a href=\"./lecture1.ipynb\"><b>Lecture 1</b></a>, we worked out the details of <font color=\"green\"><b>logistic regression</b></font> as\n",
    "the simplest neural net.<br>\n",
    "We introduced the following concepts:\n",
    "* **neuron/unit** (`perceptron`)\n",
    "* **non-linear activation function** (`sigmoid`)\n",
    "* **loss/cost function** (metric to optimize the neural net) for `binary classification`\n",
    "* **gradient descent method** (`optimization` of the weights and bias)\n",
    "* **forward propagation** (moving forward from the input to the loss/cost function)\n",
    "* **backward propagation** (moving backwards from the loss/cost function to the inputs to calculate the gradients)\n",
    "\n",
    "## Goal\n",
    "In this Lecture, we will generalize the previous example to a <font color=\"green\"><b>fully connected dense neural net</b></font>.\n",
    "\n",
    "## Note\n",
    "* At the end of some paragraphs you may find a section \"<font color=\"teal\"><b>Note (for sake of completeness)</b>\"</font><br>\n",
    "* I have added a few additional <font color=\"orange\"><b>notebooks</b></font>, such as\n",
    "  - <a href=\"./ewma.ipynb\"><b>ewma.ipynb</b></a>\n",
    "  - <a href=\"./bias-variance.ipynb\"><b>bias-variance.ipynb</b></a>\n",
    "  - <a href=\"./finitediff.ipynb\"><b>finitediff.ipynb</b></a>\n",
    "  \n",
    "  to elucidate on certain topics.\n",
    "* These sections and notebooks can be <font color=\"teal\"><b>skipped</b></font>. They target people who want to dig deeper<br>\n",
    "(in case you want to implement your own deep learning code from scratch in e.g. <a href=\"https://cupy.dev/\"><b>CuPy</b></a>, <a href=\"https://developer.nvidia.com/nvmath-python\"><b>nv-math</b></a> or a compiled language)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c24887-f06c-4021-9a55-bbf54938cebc",
   "metadata": {},
   "source": [
    "#### <font color=\"darkred\"><b>Exercise 1</b></font>\n",
    "- Generate & plot the data set ($X,y$) by invoking the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9fd4ef-d73a-4e6f-ae34-3165f9f7095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./create_data2.py\n",
    "X, y = make_spirals(n_samples=5000, noise=0.3, turns=4, seed=123)\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(X[:,0], X[:,1], c=y, cmap='coolwarm', s=8, alpha=0.7, edgecolors='none')\n",
    "plt.title(\"Spirals Dataset\")\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922acff8-6fb6-4046-be39-356d0e1d9201",
   "metadata": {},
   "source": [
    "* The <font color=\"green\"><b>original data</b></font> will be <font color=\"green\"><b>partitioned</b></font> into 2 sets:\n",
    "  - training set (containing $60\\%$ of the original set)\n",
    "  - temporary set (containing the remainder of the original set)\n",
    "* The <font color=\"green\"><b>training set</b></font> will be normalized/scaled.<br>\n",
    "  The identical scaling will be applied to the temporary set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3537edd6-c15b-4cb8-8a48-844a8159af60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'stratify' parameter : requires an array-like onput [None]\n",
    "#                            enforces the proportionality of the original classes in the new sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, train_size=0.60, random_state=123, stratify=y)\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_temp_s = scaler.transform(X_temp)\n",
    "print(f\"Partitioning of the original data:\")\n",
    "print(f\"  X_train: {X_train_s.shape}\")\n",
    "print(f\"  X_temp : {X_temp_s.shape}\")\n",
    "print(f\"  y_train: {y_train.shape}\")\n",
    "print(f\"  y_temp : {y_temp.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b10ce29-fae5-4480-97ea-096315703555",
   "metadata": {},
   "source": [
    "* Split the <font color=\"green\"><b>temporary set</b></font> into 2 <font color=\"green\"><b>equally sized</b></font> sets, i.e.<br>\n",
    "  - $\\texttt{X\\_temp\\_s}$ into $\\texttt{X\\_val\\_s}$ and $\\texttt{X\\_test\\_s}$\n",
    "  - $\\texttt{y\\_temp}$ into $\\texttt{y\\_val}$ and $\\texttt{y\\_test}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6e7d3b-cda0-45c5-964e-13a0a6f56a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex. 1 \n",
    "X_val_s, X_test_s, y_val, y_test =  # Here comes your code ( 1 line)\n",
    "print(f\"Training/validation/test::\")\n",
    "print(f\"  X_train (scaled):{X_train_s.shape}    y_train :{y_train.shape}\")\n",
    "print(f\"  X_val (scaled)  :{X_val_s.shape  }    y_val   :{y_val.shape}\")\n",
    "print(f\"  X_test (scaled) :{X_test_s.shape }    y_test  :{y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8497150a-71a3-4d44-be0d-8a5d5de7aaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/lec2/ex1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17180995-d326-4537-849c-384f7a980a21",
   "metadata": {},
   "source": [
    "#### <font color=\"darkred\"><b>Exercise 2</b></font>\n",
    "We will now apply logistic regression using $\\texttt{scikit-learn}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e34889-c2dd-4c10-86fb-7f68a768325b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex. 2:\n",
    "lr_sk = LogisticRegression(solver='lbfgs', max_iter=2000, random_state=123)\n",
    "# <--- Here comes your code to train/'fit' (1 line)\n",
    "\n",
    "# Predict the outcome of X_train_s\n",
    "y_train_pred = # <--- Here comes your code to predict 'y_train_pred' (1 line)\n",
    "\n",
    "# Calculate the accuracy_score\n",
    "acc_train = # <--- Here comes your code to calculate the accuracy_score (1 line)\n",
    "print(f\"  acc. (train): {acc_train:6.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dc0c2f-d043-461a-8f3f-c101331ffb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/lec2/ex2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21703af-5246-48ac-a47e-163b361bc509",
   "metadata": {},
   "source": [
    "##### Conclusion:\n",
    "As to be expected, simple `Logistic Regression` does **not** work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371402ba-7712-4c37-887f-a5f7ec29e6b6",
   "metadata": {},
   "source": [
    "# 1.Dense Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd355ac-40a3-4e9b-b8a6-75576197d076",
   "metadata": {},
   "source": [
    "## Definitions:\n",
    "A **fully connected dense neural network** of <font color=\"green\"><b>depth</b></font> $L$ is a neural net where:\n",
    "- <font color=\"green\"><b>every neuron</b></font> in layer ($l$) is connected to <font color=\"green\"><b>every</b></font> neuron in layer ($l+1$) where $l=\\{0,1,2,\\ldots,L-1\\}$.\n",
    "- these connections are **directed** and **acyclic graphs** (<a href=\"https://en.wikipedia.org/wiki/Directed_acyclic_graph\"><b>DAG</b></a>), i.e. they are unidirectional from layer $l$ towards layer $l+1$ and do\n",
    "  **NOT** cycle back.\n",
    "- the number of nodes in layer $l$ (<font color=\"green\"><b>width</b></font> of layer $l$) will be designated as $n^{[l]}$.\n",
    "- types of layers:\n",
    "  + <font color=\"green\"><b>input</b></font> layer: $l=0$. It is **NOT** counted as a real layer (this will become obvious soon!). \n",
    "  + <font color=\"green\"><b>output</b></font> layer: $l=L$.\n",
    "  + <font color=\"green\"><b>hidden</b></font> layers: $l \\in \\{1,2,\\ldots,L-1\\}$.\n",
    "\n",
    "Neural nets with <font color=\"green\"><b>NO hidden</b></font> layers ($L=1$) are called \n",
    "<font color=\"green\"><b>shallow</b></font>.<br> The neural net we discussed previously (Lecture 1) is an example of a shallow neural net with $n^{[0]}=2$ and $n^{[1]}=1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce341675-e458-4b9b-97ed-bfa36de4f096",
   "metadata": {},
   "source": [
    "## Example:\n",
    "The image below displays a deep neural with $L=4$ layers (neural net of depth 4).\n",
    "\n",
    "<img src=\"dnn_4layers.jpeg\" alt=\"Deep neural net with 4 layers\" style=\"width:65%; height:auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267dcf5e-5b31-49a0-a6fb-10c896b1871a",
   "metadata": {},
   "source": [
    "* <font color=\"green\"><b>input</b></font> layer: layer $0$\n",
    "  + the input layer requires input vectors/examples each of length $n^{[0]}=6$\n",
    "* <font color=\"mediumslateblue\"><b>hidden</b></font> layers: layers $1,2,3$\n",
    "  + layer $1$: has $4$ nodes ($n^{[1]}=4$)\n",
    "  + layer $2$: has $3$ nodes ($n^{[2]}=3$)\n",
    "  + layer $3$: has $6$ nodes ($n^{[3]}=6$)\n",
    "* <font color=\"red\"><b>output</b></font> layer: layer $4$\n",
    "  + layer $4$: has $4$ nodes ($n^{[4]}=4$)\n",
    "\n",
    "The output layer is followed by a <font color=\"green\"><b>loss function</b></font> which is \n",
    "the <font color=\"green\"><b>objective function</b></font> to be minimized during the <font color=\"green\"><b>training</b></font> phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c56da9a-61a8-4212-bda7-2c08cfd7152e",
   "metadata": {},
   "source": [
    "<font color=\"teal\"><b>Note (for sake of completeness)</b></font><br>\n",
    "<details>\n",
    "  <summary>Click here to expand!</summary>\n",
    "    \n",
    "There are exceptions to the aforementioned definition, e.g. to name a few:\n",
    "+ <a href=\"https://en.wikipedia.org/wiki/Residual_neural_network\"><b>Residual Neural Networks</b></a> (ResNets)\n",
    "+ <a href=\"https://en.wikipedia.org/wiki/Bidirectional_recurrent_neural_networks\"><b>Bidirectional Recurrent Neural Networks</b></a> (BRNNs)\n",
    "+ $\\ldots$\n",
    " \n",
    "This type of networks will **NOT** be covered in this lecture but may the subject of future talks.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3981fc77-41dc-49cf-8b64-0e95c6575cac",
   "metadata": {},
   "source": [
    "# 2.Activation functions\n",
    "Previously, we discussed the `sigmoid` activation function:<br>\n",
    "  $\\begin{eqnarray}\n",
    "     \\sigma(z) & = & \\frac{1}{1\\,+\\,\\exp(-z)}\\,, z \\,\\in \\mathbb{R}\n",
    "  \\end{eqnarray}$\n",
    "\n",
    "The activation functions for the output layer and the hidden layers\n",
    "are in general <font color=\"green\"><b>different</b></font>.\n",
    "\n",
    "- <font color=\"red\"><b>output</b></font> layer:<br>the activation function is <font color=\"green\"><b>determined</b></font> by the subsequent loss function.\n",
    "  * `binary` classification: <br>\n",
    "    + `sigmoid` $\\sigma(z)$\n",
    "    + $\\sigma(z) \\in [0,1]$\n",
    "  * `multiclass` classification: \n",
    "    + $\\begin{eqnarray}\n",
    "       \\texttt{softmax}(z_{ij}) & := &  \\frac{\\exp(z_{ij})}{ \\displaystyle\\sum_{k=1}^C \\exp(z_{ik})}\n",
    "        \\end{eqnarray}$\n",
    "    + $C$ stands for the number of classes \n",
    "    + $\\texttt{softmax}(z_{ij}) \\in [0,1]$  \n",
    "  * `linear regression`:<br>\n",
    "    + `1` (no action) \n",
    "\n",
    "- <font color=\"mediumslateblue\"><b>hidden</b></font> layers:<br>\n",
    "  There are several activation functions that are commonly used:\n",
    "  * $\\texttt{relu}(x)$: (**Re**ctified **L**inear **U**nit) - the most commonly used<br>\n",
    "    + $\\texttt{relu}(x) := \\max(0,x)$\n",
    "    + $\\texttt{relu}(x) \\geq 0 $\n",
    "  * $\\texttt{lrelu}(x;\\alpha)$: (**L**eaky **Re**ctified **L**inear **U**nit)<br>\n",
    "    + $\\texttt{lrelu}(x;\\alpha) := \\max(\\alpha x, x) \\,,\\,0 \\leq \\alpha < 1$\n",
    "  * $\\texttt{gelu}(x)$: (**G**aussian-**e**rror **Linear** **U**nit)<br>\n",
    "    + $\\begin{eqnarray} \\texttt{gelu}(x) &:= & x\\,\\Phi(x) \\end{eqnarray}$ <br>\n",
    "      where $\\Phi(x)$ stands for the <a href=\"https://en.wikipedia.org/wiki/Cumulative_distribution_function\"><b>CDF</b></a> of the standard normal distribution.\n",
    "    + $\\begin{eqnarray} \\texttt{gelu}(x)              & = &x \\displaystyle\\int_{-\\infty}^x \\frac{1}{\\sqrt{2 \\pi}}{ \\exp\\bigg(-\\frac{t^2}{2}\\bigg)} dt\n",
    "       \\end{eqnarray}$\n",
    "  * $\\texttt{swish}(x;\\beta) := x \\, \\sigma(\\beta x)$    \n",
    "  * $\\texttt{tanh}(x)$: (Hyperbolic Tangent)<br>\n",
    "    + $\\texttt{tanh}(x) := \\displaystyle \\frac{\\exp(x) - \\exp(-x)}{\\exp(x) + \\exp(-x)}$\n",
    "    + $\\texttt{tanh}(x) \\in [-1,1]$\n",
    "  * $\\ldots$  <br>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1d364f-43a4-408a-8e7b-f5f57048badd",
   "metadata": {},
   "source": [
    "## Plot of some of the activation functions\n",
    "Below you will find some of the aforementioned activation functions displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1441802d-00c7-4793-a910-7c2194791a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "START= -2.5\n",
    "END= 2.5\n",
    "DENS_PER_UNIT=20\n",
    "ALPHA=0.05\n",
    "x = np.linspace(start=START,stop=END, num=int((END-START))*DENS_PER_UNIT+1)\n",
    "y_relu = np.maximum(0,x)\n",
    "y_lrelu = np.maximum(ALPHA*x,x)\n",
    "y_tanh = np.tanh(x)\n",
    "y_sigmoid = 1.0/(1.0+np.exp(-x))\n",
    "y_swish = x*y_sigmoid\n",
    "y_gelu = x* ss.norm.cdf(x)\n",
    "plt.title(r\"Plot of the most common activation functions\")\n",
    "plt.plot(x,y_lrelu, label=r\"$\\mathrm{lrelu(x;\\alpha=0.05)}$\", color=\"blue\")\n",
    "plt.plot(x,y_relu, label=r\"$\\mathrm{relu(x)}$\", color=\"red\")\n",
    "plt.plot(x,y_gelu, label=r\"$\\mathrm{gelu(x)}$\")\n",
    "plt.plot(x,y_tanh, label=r\"$\\mathrm{tanh(x)}$\", color=\"green\")\n",
    "plt.plot(x,y_sigmoid, label=r\"$\\mathrm{\\sigma(x)}$\", color=\"orange\")\n",
    "plt.plot(x,y_swish, label=r\"$\\mathrm{swish(x;\\beta=1.0)}$\", color=\"brown\")\n",
    "plt.xticks(np.arange(START,END+0.5, step=0.5))\n",
    "plt.xlabel(r\"$x$\")\n",
    "plt.ylabel(r\"$y$\",rotation=0)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458081c5-5c69-4cf5-8863-4ec6d22cf31a",
   "metadata": {},
   "source": [
    "# 3.Cost/Loss functions\n",
    "\n",
    "* The <font color=\"green\"><b>cost</b></font> function ($\\mathcal{C}$) is defined as follows:<br>\n",
    "$\\begin{eqnarray}\n",
    "    \\mathcal{C} & := & \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}^{(i)}\n",
    " \\end{eqnarray}$<br>\n",
    " where $\\mathcal{L}^{(i)}$ stands for the <font color=\"green\"><b>loss</b></font> function for example $i$ and $m$ stands for the batch\n",
    " of examples used.\n",
    "\n",
    "* Depending on the task we want to accomplish, we need to use different loss functions.<br>\n",
    "  The most <font color=\"blue\"><b>common</b></font> loss functions are:<br>\n",
    "  * `binary classification`<br>\n",
    "    $\\begin{eqnarray}\n",
    "        \\mathcal{L}^{(i)} &:= & -y_i\\,\\log(a^{[L]}_i)\n",
    "      \\end{eqnarray}$<br>\n",
    "    where:\n",
    "    - $y_i$ is either $0$ or (<font color=\"red\"><b>exclusive</b></font>) $1$ and represents the label of example $i$.\n",
    "  * `multiclass classification` (exclusionary)<br>\n",
    "    $\\begin{eqnarray}\n",
    "        \\mathcal{L}^{(i)} &:= & \\sum_{j=1}^C -y_{ij}\\,\\log(a^{[L]}_{ij})\n",
    "      \\end{eqnarray}$<br>\n",
    "    where:<br>\n",
    "    + $y_i$ is a <a href=\"https://en.wikipedia.org/wiki/One-hot\"><b>one-hot</b></a> encoding vector representing the <font color=\"green\"><b>label</b></font> of example $i$.\n",
    "    + $C$ stands for the <font color=\"green\"><b>number of classes</b></font>.\n",
    "  * `linear regression`<br>\n",
    "    $\\begin{eqnarray}\n",
    "        \\mathcal{L}^{(i)} &:= \\frac{1}{2} (y_i - a^{[L]}_i)^2\n",
    "      \\end{eqnarray}$<br>\n",
    "    where:<br>\n",
    "    + $y_i$ is the value associated with example $i$.\n",
    "* The cost function can also contain extra terms (i.e. <font color=\"green\"><b>regularization</b></font>) to constrain certain parameters/prevent <font color=\"green\"><b>overfitting</b></font>.<br>\n",
    "  We will elaborate on this later on in this Lecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a48fcbb-c173-4df3-ad17-206ab872b2c5",
   "metadata": {},
   "source": [
    "<font color=\"teal\"><b>Note (for sake of completeness)</b></font><br>\n",
    "<details>\n",
    "  <summary>Click here to expand!</summary>\n",
    "\n",
    "- A lot of loss functions can be easily derived using:\n",
    "  + the <a href=\"https://en.wikipedia.org/wiki/Maximum_likelihood_estimation\"><b>Maximum Likelihood Estimation</b></a> (MLE) method or using the <a href=\"https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\"><b>Kullback-Leibler divergence</b></a>\n",
    "  + combined with the theory of <a href=\"https://en.wikipedia.org/wiki/Generalized_linear_model\"><b>Generalized Linear Models</b></a> (GLM).<br>\n",
    "  e.g. `Poisson loss function` (to predict discrete counts),...\n",
    "- To improve <font color=\"green\"><b>numerical stabilty</b></font>, the activation function for the\n",
    "    last layer ($L$) and the loss function are:\n",
    "  + often <font color=\"green\"><b>calculated together/at once</b></font>\n",
    "  + and then bear a separate (information-theoretic) name.\n",
    "\n",
    "- In the output layer there can be several types of activation functions at the same time,<br>\n",
    "  e.g. a `linear regression` problem where we are interested at the same time:\n",
    "  + in a predicted value of $a^{[L]}_{i1} \\in \\mathbb{R}$\n",
    "  + the prediction of the variance ($ a^{[L]}_{i2}\\in \\mathbb{R}^+$)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6c434a-28c2-498d-a978-1d7041f135e4",
   "metadata": {},
   "source": [
    "# 4.Structure of the DNN: summary\n",
    "- We just described the <font color=\"green\"><b>basic structure</b></font> of a deep neural network, i.e.:\n",
    "  + the layers $l \\in \\{0,1,\\ldots, L\\}$.\n",
    "  + the number of nodes per layer i.e. $n^{[l]}$ for $l \\in \\{0,1,\\ldots, L\\}$.\n",
    "- We also introduced the two other structural components:\n",
    "  + activation functions.<br>\n",
    "    <font color=\"green\"><b>ONE</B></font> activation function ($\\widehat{h}^{[l]}$) is <font color=\"green\"><b>REQUIRED</b></font> per layer $l \\in \\{1,\\ldots, L\\}$.\n",
    "  + the loss/cost function $\\mathcal{L}^{(i)}/\\mathcal{C}$.<br>\n",
    "    The loss/cost function is only <font color=\"green\"><b>MANDATORY</b></font> during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187c9f61-970b-4188-9602-0d037f4e4fee",
   "metadata": {},
   "source": [
    "## Implementation of the previous DNN example in Keras/PyTorch\n",
    "We have a deep neural with $L=4$ layers (neural net of depth 4).\n",
    "* <font color=\"green\"><b>input</b></font> layer: layer $0$\n",
    "  + the input layer requires input vectors/examples each of length $n^{[0]}=6$\n",
    "* <font color=\"mediumslateblue\"><b>hidden</b></font> layers: layers $1,2,3$\n",
    "  + layer $1$: has $4$ nodes ($n^{[1]}=4$) - activation $h^{[1]}$: `ReLu` \n",
    "  + layer $2$: has $3$ nodes ($n^{[2]}=3$) - activation $h^{[2]}$: `ReLu`\n",
    "  + layer $3$: has $6$ nodes ($n^{[3]}=6$) - activation $h^{[3]}$: `ReLu` \n",
    "* <font color=\"red\"><b>output</b></font> layer: layer $4$\n",
    "  + layer $4$: has $4$ nodes ($n^{[4]}=4$) - activation $h^{[4]}$: `softmax`\n",
    "    (We assume that the model will train to classify $4$ classes). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d781e64-7a75-481d-93c4-370d451857a5",
   "metadata": {},
   "source": [
    "### Temporary check\n",
    "How many <font color=\"green\"><b>parameters</b></font> do we have in the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7275d400-3949-4599-b82f-1f299a0c6f7b",
   "metadata": {},
   "source": [
    "#### A.Set up the model (in Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3932136-b245-4833-acbc-b2219e25904d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting up the model using Keras (see also in kerastorch.ipynb)\n",
    "mymodel = keras.Sequential([\n",
    "            keras.layers.Input(shape=(6,), name='input'),  # Input: 6-dimensional input vector\n",
    "            keras.layers.Dense(units=4, activation='relu', name='hila1_nodes=4'), # Layer 1\n",
    "            keras.layers.Dense(units=3, activation='relu', name='hila2_nodes=3'), # Layer 2\n",
    "            keras.layers.Dense(units=6, activation='relu', name='hila3_nodes=6'), # Layer 3\n",
    "            keras.layers.Dense(units=4, activation='softmax', name='output')],    # Output: 4 classes\n",
    "            name=\"3HiLa_4-3-6\") \n",
    "print(mymodel.summary())  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5da22a3-9bb9-4f86-81be-0a3e3a6853fb",
   "metadata": {},
   "source": [
    "# 5. Training of a Deep Neural Network\n",
    "\n",
    "During <font color=\"green\"><b>TRAINING</b></font>, a `function` needs to be found<br>\n",
    "which is able to <font color=\"green\"><b>LEARN/GENERALIZE</b></font> well (i.e. to deal sufficiently well with unseen examples).\n",
    "\n",
    "Therefore, two questions arise:\n",
    "- a. does such a `function` even exist?\n",
    "- b. and if so, how to find such a `function`/how to train a network.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd23507-fe4c-4dec-885c-c97ed4be8d41",
   "metadata": {},
   "source": [
    "## a. The universal approximation theorems (UATs)\n",
    "<font color=\"green\"><b>Theorem</b></font>:<br>\n",
    "A feedforward neural network with:\n",
    "1. at least one hidden layer\n",
    "2. a finite number of neurons\n",
    "3. a <font color=\"red\"><b>nonlinear</b></font> activation function\n",
    "\n",
    "can approximate to any accuracy<br>\n",
    "<font color=\"green\"><b>any continuous function</b></font> on a <font color=\"green\"><b>compact subset</b></font> of $\\mathbb{R}^n$.\n",
    "\n",
    "This theorem is related to the <font color=\"green\"><b>Stone-Weierstrass theorem</b></font> from analysis<br> (see e.g. <a href=\"https://david92jackson.neocities.org/images/Principles_of_Mathematical_Analysis-Rudin.pdf\"><b>Principles of Mathematical Analysis by Walter Rudin</b></a>)\n",
    "\n",
    "Sources:\n",
    "* <a href=\"https://web.njit.edu/~usman/courses/cs675_fall18/10.1.1.441.7873.pdf\">Cybenko, G. (1989). Approximation by superposition of sigmodial functions. Mathematics of Control, Signals and Systems, 2(4), p. 303-314.</a>\n",
    "* <a href=\"https://web.njit.edu/~usman/courses/cs677/hornik-nn-1991.pdf\"> Hornik, K. (1991). Approximation Capabilities of Multilayer Feedforward Networks. Neural Networks, 4, p. 251-257.</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b714b99-16cb-4c8e-9e7f-221334d8bd61",
   "metadata": {},
   "source": [
    "## b.Training a deep neural network.\n",
    "\n",
    "- In <a href=\"./lecture1.ipynb\"><b>Lecture 1</b></a>, we trained a <font color=\"red\"><b>shallow</b></font> neural network ($1$ node in $1$ layer).<br>\n",
    "  To do so, we had to <font color=\"green\"><b>iterate</b></font> repeatedly over the <font color=\"blue\"><b>following loop</b></font>:\n",
    "  + Perform the <font color=\"green\"><b>forward propagation</b></font></color>.<br>Start at the input layer and proceed forward to the output layer and the loss function.\n",
    "  + Perform the <font color=\"green\"><b>backward propagation</b></font></color>.<br>Start at the loss function and recede all the way to the first layer\n",
    "    in order<br> to obtain the <font color=\"green\"><b>gradients of the parameters</b></font>.\n",
    "  + <font color=\"green\"><b>Update the parameters</b></font> (weights and bias) using the <font color=\"green\"><b>gradient descent</b></font> method.<br>  \n",
    "\n",
    "\n",
    "- In the following sections, we will elaborate on the same components for a deep neural net."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e82191-8afc-4dc3-af62-21b5c5065962",
   "metadata": {},
   "source": [
    "# 6.Forward propagation (dense deep neural net)\n",
    "\n",
    "We extracted a slice from the previous example to illuminate the principle of forward propagation .<br><br>\n",
    "<img src=\"forward1.jpeg\" alt=\"Connections between layers 1 and 2\" style=\"width:65%; height:auto\"><br>\n",
    "\n",
    "**Setup/Context**\n",
    "- We have a dense deep neural net with **$L$** layers. Each layer $l$ has **$n^{[l]}$** nodes.\n",
    "- The input layer ($l=0$) has $n^{[0]}$ nodes.<br>\n",
    "- The number of nodes ($n^{[0]}$) is **equal** to the dimensionality of each input vector $x_i$, where:<br>\n",
    "  $\\begin{eqnarray}\n",
    "    x_i := \\begin{pmatrix}\n",
    "            x_{i,1} & x_{i,2} & \\ldots & x_{i,n^{[0]}}\n",
    "           \\end{pmatrix}\n",
    "  \\end{eqnarray}$<br>\n",
    "- $ a^{[0]}_{i} := x_i $   \n",
    "\n",
    "**Transformation**\n",
    "- At <font color=\"green\"><b>layer</b></font> $l \\in \\{1,2,\\ldots,L\\}$ the vector associated with example $i$ is transformed as follows:<br>\n",
    "    $\\begin{eqnarray}\n",
    "      z^{[l]}_{i,k} &= & \\sum_{j=1}^{n^{[l-1]}} a^{[l-1]}_{i,j} w^{[l]}_{j,k} + b^{[l]}_k \\; \\;,\\; k \\in \\{1,2, \\dots,n^{[l]}\\} \\\\\n",
    "      a^{[l]}_{i,k} &= & \\widehat{h}^{[l]}(z^{[l]}_{i,k}) \\; \\;,\\; k \\in \\{1,2, \\dots,n^{[l]}\\}\n",
    "    \\end{eqnarray}$<br>\n",
    "  i.e.:\n",
    "    + at **each** node $k$ of layer $l$ the input vector $a^{[l-1]}_i$ is <font color=\"green\"><b>weighted</b></font> and a <font color=\"green\"><b>bias</b></font> is added ($ \\Rightarrow z^{[l]}_{i,k}$)\n",
    "    + subsequently, at **each** node $k$ of layer $l$ the corresponding value $z^{[l]}_{i,k}$ is <font color=\"green\"><b>transformed</b></font> using a **non-linear** activation function\n",
    "      $\\widehat{h}^{[l]}$.<br> The output of the activation function for example $i$ at node $k$ of layer $l$ is labeled $a^{[l]}_{i,k}$.<br>\n",
    "    + given that the $l$-th layer contains $n^{[l]}$\n",
    "      nodes, the transformation of example $i$ at layer $l$ results into a vector of dimension $n^{[l]}$.\n",
    "\n",
    "**Vectorized Form**   \n",
    "- This procedure can easily be <font color=\"green\"><b>vectorized</b></font>:<br> $m$ example vectors can be transformed simultaneously/in batch at the same layer .\n",
    "\n",
    "- <font color=\"green\"><b>Algorithm</b></font>:\n",
    "  + Let $X$ be an input matrix of $m$ examples (one example per row) and $n^{[0]}$ columns.\n",
    "  + $A^{[0]}:=X$ $,\\hspace{0.5in}A{[0]} \\in \\mathbb{R}^{m\\times n^{[0]}}$<br>\n",
    "    $\\texttt{for(l=1,..,L)}$<br>\n",
    "    $\\texttt{do}$<br>\n",
    "    $\\begin{eqnarray}\n",
    "      \\hspace{0.35in}Z^{[l]} &= & A^{[l-1]}W^{[l]} + b^{[l]} \\\\\n",
    "      A^{[l]} &= &\\widehat{h}^{[l]}(Z^{[l]})\n",
    "     \\end{eqnarray}$<br>\n",
    "    $\\texttt{done}$\n",
    "  + Calculate $\\mathcal{C}$ using $A^{[L]}$ and $Y$.\n",
    "\n",
    "**Summary**   \n",
    "- In order to learn, the network requires weight, biases matrices and activation functions for each layer $l$:  \n",
    "  * <font color=\"green\"><b>Weight matrices</b></font>: $W^{[l]} \\in \\mathbb{R}^{n^{[l-1]}\\times n^{[l]}}$\n",
    "  * <font color=\"green\"><b>Bias vectors</b></font>: $b^{[l]} \\in \\mathbb{R}^{ 1 \\times n^{[l]}}$\n",
    "  * <font color=\"green\"><b>Non-linear activation functions</b></font>: $\\widehat{h}^{[l]}$\n",
    "- The weight matrices and the bias vectors are known as the <font color=\"green\"><b>PARAMETERS</b></font> of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cc295e-a36c-4274-a8f7-48e1caf0739a",
   "metadata": {},
   "source": [
    "# 7.Back propagation\n",
    "\n",
    "The <font color=\"green\"><b>back propagation</b></font> operates in the <font color=\"green\"><b> opposite</b></font> direction of the forward propagation:\n",
    "  - it starts with the cost function $\\mathcal{C}$ and recedes all the way back to $l=1$.\n",
    "  - during the back propagation, the <font color=\"green\"><b>gradients</b></font> of the parameters $W^{[l]}, b^{[l]}$ are calculated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e599aaa-f7d5-4b0f-8fbf-b09f54b59843",
   "metadata": {},
   "source": [
    "<font color=\"teal\"><b>Note (for sake of completeness)</b></font><br>\n",
    "<details>\n",
    "  <summary>Click here to expand!</summary>\n",
    "\n",
    "In these section we will describe the backpropagation in more detail.<br>The expressions below can easily\n",
    "be derived by the application of the chain rule.\n",
    "\n",
    "- We start at the <font color=\"green\"><b>output</b></font> layer ($l=L$):\n",
    "  * $\\begin{eqnarray}\n",
    "     \\mathcal{C} & := & \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}^{(i)}\n",
    "     \\end{eqnarray}$\n",
    "  * Let:<br>\n",
    "    $\\begin{eqnarray}\n",
    "     (dA^{[L]})_{ij} &:=& \\frac{\\partial \\mathcal{L}^{(i)}} {\\partial a^{[L]}_{i,j}} \\\\\n",
    "     (dZ^{[L]})_{ij} &:=& \\frac{\\partial \\mathcal{L}^{(i)}} {\\partial z^{[L]}_{i,j}} &= &\\sum_{k=1}^{n^{[L]}}\\frac{\\partial \\mathcal{L}^{(i)}} {\\partial a^{[L]}_{i,k}} \\frac{\\partial a^{[L]}_{i,k}}\n",
    "     {\\partial z^{[L]}_{i,j}}\\\\\n",
    "     (dW^{[L]})_{i,j} & := &  \\frac{\\partial \\mathcal{C}}{\\partial w^{[L]}_{i, j}} \\\\\n",
    "     (db^{[L]})_{j} & := &  \\frac{\\partial \\mathcal{C}}{\\partial b^{[L]}_{j}} \n",
    "    \\end{eqnarray}$\n",
    "  * The application of the <font color=\"green\"><b>chain rule</b></font> results into:<br>\n",
    "    $\\begin{eqnarray}\n",
    "     dW^{[L]} & = & \\frac{1}{m}(A^{[L-1]})^T.dZ^{[L]} \\\\\n",
    "     db^{[L]} & = & \\frac{1}{m}\\sum_{i=1}^m(dZ^{[L]})_i \n",
    "    \\end{eqnarray}$\n",
    "  \n",
    "- Using the relation $Z^{[l]} = A^{[l-1]}.W^{[l]} + b^{[l]}$<br>\n",
    "  we can determine $dW^{[l]}, db^{[l]}$ for the <font color=\"green\"><b>other layers</b></font> $ l \\in \\{L-1,\\ldots,2,1\\}$.\n",
    "\n",
    "  Therefore,<br>\n",
    "  $\\texttt{for(l=L,...,2)}$<br>\n",
    "  $\\texttt{do}$<br>\n",
    "  $\\begin{eqnarray}\n",
    "      \\hspace{0.35in}dA^{[l-1]} &= & dZ^{[l]}.(W^{[l]})^T \\\\\n",
    "                     dZ^{[l-1]} &= & dA^{[l-1]} \\odot d\\widehat{H}^{[l-1]} \\\\\n",
    "                     dW^{[l-1]} & =& \\frac{1}{m}(A^{[l-2]})^T.dZ^{[l-1]} \\\\\n",
    "                     db^{[l-1]} & =& \\frac{1}{m}\\sum_{i=1}^m(dZ^{[l-1]})_i\n",
    "  \\end{eqnarray}$<br>\n",
    "  $\\texttt{done}$\n",
    "\n",
    "  where:\n",
    "  + $\\odot$ stands for the <a href=\"https://en.wikipedia.org/wiki/Hadamard_product_(matrices)\"><b>Hadamard product</b></a> (element-wise matrix multiplication)\n",
    "  + $(d\\widehat{H}^{[l-1]})_{i,j} := (\\widehat{h}^{\\prime[l-1]})_{i,j} $<br>\n",
    "  \n",
    "  <font color=\"brown\"><b>Remark:</b></font>\n",
    "    \n",
    "  + $dZ^{[l-1]} =  dA^{[l-1]} \\odot d\\widehat{H}^{[l-1]}$ <br>$\\texttt{iff}$<br>\n",
    "    $\\begin{eqnarray}\n",
    "      \\frac{\\partial a^{[l]}_{ik}}{\\partial z^{[l]}_{ij}} &= &\\widehat{h}^{[l]}(z^{[l]}_{ij})\n",
    "      \\end{eqnarray}$ which is indeed the case for `ReLU`\n",
    "\n",
    "</details>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c331abce-373a-413a-bcd5-c5b25ade6fcd",
   "metadata": {},
   "source": [
    "# 8.Updating the parameters\n",
    "\n",
    "In order to update the parameters, we used in <a href=\"lecture1.ipynb\"><b>Lecture 1</b></a>:\n",
    "+ <font color=\"green\"><b>simple gradient descent</b></font>\n",
    "+ the complete <font color=\"green\"><b>training data at once</b></font><br>\n",
    "\n",
    "For the general case, we need to add a few important <font color=\"red\"><b>additions/modifications</b></font>:<br>\n",
    "\n",
    "- **Epochs & the batch size $m$**\n",
    "  * In <font color=\"green\"><b>regular/batch gradient descent</b></font> we consider **all examples at once**:\n",
    "    + to calculate the gradients\n",
    "    + and update the parameters.<br>\n",
    "    In the case of DL, this approach in <font color=\"red\"><b>generally impossile</b></font> to\n",
    "    the sheer size of the data.\n",
    "\n",
    "  * <font color=\"green\"><b>In praxi</b></font>, the following approach is followed in DL:\n",
    "    + the complete dataset ($N$ examples) is split up into `mini-batches` of size $m$. \n",
    "    + it takes $\\kappa:= \\frac{N}{m}$ `mini-batches` to proceed through the dataset (assuming $m | N$).\n",
    "    + in each `mini-batch`, the gradients will be calculated and the parameters updated.\n",
    "    + A <font color=\"green\"><b>single pass</b></font> through the <font color=\"green\"><b>ENTIRE data set</b></font> is called an <font color=\"green\"><b>epoch</b></font>.\n",
    "    + When $m=1$, the gradient procedure is known as <font color=\"green\"><b>stochastic gradient descent</b></font>.<br>\n",
    "      (This is <font color=\"red\"><b>NOT a great approach</b></font> - <font color=\"red\"><b>volatility</b></font> of the loss)\n",
    "    + $m$ is a <font color=\"orangered\"><b>hyper parameter</b></font>.<br>\n",
    "      It is often set to be a power of $2$ (data alignment with the memory architecture) e.g. $64,128,256,512,1024$.\n",
    "\n",
    "  * So, we end up with the following <font color=\"green\"><b>training algorithm</b></font>:\n",
    "\n",
    "    $\\texttt{for(iepoch=1,...,num\\_epochs)}$<br>\n",
    "    $\\texttt{do}$<br>\n",
    "    $\\hspace{0.35in}\\texttt{for(ibatch=1,...,num\\_batches)}$<br>\n",
    "    $\\hspace{0.35in}\\texttt{do}$<br>\n",
    "    $\\hspace{0.7in}\\texttt{Calculate Cost (forward)}$<br>\n",
    "    $\\hspace{0.7in}\\texttt{Calculate gradients (backward)}$<br>\n",
    "    $\\hspace{0.7in}\\texttt{Update parameters}$<br>\n",
    "    $\\hspace{0.35in}\\texttt{done}$<br>\n",
    "    $\\texttt{done}$<br>\n",
    "\n",
    "- **Normalize the input**\n",
    "  * In order to speed up the optimization it is recommended to normalize the inputs.<br>\n",
    "    There are different approaches (depending on the problem) to do so, e.g.:\n",
    "    - Standardization: calculate the <font color=\"green\"><b>mean</b></font> and the <font color=\"green\"><b>variance</b></font> of the (mini-)batch for each \"feature\" ($j$):\n",
    "      \n",
    "      $\\begin{eqnarray}\n",
    "        \\overline{x_j}     & := &\\frac{1}{m} \\sum_{i=1}^m x_{i,j}  \\\\\n",
    "        s^2_j              & := & \\frac{1}{m} \\sum_{i=1}^m \\Big( x_{i,j} - \\overline{x_j} \\Big)^2 \\\\\n",
    "        \\widetilde{x_{i,j}} & := & \\frac{ x_{i,j} - \\overline{x_j}}{ \\sqrt{s^2_j+\\epsilon}}\n",
    "       \\end{eqnarray}$\n",
    "\n",
    "      1. After $1$ epoch: the mean and variance of the <b>complete</b> training set are known.\n",
    "      2. The mean and variance of the global training set should be used:\n",
    "         - for the cross-validation and testing set\n",
    "         - during inference\n",
    "         \n",
    "    - For images, the following approach is often used:<br>\n",
    "      $\\begin{eqnarray}\n",
    "         \\widetilde{x_{i,j}} & = & \\displaystyle \\frac{ x_{i,j} - \\min_{i}(x_{i,j})}{\\max_{i} (x_{i,j})- \\min_i(x_{i,j})}\n",
    "      \\end{eqnarray}$\n",
    "\n",
    "- **Non-convexity of the objective function**<br>\n",
    "  * In <a href=\"./lecture1.ipynb\"><b>Lecture 1</b></a>, our objective/loss function was <font color=\"green\"><b>convex</b></font>:\n",
    "    when we reached the <br>\n",
    "    <font color=\"green\"><b>local minimum</b></font> we were assured to obtain the <font color=\"green\"><b>global minimum</b></font> of the objective/loss function.<br>\n",
    "  \n",
    "  * In general, the cost function for deep neural nets is <font color=\"red\"><b>NOT convex</b></font>:<br>we can never be certain          that we have reached the global minimum of this highly dimensional surface.\n",
    "\n",
    "\n",
    "- **Use a more advanced version of gradient descent**<br>\n",
    "  * The <font color=\"green\"><b>rate of convergence</b></font> for simple gradient descent (<font color=\"teal\"><b>first-order method</b></font>) is very slow.\n",
    "  * There are <font color=\"teal\"><b>second-order</b></font> optimization methods:\n",
    "    + <font color=\"green\"><b>pro</b></font>: <font color=\"green\"><b>speed up</b></font> the optimization process.\n",
    "    + <font color=\"red\"><b>con</b></font>: very <font color=\"red\"><b>expensive</b></font> (require the\n",
    "      calculation of the Hessian for the parameters).\n",
    "  * Use a <font color=\"green\"><b>more performant version</b></font> of gradient descent to <font color=\"green\"><b>enhance   the rate of convergence</b></font>.<br>\n",
    "    The most commonly used version is called <a href=\"https://arxiv.org/pdf/1412.6980\"><b>Adam</b></a>. (See <a href=\"./ewma.ipynb\"><b>ewma.ipynb</b></a> for more details)\n",
    "\n",
    "- **Use a non-constant learning rate**<br>\n",
    "  Its rationale:<br>\n",
    "  * Far from minimum, take <font color=\"green\"><b>larger</b></font> steps\n",
    "  * Close to minimum, take <font color=\"green\"><b>smaller</b></font> steps (avoid oscillations around the minimum)\n",
    "  * Therefore, schedule the learning rate ($\\alpha$) e.g.<br>\n",
    "      $\\begin{eqnarray}\n",
    "        \\alpha & = & \\displaystyle \\alpha_s \\kappa^{\\texttt{iepoch}}\n",
    "       \\end{eqnarray}$<br>\n",
    "       where $\\alpha_s:= \\texttt{const}$, $0 < \\kappa < 1$.\n",
    "          \n",
    "- **Initialization of the weights**<br>\n",
    "  * + $W^{[l]}$ should be initialized (randomly) and <font color=\"red\"><b>NOT</b></font> be set to $0.0$ (<font color=\"green\"><b>get rid of symmetry</b></font>)<br>\n",
    "       $b^{[l]}$ can be set to $0.0$.\n",
    "    + However, random initialization of $W^{[l]}$ can lead to:<br>\n",
    "      1. <font color=\"red\"><b>slow convergence/different results</b></font> (due to the non-convexity of the loss function).\n",
    "      2. for large networks: <font color=\"red\"><b>vanishing/exploding</b></font> gradients.\n",
    "       \n",
    "  * A note on <font color=\"green\"><b>vanishing/exploding</b></font> gradients<br>\n",
    "    Let's consider a simple $\\texttt{Gedankenexperiment}$:\n",
    "    + Let $b^{[l]}:= 0$, for $l \\in \\{1,2, \\ldots, L\\}$\n",
    "    + Let $h^{[l]}:= 1$, for $l \\in \\{1,2, \\ldots, L\\}$ <br>\n",
    "      Then we get:<br>\n",
    "      $\\begin{eqnarray}\n",
    "        A^{[L]} = X^{[0]}W^{[1]} W^{[2]} \\ldots W^{[L]}\n",
    "      \\end{eqnarray}$\n",
    "    + If $W^{[l]}$ is $\\texttt{diagonal}$, and<br>\n",
    "      $W^{[l]}=\\varrho \\widehat{\\mathbb{1}}$, for $l \\in \\{1,2,\\ldots,L\\}$,<br>\n",
    "      then:<br>\n",
    "      $\\begin{eqnarray}\n",
    "          \\lim_{L \\to \\infty} A^{[L]}_{ii} &=& 0 , \\;\\;\\textrm{if}\\;\\; 0<=\\varrho < 1 \\\\\n",
    "          \\lim_{L \\to \\infty} A^{[L]}_{ii} &=& \\infty ,   \\;\\;\\textrm{if}\\;\\; \\varrho > 1\n",
    "       \\end{eqnarray}$<br>\n",
    "      the former case will lead to <font color=\"red\"><b>vanishing gradients</b></font>.<br>\n",
    "      the latter case will lead to <font color=\"red\"><b>exploding gradients</b></font>. \n",
    "  * In order to circumvent these issues, the following (heuristic) <font color=\"green\"><b>weight initializations</b></font> were introduced:<br>    \n",
    "    + <a href=\"https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\"><b>Glorot/Xavier</b></a>:\n",
    "      - $b^{[l]} = 0.0$\n",
    "      - $W^{[l]} = \\texttt{Unif}\\Big ( \\displaystyle - \\sqrt{\\frac{6}{n^{[l-1]} +n^{[l]}}} , \\sqrt{\\frac{6}{n^{[l-1]} +n^{[l]}}} \\Big )$\n",
    "    * <a href=\"https://arxiv.org/pdf/1502.01852\"><b>He</b></a>:\n",
    "      - $b^{[l]} = 0.0$\n",
    "      - $W^{[l]} = \\texttt{Norm}\\Big ( \\mu =0 , \\sigma^2 =  \\displaystyle \\frac{2}{n^{[l-1]}}\\Big )$ for $a^{[l]} \\in \\texttt{ReLu}$ family.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70a8515-de23-48e6-aa82-779ee989eef2",
   "metadata": {},
   "source": [
    "#### B.Compile the model (in Keras)\n",
    "The `compile` method's task it to provide the model with:\n",
    "- an <font color=\"teal\"><b>optimization method/optimizer</b></font> (e.g. `SGD`, `RMSprop`,`Adam`, `AdamW`, $\\ldots$)<br>\n",
    "  (For more info on the optimizers, see <a href=\"https://keras.io/api/optimizers/\"><b>here</b></a>)\n",
    "- a <font color=\"teal\"><b>loss</b></font> function (e.g. `binary_crossentropy`, `categorical_crossentropy`, `mean_squared_error` , $\\ldots$)<br>\n",
    "  (For more info on the loss function, see <a href=\"https://keras.io/api/losses/\"><b>here</b></a>)<br>\n",
    "  + `categorical cross-entropy`: multi-class classification using <font color=\"green\"><b>hot-encoding</b></font>\n",
    "  + `sparse categorical cross-entropy`: multi-class classification using <font color=\"green\"><b>sparse encoding</b></font><br>Example: assume we have $10$ classes and the class=$3$\n",
    "    * `categorical cross-entropy`:<br> $y= \\begin{bmatrix} 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0\\end{bmatrix}^T$\n",
    "    * `sparse categorical cross-entropy`:<br> $y = \\begin{bmatrix} 3 \\end{bmatrix}$\n",
    "- <font color=\"teal\"><b>metrics</b></font> (e.g. `accuracy` $\\ldots$)<br>\n",
    "  (For more info on the metrics, see <a href=\"https://keras.io/api/metrics/\"><b>here</a></a>)\n",
    "\n",
    "`compile`: (For more info on the method, see <a href=\"https://keras.io/api/models/model_training_apis/#compile-method\"><b>here</b></a>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a4db0e-8e7c-482d-b9db-4da1689f305b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilation of the model\n",
    "optimizer = Adam(learning_rate=0.075)             # Using the Adam optimizer and set learning rate\n",
    "mymodel.compile( optimizer=optimizer, \n",
    "                 loss='categorical_crossentropy', # loss function for multiclass with one-hot encoded vectors\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "print(f\"Info on the compilation settings\")\n",
    "print(f\"  Optimizer Config:\\n\\t{mymodel.optimizer.get_config()}\\n\")\n",
    "print(f\"  Loss Function:\\n\\t'{mymodel.loss}'\\n\")\n",
    "print(f\"  Metrics:\\n\\t{mymodel.metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e34baaa-5e25-4f99-ba88-eb4e236a26d5",
   "metadata": {},
   "source": [
    "#### C.Fit/train the model (in Keras)\n",
    "- The `fit` method is used to train the model.<br>\n",
    "  (For more info on the method, see <a href=\"https://keras.io/api/models/model_training_apis/#fit-method\"><b>here</b></a>)\n",
    "- This method has important arguments, e.g.:<br>\n",
    "  * `epochs`: number of passes through the <font color=\"green\"><b>complete</b></font> data set.\n",
    "  * `batch_size`: number of samples used per gradient update. (Default: $[32]$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fa8876-0eba-478b-b4da-49eb00edf18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the code to perform the training in Keras\n",
    "#history = mymodel.fit(X_train_set, y_train, epochs=100, verbose=0, batch_size=32)\n",
    "#loss = history.history['loss']\n",
    "#accuracy=history.history['accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6ec27b-8433-4a79-83ea-356cbd808900",
   "metadata": {},
   "source": [
    "# 9.Practical considerations\n",
    "\n",
    "## 9.1.Training/Cross-Validation/Testing\n",
    "* The <font color=\"green\"><b>data</b></font> are split up into three <font color=\"green\"><b>partitions</b></font>:\n",
    "  - <font color=\"green\"><b>training</b></font> set: data to train a model\n",
    "  - <font color=\"green\"><b>cross-validation/dev</b></font> set: data to cross-validate the different models<br>\n",
    "                           (select a model based on the hyperparameters)\n",
    "  - <font color=\"green\"><b>testing</b></font>: data to test the final model.<br>\n",
    "    <font color=\"red\"><b>NEVER</b></font> to be used for either training or cross-validation purposes.\n",
    "    \n",
    "  <font color=\"orangered\"><b>Fundamental assumption</b></font>:<br>\n",
    "  The data for each of these partitions <font color=\"green\"><b>must</b></font> belong to the same (statistical) distribution.\n",
    "\n",
    "* How to split the data set?\n",
    "  - For data sets ($<=60,000$ entries), the following ratios are often chosen:\n",
    "    * $60$/$20$/$20$\n",
    "    * $70$/$15$/$15$\n",
    "  - For large data sets (>$1,000,000$), the training set may take up $99\\%$ or $99.5\\%$ of the data. \n",
    "\n",
    "* *Mention folding*\n",
    "  \n",
    "## 9.2.Bias/Variance:\n",
    "\n",
    "* In <font color=\"orangered\"><b>ML/DEEP LEARNING</b></font>:\n",
    "  - the concepts of <font color=\"green\"><b>bias</b></font> and\n",
    "    <font color=\"green\"><b>variance</b></font> are borrowed from statistics.\n",
    "  - though similar, they are <font color=\"red\"><b>NOT</b></font> the same (see <a href=\"./bias-variance.ipynb\"><b>Bias/Variance in statistics</b></a>).\n",
    "* <font color=\"green\"><b>BIAS</b></font>: the distance between the raw training data and its corresponding estimates/predictions by our model.<br>\n",
    "  - $\\texttt{Bias} \\sim \\displaystyle \\frac{1}{n} \\sum_{i=1}^n (\\widehat{y}_i - y_i)$, for $i \\in$ training data.\n",
    "  - The <font color=\"red\"><b>LARGER THE BIAS</b></font>, the larger the inadequacy of our model. ($\\Rightarrow$ <font color=\"green\"><b>UNDERFITTING</b></font>)\n",
    "* <font color=\"green\"><b>VARIANCE</b></font>: the difference in error between the predicted training data and the\n",
    "    predicted data of a modified data set.<br>Both predictions are based on a **SAME** model.<br>\n",
    "  - In essence, the variance is a measure\n",
    "    of the **sensitivity to fluctuations** in the data set.\n",
    "  - The <font color=\"red\"><b>LARGER THE VARIANCE</b></font>, the larger the gap/error between the predicted data from the training set<br> and the data from\n",
    "    the cross-validation set ($\\Rightarrow$ <font color=\"green\"><b>OVERFITTING</b></font>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a22c51-1d5e-4408-b79f-b818c1716d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SZ=21\n",
    "rnd.seed(13)\n",
    "noise = 0.05*rnd.normal(size=SZ)\n",
    "ind_train = [2,8,9,10,11,12,18]\n",
    "ind_test = np.setdiff1d(np.arange(SZ), ind_train)\n",
    "\n",
    "x = np.linspace(0.,2.0,21)\n",
    "y = x**2 -2*x + 1 + noise\n",
    "# Create training and testing set\n",
    "x_train = x[ind_train]\n",
    "y_train = x_train**2 -2*x_train +1 + noise[ind_train]\n",
    "x_test = x[ind_test]\n",
    "y_test = x_test**2 -2*x_test +1 + noise[ind_test]\n",
    "\n",
    "# Fit sel. points to polynomials of a order 1,2 and 4\n",
    "coeffs1 = np.polyfit(x_train, y_train, 1)\n",
    "p1 = np.poly1d(coeffs1)\n",
    "y1 = p1(x)\n",
    "\n",
    "coeffs2 = np.polyfit(x_train, y_train, 2)\n",
    "p2 = np.poly1d(coeffs2)\n",
    "y2 = p2(x)\n",
    "\n",
    "coeffs4 = np.polyfit(x_train, y_train, 4)\n",
    "p4 = np.poly1d(coeffs4)\n",
    "y4 = p4(x)\n",
    "\n",
    "plt.plot(x_train, y_train, '.', label=\"Training data\")\n",
    "plt.plot(x_test, y_test, '+', label=\"Test data\")\n",
    "plt.plot(x, y1, '--', label=\"Poly. deg=1 (\"+r\"$\\mathtt{Underfitting}$\"+\")\")\n",
    "plt.plot(x, y2, '--', label=\"Poly. deg=2\")\n",
    "plt.plot(x, y4, '--', label=\"Poly. deg=4 (\"+r\"$\\mathtt{Overfitting}$\"+\")\" )\n",
    "plt.title(r\"Example of $\\mathtt{Underfitting}$/$\\mathtt{Overfitting}$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93369900-353b-40b6-87e0-53bc8c7c29dd",
   "metadata": {},
   "source": [
    "## 9.3.Hints to develop neural nets \n",
    "\n",
    "* <font color=\"green\"><b>Algorithm</b></font>:<br>\n",
    "  $\\texttt{Start with simple model}$<br>\n",
    "  \n",
    "  $\\texttt{while(Bias == High)}$<br>\n",
    "  $\\texttt{\\{}$<br>\n",
    "     $\\hspace{0.35in}\\texttt{Reduce High Bias}$<br>\n",
    "  $\\texttt{\\}}$<br>\n",
    "\n",
    "  $\\texttt{while(Variance==High)}$<br>\n",
    "  $\\texttt{\\{}$<br>\n",
    "     $\\hspace{0.35in}\\texttt{Reduce High Variance}$<br>\n",
    "  $\\texttt{\\}}$<br>\n",
    "    \n",
    "* How to deal with High Bias and Variance?\n",
    "  - <font color=\"red\"><b>High Bias</b></font> ($\\Rightarrow$ training model needs improvement)\n",
    "    * increase the number of <font color=\"green\"><b>hidden layers</b></font>\n",
    "    * increase the number of <font color=\"green\"><b>units per layer</b></font>\n",
    "    * increase the number of <font color=\"green\"><b>epochs</b></font> \n",
    "    * may have to change <font color=\"green\"><b>NN architecture</b></font>\n",
    "  - <font color=\"red\"><b>High Variance</b></font> (your model does not work well at crosss-validation)\n",
    "    * use <font color=\"green\"><b>regularization</b></font> to reduce overfitting.\n",
    "    * may have to <font color=\"green\"><b>increase the training data</b></font> to have a more versatile model.<br>\n",
    "      <font color=\"orangered\"><b>Note:</b></font><br>\n",
    "      The technique of <a href=\"https://en.wikipedia.org/wiki/Data_augmentation\"><b>data augmentation</b></a> may be an option.<br>\n",
    "      + images: rotation, flipping, cropping, filtering, ...\n",
    "      + audio: adding noise, shifting pitch, (in/de)crease speed, ...\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582c83ab-de11-4821-9664-0111ea433261",
   "metadata": {},
   "source": [
    "#### D.Evaluate/test the model\n",
    "The `evaluate` method returns the `loss` value & `metrics` values for the model in test mode.<br>\n",
    "(For more info on the method, see <a href=\"https://keras.io/api/models/model_training_apis/#evaluate-method\"><b>here</b></a>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c684a7e-ef2d-4a98-9bc8-cabf853d17c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the code \n",
    "# test_loss, test_accuracy = mymodel.evaluate(X_test, y_test)\n",
    "# print(f\"Evaluation of the test set\")\n",
    "# print(f\"  Accuracy : {test_accuracy:8.4f}\")\n",
    "# print(f\"  Loss     : {test_loss:8.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b95ecc4-4210-4256-a048-05c5719bf8c8",
   "metadata": {},
   "source": [
    "## 9.4.Hyper parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2378ffaf-7616-4082-b5e5-e99e86ac34c3",
   "metadata": {},
   "source": [
    "* Deep Learning has a <font color=\"green\"><b>lot of hyperparameters</b></font>\n",
    "  + learning rate ($\\alpha$), learning rate decay\n",
    "  + momentum terms: $\\beta_1$, $\\beta_2$\n",
    "  + $L$, $n^{[l]}$ for $l \\in \\{1,2, \\ldots, L\\}$\n",
    "  + $m$ (Size of the mini-batch)\n",
    "  + ...\n",
    "* <font color=\"green\"><b>Importance</b></font>: (in descending order)<br>\n",
    "  $\\alpha$ > $\\beta_1$, $\\beta_2$ > $m$ > $n^{[l]}$ > $L$\n",
    "* How to <font color=\"green\"><b>optimize</b></font> the hyper parameters:\n",
    "  - Avoid a grid (curse of combinatorics)\n",
    "  - Choose first random values (cfr. Monte-Carlo vs. trapezoid integration)\n",
    "  - Then focus on smaller areas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a9a3bc-582f-4b03-a7f9-c7ceab707711",
   "metadata": {},
   "source": [
    "#### <font color=\"darkred\"><b>Exercise 3</b></font>\n",
    "- Several DNNs will be used to perform binary classification on the<br>\n",
    "  ($\\texttt{X\\_train\\_s,y\\_train}$) data set:\n",
    "- We will have:\n",
    "  * $1$ <font color=\"teal\"><b>hidden<b></font> layer in each model -> activation function: $\\texttt{ReLu}$\n",
    "  * containing $n_1$ units in layer $1$ :<br> $2, 5, 10, 16, 32, 64, 128$ .\n",
    "- Train these $7$ models and obtain their loss & accuracy\n",
    "- You can use a $\\texttt{learning\\_rate}=0.005$ and $\\texttt{num\\_epochs}=30$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363daf69-f03b-474f-a976-d8d67d6e05ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optim_1hlayer_model( X_train: np.ndarray,  # features matrix (np.ndarray) \n",
    "                         y_train: np.ndarray,  # labels (np.ndarray)\n",
    "                         n1: int,              # number of neurons in layer 1 (integer)\n",
    "                         lr: float = 0.005,    # learning rate (float)      [0.005]\n",
    "                         num_epochs: int = 20, # number of epochs (integer) [20]\n",
    "                         batch_sz: int = 32,   # batch size (integer)       [32]\n",
    "                         verbosity: int = 0    # verbosity level (integer)  [0]\n",
    "                       ) -> dict[str, Any]:    \n",
    "   \n",
    "    \"\"\"\n",
    "    Trains and optimizes a Keras Sequential model with one hidden layer for binary classification.\n",
    "\n",
    "    The model architecture consists of:\n",
    "    1. An Input Layer (shape 2).\n",
    "    2. A Dense Hidden Layer with 'n1' neurons and 'relu' activation.\n",
    "    3. A Dense Output Layer with 1 neuron and 'sigmoid' activation.\n",
    "\n",
    "    The model is compiled using the Adam optimizer, binary_crossentropy loss, and accuracy metrics.\n",
    "\n",
    "    Args:\n",
    "        X_train (Any): Training feature data (e.g., shape (m, 2), a numpy array).\n",
    "        y_train (Any): Training target labels (e.g., shape (m, 1), a numpy array).\n",
    "        n1 (int): The number of neurons in the single hidden layer.\n",
    "        lr (float, optional): The learning rate for the Adam optimizer. Defaults to 0.005.\n",
    "        num_epochs (int, optional): The number of epochs for training. Defaults to 20.\n",
    "        batch_sz (int, optional): The size of a mini-batch. Defaults to 32.\n",
    "        verbosity (int, optional): Keras training verbosity (0=silent, 1=progress bar, 2=one line per epoch). Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, Any]: A dictionary containing:\n",
    "          'model' (keras.Model): The trained Keras model object.\n",
    "          'loss' (list[float]): List of training loss values per epoch.\n",
    "          'accuracy' (list[float]): List of training accuracy values per epoch.        \n",
    "    \"\"\"\n",
    "    \n",
    "    tstart = time.time()\n",
    "    \n",
    "    # A.Set-up the model\n",
    "    name_layer1 = \"hila1_nodes=\" + str(n1)  # Hidden Layer 1 with n1 neurons\n",
    "    name_model =  \"1HiLa_\" + str(n1)        # Name of the model\n",
    "    \n",
    "    print(f\"  Starting the optimization for model {name_model}\") \n",
    "   \n",
    "    # A. Setup your model\n",
    "    # model =           <-- Here comes your code\n",
    "\n",
    "    # B.Compile the model\n",
    "    # model.compile     <-- Here comes your code\n",
    "\n",
    "    # C.Fit the model\n",
    "    # hist = model.fit  <-- Here comes your code\n",
    "     \n",
    "    # Extract loss & accuracy \n",
    "    loss = hist.history['loss']\n",
    "    acc = hist.history['accuracy']\n",
    "\n",
    "    # Return model, loss and accuracy\n",
    "    d = {\"model\": model, \n",
    "         \"loss\": loss, \n",
    "         \"accuracy\": acc}\n",
    "    \n",
    "    tend = time.time()\n",
    "    print(f\"  Finishing the optimization for model {name_model} -> Elapsed Time:{tend-tstart:6.2f} sec\\n\")\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4d8897-6479-49b2-8e91-e3d09e6715ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/lec2/ex3.py\n",
    "# Ex3: Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824495f0-cd4a-41a4-a779-161b4a2df9ce",
   "metadata": {},
   "source": [
    "##### **Train** the following $7$ models each with 1 <font color=\"teal\"><b>hidden</b></font> layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9959bcc7-0182-45c4-a674-50e23a37e87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the code to train the different models (each having 1 hidden layer)\n",
    "lstNeurons = [2,5,10,16,32,64,128]\n",
    "lstResH1Layers = []\n",
    "for i, n1 in enumerate(lstNeurons):\n",
    "    print(f\"Model:'{i+1}'\")\n",
    "    d = optim_1hlayer_model(X_train_s, y_train, n1, lr=0.005, num_epochs=30)\n",
    "    lstResH1Layers.append(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9583f4b8-26a4-4c2a-80f0-7bad81d47890",
   "metadata": {},
   "source": [
    "##### Plot the **loss** and the **accuracy** of the trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960e1573-c2ca-4970-a667-21c0a6ae1f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Accuracy training data\n",
    "plt.title(\"Training data: accuracy\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "for item in lstResH1Layers:\n",
    "    it = np.arange(len(item['loss']))\n",
    "    label = item['model'].name\n",
    "    acc = item['accuracy']\n",
    "    plt.plot(it,acc,\"-\", label=label)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785d8959-9e10-41d0-9d74-e447ef786dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Loss training data\n",
    "plt.title(\"Training data: loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "for item in lstResH1Layers:\n",
    "    it = np.arange(len(item['loss']))\n",
    "    label = item['model'].name\n",
    "    loss = item['loss']\n",
    "    plt.plot(it,loss,\"-\", label=label)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b40c64-c9df-4c79-bfa4-c92507973e6c",
   "metadata": {},
   "source": [
    "#### <font color=\"darkred\"><b>Exercise 4</b></font>\n",
    "- Several DNNs will be used to perform <font color=\"green\"><b>binary classification</b></font> on the<br>\n",
    "  ($\\texttt{X\\_train\\_s,y\\_train}$) data set:\n",
    "- We will have:\n",
    "  * $2$ <font color=\"teal\"><b>hidden</b></font> layers in each model\n",
    "    + <font color=\"teal\"><b>hidden</b></font> layer $1$: $n_1$ units -> activation function: $\\texttt{ReLu}$\n",
    "    + <font color=\"teal\"><b>hidden</b></font> layer $2$: $n_2$ units -> activation function: $\\texttt{ReLu}$\n",
    "- Train the following $5$ models and obtain their loss & accuracy\n",
    "  * ($n_1=10$, $n_2=10$)\n",
    "  * ($n_1=16$, $n_2=16$)\n",
    "  * ($n_1=32$, $n_2=32$)\n",
    "  * ($n_1=64$, $n_2=64$)\n",
    "  * ($n_1=128$, $n_2=128$)\n",
    "- We use use a $\\texttt{learning\\_rate}=0.005$ and $\\texttt{num\\_epochs}=32$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c8b4af-0523-4571-aae9-474d26c2f16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex. 4: Solution\n",
    "def optim_2hlayer_model(X_train, y_train, n1, n2, lr=0.005, num_epochs=20, verbosity=0):\n",
    "\n",
    "    tstart = time.time()\n",
    "    # A.Set-up the model\n",
    "    name_layer1 = \"hila1_nodes=\" + str(n1)  # Hidden Layer 1 with n1 neurons\n",
    "    name_layer2 = \"hila2_nodes=\" + str(n2)  # Hidden Layer 2 with n2 neurons\n",
    "    name_model =  \"2HiLa_\" + str(n1) + \"-\" + str(n2)\n",
    "\n",
    "    print(f\"  Starting the optimization for model {name_model}\")\n",
    "    \n",
    "    # A. Set-up yor model\n",
    "    # model =             <-- Here comes your code\n",
    "\n",
    "    # B.Compile the model\n",
    "    # model.compile()     <-- Here comes your code\n",
    "\n",
    "    # C.Fit the model\n",
    "    # hist = model.fit()  <-- Here comes your code\n",
    "\n",
    "    # Extract loss & accuracy\n",
    "    loss = hist.history['loss']\n",
    "    acc = hist.history['accuracy']\n",
    "\n",
    "    # Return model, loss and accuracy\n",
    "    d = {\"model\":model,\n",
    "         \"loss\": loss,\n",
    "         \"accuracy\": acc}\n",
    "    tend = time.time()\n",
    "    print(f\"  Finishing the optimization for model {name_model} -> Elapsed Time:{tend-tstart:6.2f} sec\\n\")\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f2a75d-4361-4462-ad6a-274cd5e4c3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/lec2/ex4.py\n",
    "# Ex. 4: Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2ece4a-478b-4ba9-bde6-28de7debadfb",
   "metadata": {},
   "source": [
    "##### **Train** the following models with 2 <font color=\"teal\"><b>hidden</b></font> layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1967b7de-29b3-43b9-986b-78885a76d55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstNeur1 = [10,16,32,64,128]\n",
    "lstNeur2 = [10,16,32,64,128]\n",
    "lstInd = range(len(lstNeur1))\n",
    "lstResH2Layers = []\n",
    "for i, n1, n2 in zip(lstInd,lstNeur1,lstNeur2):\n",
    "    print(f\"Model:'{i+1}'\")\n",
    "    d = optim_2hlayer_model(X_train_s, y_train, n1, n2, lr=0.005, num_epochs=120, verbosity=0)\n",
    "    lstResH2Layers.append(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc559377-2633-4bb3-9f75-82f2c501d929",
   "metadata": {},
   "source": [
    "##### Plot the **loss** and the **accuracy** of the trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3727c960-7103-41f5-bbc4-0b5d0daab96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy training data\n",
    "plt.title(\"Accuracy of the training data\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "for item in lstResH2Layers:\n",
    "    it = np.arange(len(item['accuracy']))\n",
    "    label = item['model'].name\n",
    "    acc = item['accuracy']\n",
    "    plt.plot(it,acc,\"-\", label=label)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e85ff7f-dd1b-4817-9acc-d19c4a982c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss training data\n",
    "plt.title(\"Loss of the training data\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "for item in lstResH2Layers:\n",
    "    it = np.arange(len(item['loss']))\n",
    "    label = item['model'].name\n",
    "    loss = item['loss']\n",
    "    plt.plot(it,loss,\"-\", label=label)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0b8973-93ff-470c-ab7a-2be97b6449dc",
   "metadata": {},
   "source": [
    "### Exercise 5: Validate the models (use of the valication set)\n",
    "- Up till now, we have trained several models.\n",
    "- We will now validate the models (find the best model among the models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10743f4-09f8-476a-8ddd-eda5103cc7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "lstAcc = []\n",
    "for item in lstResH2Layers:\n",
    "    label = item['model'].name\n",
    "    obj = item['model']\n",
    "    loss_val, acc_val = # <--- Here comes you evaluation of the cross-validation set\n",
    "    lstAcc.append(acc_val)\n",
    "    print(f\"Model:{label}\")\n",
    "    print(f\"  Loss: {loss_val}\")\n",
    "    print(f\"  Acc:  {acc_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53269994-6f27-48b6-845a-49e29999caa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/lec2/ex5.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af682e8-1f4c-4953-991e-70d0dd20b661",
   "metadata": {},
   "source": [
    "### Exercise 6: Test the best model (use the test set)\n",
    "* Find the <font color=\"green\"><b>best<b></font> model (from the validation results)\n",
    "* Check whether it <font color=\"green\"><b>generalizes</b></font> well on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c22b816-58b8-45ac-bb30-681400d52236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here comes your code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c01f00a-5729-4456-84d3-f0257591d973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/lec2/ex6.py\n",
    "# Ex 6: Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217b73d6-7832-448a-a988-a0546872886b",
   "metadata": {},
   "source": [
    "# 10.Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc4f414-3eff-4871-9f82-59a0527096eb",
   "metadata": {},
   "source": [
    "- **Regularization**<br>\n",
    "  Aim: to deal with <font color=\"red\"><b>overfitting</b></font>/<font color=\"red\"><b>high variance</b></font><br>\n",
    "  There are several approaches:\n",
    "\n",
    "  * **Using an additional regularization term**<br>\n",
    "    Append a <font color=\"green\"><b>regularization term</b></font> ($\\mathcal{R}$) to the Cost function, i.e.<br>\n",
    "    $\\begin{eqnarray}\n",
    "    \\mathcal{C} & := & \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}^{(i)} + \\mathcal{R}\n",
    " \\end{eqnarray}$\n",
    "    - $\\texttt{L}^2$ regularization ($\\texttt{Ridge}$ or $\\texttt{Tikhonov}$ regularization):<br>\n",
    "      $\\begin{eqnarray}\n",
    "        \\mathcal{R} &:= & \\displaystyle \\frac{\\lambda_2}{2m} \\sum_{l=1}^L \\sum_{i=1}^{n^{[l-1]}} \\sum_{j=1}^{n^{[l]}} \\Big(W^{[l]}_{i,j} \\Big)^2 \\nonumber \\\\\n",
    "                    & = & \\displaystyle \\frac{\\lambda_2}{2m} \\sum_{l=1}^L  \\left\\| W^{[l]}\\right\\|^2_F \n",
    "       \\end{eqnarray}$<br>\n",
    "      where $\\left\\| W^{[l]}\\right\\|^2_F$ stands for the <a href=\"https://mathworld.wolfram.com/FrobeniusNorm.html\">$\\texttt{Frobenius}$</a> norm of the matrix $W^{[l]}$.<br>\n",
    "      <font color=\"orangered\"><b>Effect</b></font>: <font color=\"teal\"><b>discourages large weights</b></font><br>\n",
    "      Original paper by *Krogh and Hertz (1991)*:<br>\n",
    "      <a href=\"https://proceedings.neurips.cc/paper_files/paper/1991/file/8eefcfdf5990e441f0fb6f3fad709e21-Paper.pdf\"><b>A Simple Weight Decay Can Improve Generalization</b></a>\n",
    "    - $\\texttt{L}^1$ regularization ($\\texttt{Lasso}$ regularization):<br>\n",
    "      $\\begin{eqnarray}\n",
    "        \\mathcal{R} &:= & \\frac{\\lambda_1}{m} \\sum_{l=1}^L \\sum_{i=1}^{n^{[l-1]}} \\sum_{j=1}^{n^{[l]}} |W^{[l]}_{i,j}|\n",
    "       \\end{eqnarray}$<br>\n",
    "      <font color=\"orangered\"><b>Effect</b></font>: <font color=\"teal\"><b>drives less important weights to zero</b></font> i.e. induces sparsity/feature selection.<br>\n",
    "      Original paper by *Tibshirani (1996)*:<br>\n",
    "      <a href=\"https://arindam.cs.illinois.edu/courses/s14cs598/paper/lasso.pdf\"><b>Regression Shrinkage and Selection via the Lasso</b></a>\n",
    "    - $\\texttt{Elastic}$ net: combination of $\\texttt{L}^1$ and $\\texttt{L}^2$ regularization (see <a href=\"https://www.stat.purdue.edu/~tlzhang/mathstat/ElasticNet.pdf\"><b>here</b></a> for the details).<br>\n",
    "      $\\begin{eqnarray}\n",
    "        \\mathcal{R} &:= & \\frac{\\lambda_1}{m} \\sum_{l=1}^L \\sum_{i=1}^{n^{[l-1]}} \\sum_{j=1}^{n^{[l]}}|W^{[l]}_{i,j}| + \\frac{\\lambda_2}{2m}  \\sum_{l=1}^L \\sum_{i=1}^{n^{[l-1]}} \\sum_{j=1}^{n^{[l]}}(W^{[l]}_{i,j})^2 \n",
    "       \\end{eqnarray}$\n",
    "\n",
    "  * **Dropout**\n",
    "    - How: <font color=\"green\"><b>turn off</b></font> activations <font color=\"green\"><b>randomly</b></font>.\n",
    "    - Original paper by *Srivastava, Hinton, et al.*:<br> <a href=\"https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf\"><b>Dropout: A Simple Way to Prevent Neural Networks from Overfitting</b></font>\n",
    "    - In essence an <font color=\"green\"><b>adaptive</b></font> version of $\\texttt{L}^2$ regularization (reduce weights) \n",
    "    - Algorithm ($\\texttt{Inverted drop-out}$)\n",
    "      + For each layer $l$:\n",
    "        + Set $p^{[l]}$: probability to <font color=\"green\"><b>keep</b></font> activation for layer $l$\n",
    "        + Calculate $A^{[l]}$ (as in the regular/non-dropout case)\n",
    "        + Generate a <font color=\"green\"><b>masking matrix</b></font> $M^{[l]} \\in \\mathbb{R}^{m \\times n^{[l]}}$\n",
    "          * Sample the elements of $M^{[l]}$ from $U(0,1)$ \n",
    "          * $\n",
    "                M^{[l]}_{i,j} = \\begin{cases}\n",
    "                             1, \\hspace{0.25in}\\texttt{for}\\;\\;\\; M^{[l]}_{i,j} \\leq p^{[l]} \\\\\n",
    "                             0, \\hspace{0.25in}\\texttt{for}\\;\\;\\; p^{[l]} \\lt M^{[l]}_{i,j} \n",
    "                \\end{cases}$\n",
    "        + Use $M^{[l]}$ to <font color=\"green\"><b>discard</b></font> some of the activations in $A^{[l]}$:<br>\n",
    "          $\\begin{eqnarray}\n",
    "            A^{[l]} &= & A^{[l]} \\odot M^{[l]}\n",
    "           \\end{eqnarray}$\n",
    "        + <font color=\"green\"><b>Rescale</b></font> $A^{[l]}$ (to correct for the missing activations):<br>\n",
    "          $\\begin{eqnarray}\n",
    "             A^{[l]} & = & \\frac{ A^{[l]}}{p^{[l]}}\n",
    "           \\end{eqnarray}$\n",
    "    - Very popular in the field of computer vision (always a shortage of data $\\Rightarrow$ <font color=\"teal\"><b>overfitting).\n",
    "\n",
    "                \n",
    "- **Batch Normalization**\n",
    "\n",
    "  Above we described the <font color=\"green\"><b>standardization/normalization</b></font> of the inputs to speed up the optimization.<br>    The application of the normalization method to the hidden layers is known as <font color=\"green\"><b>batch normalization</b></font>.\n",
    "\n",
    "  <font color=\"orangered\"><b>Still to be worked further out.</b></font>\n",
    "\n",
    "  Source:<br>\n",
    "  <a href=\"https://arxiv.org/pdf/1502.03167\"><b><it>Sergey Ioffe and Christian Szegedy (2015)</it>.<br>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</b></a>\n",
    "  \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3e789b-44c8-4454-a561-a828c855fae1",
   "metadata": {},
   "source": [
    "<font color=\"teal\"><b>Note (for sake of completeness)</b></font><br>\n",
    " <details>\n",
    "   <summary>Click here to expand!</summary>\n",
    "   - If $\\texttt{L}^{1}$, $\\texttt{L}^{2}$ or $\\texttt{elastic net}$ regularization is to be used,<br> \n",
    "     the gradient term needs to be <font color=\"green\"><b>altered</b></font>: <br>\n",
    "\n",
    "     * $\\texttt{L}^{2}$ regularization:<br>\n",
    "       $\\begin{eqnarray}\n",
    "         dW^{[l]} & = & dW^{[l]} \\,+\\,\\frac{\\lambda_2}{m} W^{[l]},\\;\\;\\;\\texttt{for}\\,\\,\\,l \\in \\{1,2,\\ldots,L\\}\n",
    "         \\end{eqnarray}$\n",
    "            \n",
    "     * $\\texttt{L}^{1}$ regularization:<br>\n",
    "           $\\begin{eqnarray}\n",
    "            dW^{[l]} & = & dW^{[l]} \\,+ \\,\\frac{\\lambda_1}{m} \\texttt{sgn}(W^{[l]}),\\;\\;\\;\\texttt{for}\\,\\,\\,l \\in                 \\{1,2,\\ldots,L\\}\n",
    "            \\end{eqnarray}$\n",
    "\n",
    "     * $\\texttt{Elastic}$ net regularization:<br>\n",
    "           $\\begin{eqnarray}\n",
    "             dW^{[l]} & = & dW^{[l]} \\,+ \\,\\frac{1}{m} \\Big [ \\lambda_1\\,\\texttt{sgn}(W^{[l]})\\, + \\,\\lambda_2  W^{[l]}\\Big],\\;\\;\\;\\texttt{for}\\,\\,\\,l \\in                 \\{1,2,\\ldots,L\\}\n",
    "            \\end{eqnarray}\n",
    "           $\n",
    "\n",
    "   - If $\\texttt{dropout}$ is used, the backpropagation step needs<br> to be <font color=\"green\"><b>altered</b></font> in the following way:<br>\n",
    "     * <font color=\"green\"><b>Switch off</b></font> those gradients of which the activations\n",
    "       were switched off during the forward propagation step.<br>\n",
    "       $\\begin{eqnarray}  \n",
    "        dA^{[l]} & = &  dA^{[l]} \\odot M^{[l]},\\;\\;\\;\\texttt{for}\\,\\,\\,l \\in \\{1,2,\\ldots,L\\}\n",
    "        \\end{eqnarray}$\n",
    "     * <font color=\"green\"><b>Rescale</b></font> the gradients<br>\n",
    "       $\\begin{eqnarray}  \n",
    "        dA^{[l]} & = &  \\frac{dA^{[l]}}{p^{[l]}},\\;\\;\\;\\texttt{for}\\,\\,\\,l \\in \\{1,2,\\ldots,L\\}\n",
    "        \\end{eqnarray}$\n",
    "   \n",
    " </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6482ece-ce1f-468a-8a0b-1cd518dd3607",
   "metadata": {},
   "source": [
    "# 11.Debugging\n",
    "The <font color=\"green\"><b>following algorithm</b></font> (based on the <font color=\"teal\"><b>numerical calculation</b></font> of the partial derivatives<br>\n",
    "of the weights and biases w.r.t. the cost function ($\\mathcal{C}$)) should be used iff:<br>\n",
    "- you write your own code\n",
    "- and want to check/debug it.\n",
    "\n",
    "It should <font color=\"red\"><b>NOT</b></font> be used to perform regular training (<font color=\"red\"><b>too expensive</b></font>).<br>\n",
    "For some background on numerical differentiation, please have a look <a href=\"./finitediff.ipynb\"><b>here</b></a>.\n",
    "  \n",
    "\n",
    "$\\texttt{Algorithm}$:\n",
    "- $\\texttt{Flatten your weight and bias vectors into one large vector }$ $\\Xi$:<br>\n",
    "  $\\begin{eqnarray}\n",
    "    \\Xi & := \\begin{pmatrix} w^{[1]}_{1,1} \\\\\n",
    "                            w^{[1]}_{2,1} \\\\\n",
    "                             \\vdots \\\\\n",
    "                            w^{[L]}_{n^{[L-1]},n^{[L]}} \\\\\n",
    "                            b^{[1]}_1 \\\\\n",
    "                            b^{[1]}_2 \\\\\n",
    "                             \\vdots \\\\\n",
    "                             b^{[L]}_{n^{[L]}}\n",
    "            \\end{pmatrix}                           \n",
    "  \\end{eqnarray}$\n",
    "- Let:\n",
    "  * $\\epsilon$ be very small ($10^{-8}, 10^{-7}$)\n",
    "  * $\\texttt{num\\_param} := |\\Xi|$ (length of the vector $|\\Xi|$)\n",
    "  * $f_i^{+} := \\mathcal{C}(\\xi_1,\\xi_2,\\ldots,\\xi_i+ \\epsilon,\\ldots, \\xi_{|\\Xi|-1},  \\xi_{|\\Xi|})$\n",
    "  * $f_i^{-} := \\mathcal{C}(\\xi_1,\\xi_2,\\ldots,\\xi_i- \\epsilon,\\ldots, \\xi_{|\\Xi|-1},  \\xi_{|\\Xi|})$\n",
    "- \n",
    "$\\texttt{for(i=1, ..., num\\_param)}$<br>\n",
    "    $\\texttt{do}$<br>\n",
    "       $\\begin{eqnarray}\n",
    "       \\;\\;\\;\\;\\;d\\xi_i & = & \\displaystyle \\frac{f_i^{+}\\,-\\, f_i^{-} }{2\\epsilon} \\\\\n",
    "        \\;\\;\\;\\;\\;\\xi_i  & = &   \\xi_i \\, - \\, \\alpha \\, d\\xi_i                    \n",
    "        \\end{eqnarray}$<br>\n",
    "    $\\texttt{done}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723977a6-62c4-4196-8523-3c21e6b70170",
   "metadata": {},
   "source": [
    "# 12.Conclusion\n",
    "\n",
    "* All <font color=\"green\"><b>fundamental techniques</b></font> to treat dense neural nets have been introduced.<br>\n",
    "* However, we have:\n",
    "  - neither treated <font color=\"orangered\"><b>Convolutional Neural Nets</b></font> (CNNs),\n",
    "  - nor discussed <font color=\"orangered\"><b>Recurrent Neural Nets</b></font> (RNNs),\n",
    "  - nor elaborated on <font color=\"orangered\"><b>Graph Neural Nets</b></font> (GNNs) and <font color=\"orangered\"><b>physics-informed neural nets</b></font> (PINNs).<br>\n",
    "    \n",
    "  The basic structure is the <font color=\"green\"><b>same</b></font>.\n",
    "* With the information from these lectures, you should be able to implement your own dense neural nets from scratch.\n",
    "* If you want to pursue this route, I recommend to start with $\\texttt{NumPy}$.<br>In a subsequent phase you proceed with one of the\n",
    "  following list: $\\texttt{CuPy}$, $\\texttt{nvmath}$, $\\texttt{C++/C}$, $\\ldots$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d90fae5-e4c9-4d6e-bf1e-bc8d70731cdc",
   "metadata": {},
   "source": [
    "# 13.Final remarks\n",
    "\n",
    "We strongly appreciate your feedback and comments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
