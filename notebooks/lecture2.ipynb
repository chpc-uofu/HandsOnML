{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fb4769-d938-4c12-9a85-52317dad236f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as ss\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "import keras\n",
    "import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb55a6c-7666-451c-8b06-8770cfdb9182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether GPU is available\n",
    "%run ./check_gpu.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b53a679-01ae-4772-a474-9d8e2dec5dac",
   "metadata": {},
   "source": [
    "# Hands-on Introduction to Deep Learning (Lecture 2)\n",
    "\n",
    "* <font color=\"teal\"><b>author: Wim R.M. Cardoen</b></font>\n",
    "* <font color=\"teal\"><b>e-mail: wcardoen [\\at] gmail.com</b></font>\n",
    "\n",
    "## Synopsis  \n",
    "In <a href=\"./lecture1.ipynb\"><b>Lecture 1</b></a>, we worked out the details of <font color=\"green\"><b>logistic regression</b></font> as\n",
    "the simplest neural net.<br>\n",
    "We introduced the following concepts:\n",
    "* **neuron** (`perceptron`)\n",
    "* **activation function** (`sigmoid`)\n",
    "* **loss function** (metric to optimize the neural net) for `binary classification`\n",
    "* **gradient descent method** (`optimization` of the weights and biases)\n",
    "* **forward propagation** (moving forward from the input to the loss function)\n",
    "* **backward propagation** (moving backwards from the loss function to the inputs to calculate the gradients)\n",
    "\n",
    "## Goal\n",
    "In this Lecture, we will generalize the previous example to a <font color=\"green\"><b>fully connected dense neural net</b></font>.\n",
    "\n",
    "## Note\n",
    "* At the end of some paragraphs you may find a section \"<font color=\"teal\"><b>Note (for sake of completeness)</b>\"</font><br>\n",
    "* I have added a few additional <font color=\"orange\"><b>notebooks</b></font>, such as\n",
    "  - <a href=\"./ewma.ipynb\"><b>ewma.ipynb</b></a>\n",
    "  - <a href=\"./bias-variance.ipynb\"><b>bias-variance.ipynb</b></a>\n",
    "  - <a href=\"./finitediff.ipynb\"><b>finitediff.ipynb</b></a>\n",
    "  \n",
    "  to elucidate on certain topics.\n",
    "* These sections and notebooks can be <font color=\"teal\"><b>skipped</b></font>. They target people who want to dig deeper<br>\n",
    "(in case you want to implement your own deep learning code from scratch in e.g. <a href=\"https://cupy.dev/\"><b>CuPy</b></a> or a compiled language)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371402ba-7712-4c37-887f-a5f7ec29e6b6",
   "metadata": {},
   "source": [
    "# 1.Dense Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd355ac-40a3-4e9b-b8a6-75576197d076",
   "metadata": {},
   "source": [
    "## Definitions:\n",
    "A **fully connected dense neural network** of <font color=\"green\"><b>depth</b></font> $L$ is a neural net where:\n",
    "- <font color=\"green\"><b>every neuron</b></font> in layer ($l$) is connected to <font color=\"green\"><b>every</b></font> neuron in layer ($l+1$) where $l=\\{0,1,2,\\ldots,L-1\\}$.\n",
    "- these connections are **directed** and **acyclic** (<a href=\"https://en.wikipedia.org/wiki/Directed_acyclic_graph\"><b>DAG</b></a>), i.e. they are unidirectional from layer $l$ towards layer $l+1$ and do\n",
    "  **NOT** cycle back.\n",
    "- the number of nodes in layer $l$ (<font color=\"green\"><b>width</b></font> of layer $l$) will be designated as $n^{[l]}$.\n",
    "- types of layers:\n",
    "  + <font color=\"green\"><b>input</b></font> layer: $l=0$. It is **NOT** counted as a real layer (this will become obvious soon!). \n",
    "  + <font color=\"green\"><b>output</b></font> layer: $l=L$.\n",
    "  + <font color=\"green\"><b>hidden</b></font> layers: $l \\in \\{1,2,\\ldots,L-1\\}$.\n",
    "\n",
    "Neural nets with <font color=\"green\"><b>NO hidden</b></font> layers ($L=1$) are called \n",
    "<font color=\"green\"><b>shallow</b></font>.<br> The neural net we discussed previously (Lecture 1) is an example of a shallow neural net with $n^{[0]}=2$ and $n^{[1]}=1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce341675-e458-4b9b-97ed-bfa36de4f096",
   "metadata": {},
   "source": [
    "## Example:\n",
    "The image below displays a deep neural with $L=4$ layers (neural net of depth 4).\n",
    "\n",
    "<img src=\"dnn_4layers.jpeg\" alt=\"Deep neural net with 4 layers\" style=\"width:65%; height:auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267dcf5e-5b31-49a0-a6fb-10c896b1871a",
   "metadata": {},
   "source": [
    "* <font color=\"green\"><b>input</b></font> layer: layer $0$\n",
    "  + the input layer requires input vectors/examples each of length $n^{[0]}=6$\n",
    "* <font color=\"mediumslateblue\"><b>hidden</b></font> layers: layers $1,2,3$\n",
    "  + layer $1$: has $4$ nodes ($n^{[1]}=4$)\n",
    "  + layer $2$: has $3$ nodes ($n^{[2]}=3$)\n",
    "  + layer $3$: has $6$ nodes ($n^{[3]}=6$)\n",
    "* <font color=\"red\"><b>output</b></font> layer: layer $4$\n",
    "  + layer $4$: has $4$ nodes ($n^{[4]}=4$)\n",
    "\n",
    "The output layer is followed by a <font color=\"green\"><b>loss function</b></font> which is \n",
    "the <font color=\"green\"><b>objective function</b></font> to be minimized during the <font color=\"green\"><b>training</b></font> phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c56da9a-61a8-4212-bda7-2c08cfd7152e",
   "metadata": {},
   "source": [
    "<font color=\"teal\"><b>Note (for sake of completeness)</b></font><br>\n",
    "<details>\n",
    "  <summary>Click here to expand!</summary>\n",
    "    \n",
    "There are exceptions to the aforementioned definition, e.g. to name a few:\n",
    "+ <a href=\"https://en.wikipedia.org/wiki/Residual_neural_network\"><b>Residual Neural Networks</b></a> (ResNets)\n",
    "+ <a href=\"https://en.wikipedia.org/wiki/Bidirectional_recurrent_neural_networks\"><b>Bidirectional Recurrent Neural Networks</b></a> (BRNNs)\n",
    "+ $\\ldots$\n",
    " \n",
    "This type of networks will **NOT** be covered in this lecture but may the subject of future talks.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3981fc77-41dc-49cf-8b64-0e95c6575cac",
   "metadata": {},
   "source": [
    "# 2.Activation functions\n",
    "Previously, we discussed the `sigmoid` activation function:<br>\n",
    "  $\\begin{eqnarray}\n",
    "     \\sigma(z) & = & \\frac{1}{1\\,+\\,\\exp(-z)}\\,, z \\,\\in \\mathbb{R}\n",
    "  \\end{eqnarray}$\n",
    "\n",
    "The activation functions for the output layer and the hidden layers\n",
    "are in general <font color=\"green\"><b>different</b></font>.\n",
    "\n",
    "- <font color=\"red\"><b>output</b></font> layer:<br>the activation function is <font color=\"green\"><b>determined</b></font> by the subsequent loss function.\n",
    "  * `binary` classification: <br>\n",
    "    + `sigmoid` $\\sigma(z)$\n",
    "    + $\\sigma(z) \\in [0,1]$\n",
    "  * `multiclass` classification: \n",
    "    + $\\begin{eqnarray}\n",
    "       \\texttt{softmax}(z_{ij}) & := &  \\frac{\\exp(z_{ij})}{ \\displaystyle\\sum_{k=1}^C \\exp(z_{ik})}\n",
    "        \\end{eqnarray}$\n",
    "    + $C$ stands for the number of classes \n",
    "    + $\\texttt{softmax}(z_{ij}) \\in [0,1]$  \n",
    "  * `linear regression`:<br>\n",
    "    + `1` (no action) \n",
    "\n",
    "- <font color=\"mediumslateblue\"><b>hidden</b></font> layers:<br>\n",
    "  There are several activation functions that are commonly used:\n",
    "  * $\\texttt{relu}(x)$: (**Re**ctified **L**inear **U**nit) - the most commonly used<br>\n",
    "    + $\\texttt{relu}(x) := \\max(0,x)$\n",
    "    + $\\texttt{relu}(x) \\geq 0 $\n",
    "  * $\\texttt{lrelu}(x;\\alpha)$: (**L**eaky **Re**ctified **L**inear **U**nit)<br>\n",
    "    + $\\texttt{lrelu}(x;\\alpha) := \\max(\\alpha x, x) \\,,\\,0 \\leq \\alpha < 1$\n",
    "  * $\\texttt{gelu}(x)$: (**G**aussian-**e**rror **Linear** **U**nit)<br>\n",
    "    + $\\begin{eqnarray} \\texttt{gelu}(x) &:= & x\\,\\Phi(x) \\end{eqnarray}$ <br>\n",
    "      where $\\Phi(x)$ stands for the <a href=\"https://en.wikipedia.org/wiki/Cumulative_distribution_function\"><b>CDF</b></a> of the standard normal distribution.\n",
    "    + $\\begin{eqnarray} \\texttt{gelu}(x)              & = &x \\displaystyle\\int_{-\\infty}^x \\frac{1}{\\sqrt{2 \\pi}}{ \\exp\\bigg(-\\frac{t^2}{2}\\bigg)} dt\n",
    "       \\end{eqnarray}$\n",
    "  * $\\texttt{swish}(x;\\beta) := x \\, \\sigma(\\beta x)$    \n",
    "  * $\\texttt{tanh}(x)$: (Hyperbolic Tangent)<br>\n",
    "    + $\\texttt{tanh}(x) := \\displaystyle \\frac{\\exp(x) - \\exp(-x)}{\\exp(x) + \\exp(-x)}$\n",
    "    + $\\texttt{tanh}(x) \\in [-1,1]$\n",
    "  * $\\ldots$  <br>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1d364f-43a4-408a-8e7b-f5f57048badd",
   "metadata": {},
   "source": [
    "## Plot of some of the activation functions\n",
    "Below you will find some of the aforementioned activation functions displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1441802d-00c7-4793-a910-7c2194791a39",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "START= -2.5\n",
    "END= 2.5\n",
    "DENS_PER_UNIT=20\n",
    "ALPHA=0.05\n",
    "x = np.linspace(start=START,stop=END, num=int((END-START))*DENS_PER_UNIT+1)\n",
    "y_relu = np.maximum(0,x)\n",
    "y_lrelu = np.maximum(ALPHA*x,x)\n",
    "y_tanh = np.tanh(x)\n",
    "y_sigmoid = 1.0/(1.0+np.exp(-x))\n",
    "y_swish = x*y_sigmoid\n",
    "y_gelu = x* ss.norm.cdf(x)\n",
    "plt.title(r\"Plot of the most common activation functions\")\n",
    "plt.plot(x,y_lrelu, label=r\"$\\mathrm{lrelu(x;\\alpha=0.05)}$\", color=\"blue\")\n",
    "plt.plot(x,y_relu, label=r\"$\\mathrm{relu(x)}$\", color=\"red\")\n",
    "plt.plot(x,y_gelu, label=r\"$\\mathrm{gelu(x)}$\")\n",
    "plt.plot(x,y_tanh, label=r\"$\\mathrm{tanh(x)}$\", color=\"green\")\n",
    "plt.plot(x,y_sigmoid, label=r\"$\\mathrm{\\sigma(x)}$\", color=\"orange\")\n",
    "plt.plot(x,y_swish, label=r\"$\\mathrm{swish(x;\\beta=1.0)}$\", color=\"brown\")\n",
    "plt.xticks(np.arange(START,END+0.5, step=0.5))\n",
    "plt.xlabel(r\"$x$\")\n",
    "plt.ylabel(r\"$y$\",rotation=0)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458081c5-5c69-4cf5-8863-4ec6d22cf31a",
   "metadata": {},
   "source": [
    "# 3.Cost/Loss functions\n",
    "\n",
    "* The <font color=\"green\"><b>cost</b></font> function ($\\mathcal{C}$) is defined as follows:<br>\n",
    "$\\begin{eqnarray}\n",
    "    \\mathcal{C} & := & \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}^{(i)}\n",
    " \\end{eqnarray}$<br>\n",
    " where $\\mathcal{L}^{(i)}$ stands for the <font color=\"green\"><b>loss</b></font> function for example $i$ and $m$ stands for the batch\n",
    " of examples used.\n",
    "\n",
    "* Depending on the task we want to accomplish, we need to use different loss functions.<br>\n",
    "  The most <font color=\"blue\"><b>common</b></font> loss functions are:<br>\n",
    "  * `binary classification`<br>\n",
    "    $\\begin{eqnarray}\n",
    "        \\mathcal{L}^{(i)} &:= & -y_i\\,\\log(a^{[L]}_i)\n",
    "      \\end{eqnarray}$<br>\n",
    "    where:\n",
    "    - $y_i$ is either $0$ or (<font color=\"red\"><b>exclusive</b></font>) $1$ and represents the label of example $i$.\n",
    "  * `multiclass classification` (exclusionary)<br>\n",
    "    $\\begin{eqnarray}\n",
    "        \\mathcal{L}^{(i)} &:= & \\sum_{j=1}^C -y_{ij}\\,\\log(a^{[L]}_{ij})\n",
    "      \\end{eqnarray}$<br>\n",
    "    where:<br>\n",
    "    + $y_i$ is a <a href=\"https://en.wikipedia.org/wiki/One-hot\"><b>one-hot</b></a> encoding vector representing the <font color=\"green\"><b>label</b></font> of example $i$.\n",
    "    + $C$ stands for the <font color=\"green\"><b>number of classes</b></font>.\n",
    "  * `linear regression`<br>\n",
    "    $\\begin{eqnarray}\n",
    "        \\mathcal{L}^{(i)} &:= \\frac{1}{2} (y_i - a^{[L]}_i)^2\n",
    "      \\end{eqnarray}$<br>\n",
    "    where:<br>\n",
    "    + $y_i$ is the value associated with example $i$.\n",
    "* The cost function can also contain extra terms (i.e. <font color=\"green\"><b>regularization</b></font>) to constrain certain parameters/prevent <font color=\"green\"><b>overfitting</b></font>.<br>\n",
    "  We will elaborate on this later on in this Lecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a48fcbb-c173-4df3-ad17-206ab872b2c5",
   "metadata": {},
   "source": [
    "<font color=\"teal\"><b>Note (for sake of completeness)</b></font><br>\n",
    "<details>\n",
    "  <summary>Click here to expand!</summary>\n",
    "\n",
    "- A lot of loss functions can be easily derived using:\n",
    "  + the <a href=\"https://en.wikipedia.org/wiki/Maximum_likelihood_estimation\"><b>Maximum Likelihood Estimation</b></a> (MLE) method or using the <a href=\"https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\"><b>Kullback-Leibler divergence</b></a>\n",
    "  + combined with the theory of <a href=\"https://en.wikipedia.org/wiki/Generalized_linear_model\"><b>Generalized Linear Models</b></a> (GLM).<br>\n",
    "  e.g. `Poisson loss function` (to predict discrete counts),...\n",
    "- To improve <font color=\"green\"><b>numerical stabilty</b></font>, the activation function for the\n",
    "    last layer ($L$) and the loss function are:\n",
    "  + often <font color=\"green\"><b>calculated together/at once</b></font>\n",
    "  + and then bear a separate (information-theoretic) name.\n",
    "\n",
    "- In the output layer there can be several types of activation functions at the same time,<br>\n",
    "  e.g. a `linear regression` problem where we are interested at the same time:\n",
    "  + in a predicted value of $a^{[L]}_{i1} \\in \\mathbb{R}$\n",
    "  + the prediction of the variance ($ a^{[L]}_{i2}\\in \\mathbb{R}^+$)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6c434a-28c2-498d-a978-1d7041f135e4",
   "metadata": {},
   "source": [
    "# 4.Structure of the DNN: summary\n",
    "- We just described the <font color=\"green\"><b>basic structure</b></font> of a deep neural network, i.e.:\n",
    "  + the layers $l \\in \\{0,1,\\ldots, L\\}$.\n",
    "  + the number of nodes per layer i.e. $n^{[l]}$ for $l \\in \\{0,1,\\ldots, L\\}$.\n",
    "- We also introduced the two other structural components:\n",
    "  + activation functions.<br>\n",
    "    <font color=\"green\"><b>ONE</B></font> activation function ($\\widehat{h}^{[l]}$) is <font color=\"green\"><b>REQUIRED</b></font> per layer $l \\in \\{1,\\ldots, L\\}$.\n",
    "  + the loss/cost function $\\mathcal{L}^{(i)}/\\mathcal{C}$.<br>\n",
    "    The loss/cost function is only <font color=\"green\"><b>MANDATORY</b></font> during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187c9f61-970b-4188-9602-0d037f4e4fee",
   "metadata": {},
   "source": [
    "## Implementation of the previous DNN example in Keras/PyTorch\n",
    "\n",
    "We have a deep neural with $L=4$ layers (neural net of depth 4).\n",
    "* <font color=\"green\"><b>input</b></font> layer: layer $0$\n",
    "  + the input layer requires input vectors/examples each of length $n^{[0]}=6$\n",
    "* <font color=\"mediumslateblue\"><b>hidden</b></font> layers: layers $1,2,3$\n",
    "  + layer $1$: has $4$ nodes ($n^{[1]}=4$) - activation $h^{[1]}$: `ReLu` \n",
    "  + layer $2$: has $3$ nodes ($n^{[2]}=3$) - activation $h^{[2]}$: `ReLu`\n",
    "  + layer $3$: has $6$ nodes ($n^{[3]}=6$) - activation $h^{[3]}$: `ReLu` \n",
    "* <font color=\"red\"><b>output</b></font> layer: layer $4$\n",
    "  + layer $4$: has $4$ nodes ($n^{[4]}=4$) - activation $h^{[4]}$: `softmax`\n",
    "    (We assume that the model will train to classify $4$ classes). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3932136-b245-4833-acbc-b2219e25904d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Keras/PyTorch code\n",
    "model = keras.Sequential([\n",
    "          keras.layers.Input(shape=(6,)),                      # Input: 6-dimensional input vector\n",
    "          keras.layers.Dense(units=4, activation='relu'),      # Layer 1\n",
    "          keras.layers.Dense(units=3, activation='relu'),      # Layer 2\n",
    "          keras.layers.Dense(units=6, activation='relu'),      # Layer 3\n",
    "          keras.layers.Dense(units=4, activation='softmax')])  # Output layer: 4 classes\n",
    "print(model.summary())  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c24887-f06c-4021-9a55-bbf54938cebc",
   "metadata": {},
   "source": [
    "### <font color=\"blue\"><b>Exercise 1:</b></font>\n",
    "- Load the fashion_mnist data set using keras\n",
    "- Split the data set into three subsets:\n",
    "  + training set:<br>\n",
    "    - training proper: $(50000, 28, 28)$ \\& $(50000,)$\n",
    "    - cross-validation set: $(10000, 28, 28)$ \\& $(10000,)$\n",
    "  + set set:\n",
    "    - test set: $(10000, 28, 28)$ \\& $(10000,)$      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5da22a3-9bb9-4f86-81be-0a3e3a6853fb",
   "metadata": {},
   "source": [
    "# 5. Training of a Deep Neural Network\n",
    "\n",
    "During <font color=\"green\"><b>TRAINING</b></font>, a `function` needs to be found<br>\n",
    "which is able to <font color=\"green\"><b>GENERALIZE</b></font> well (i.e. to deal sufficiently well with unseen examples).\n",
    "\n",
    "Therefore, two questions arise:\n",
    "1. does such a `function` even exist?\n",
    "2. and if so, how to find such a `function`/how to train a network.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd23507-fe4c-4dec-885c-c97ed4be8d41",
   "metadata": {},
   "source": [
    "## 1. The universal approximation theorems (UATs)\n",
    "<font color=\"green\"><b>Theorem</b></font>:<br>\n",
    "A feedforward neural network with:\n",
    "1. at least one hidden layer\n",
    "2. a finite number of neurons\n",
    "3. a nonlinear activation function\n",
    "\n",
    "can approximate to any accuracy<br>\n",
    "<font color=\"green\"><b>any continuous function</b></font> on a <font color=\"green\"><b>compact subset</b></font> of $\\mathbb{R}^n$.\n",
    "\n",
    "This theorem is related to the <font color=\"green\"><b>Stone-Weierstrass theorem</b></font> from analysis<br> (see e.g. <a href=\"https://david92jackson.neocities.org/images/Principles_of_Mathematical_Analysis-Rudin.pdf\"><b>Principles of Mathematical Analysis by Walter Rudin</b></a>)\n",
    "\n",
    "Sources:\n",
    "* <a href=\"https://web.njit.edu/~usman/courses/cs675_fall18/10.1.1.441.7873.pdf\">Cybenko, G. (1989). Approximation by superposition of sigmodial functions. Mathematics of Control, Signals and Systems, 2(4), p. 303-314.</a>\n",
    "* <a href=\"https://web.njit.edu/~usman/courses/cs677/hornik-nn-1991.pdf\"> Hornik, K. (1991). Approximation Capabilities of Multilayer Feedforward Networks. Neural Networks, 4, p. 251-257.</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b714b99-16cb-4c8e-9e7f-221334d8bd61",
   "metadata": {},
   "source": [
    "## 2.Training a deep neural network.\n",
    "\n",
    "- In <a href=\"./lecture1.ipynb\"><b>Lecture 1</b></a>, we trained a shallow neural network ($1$ node in $1$ layer).<br>\n",
    "  To do so, we had to <font color=\"green\"><b>iterate</b></font> repeatedly over the <font color=\"blue\"><b>following loop</b></font>:\n",
    "  + Perform the <font color=\"green\"><b>forward propagation</b></font></color>.<br>Start at the input layer and proceed forward to the output layer and the loss function.\n",
    "  + Perform the <font color=\"green\"><b>backward propagation</b></font></color>.<br>Start at the loss function and recede all the way to the first layer\n",
    "    in order<br> to obtain the <font color=\"green\"><b>gradients of the parameters</b></font>.\n",
    "  + <font color=\"green\"><b>Update the parameters</b></font> (weights and bias) using the <font color=\"green\"><b>gradient descent</b></font> method.<br>  \n",
    "\n",
    "\n",
    "- In the following sections, we will elaborate on the same components for a deep neural net."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e82191-8afc-4dc3-af62-21b5c5065962",
   "metadata": {},
   "source": [
    "# 6.Forward propagation (dense deep neural net)\n",
    "\n",
    "We extracted a slice from the previous example to illuminate the principle of forward propagation .<br><br>\n",
    "<img src=\"forward1.jpeg\" alt=\"Connections between layers 1 and 2\" style=\"width:65%; height:auto\"><br>\n",
    "\n",
    "**Setup/Context**\n",
    "- We have a dense deep neural net with **$L$** layers. Each layer $l$ has **$n^{[l]}$** nodes.\n",
    "- The input layer ($l=0$) has $n^{[0]}$ nodes.<br>\n",
    "- The number of nodes ($n^{[0]}$) is **equal** to the dimensionality of each input vector $x_i$, where:<br>\n",
    "  $\\begin{eqnarray}\n",
    "    x_i := \\begin{pmatrix}\n",
    "            x_{i,1} & x_{i,2} & \\ldots & x_{i,n^{[0]}}\n",
    "           \\end{pmatrix}\n",
    "  \\end{eqnarray}$<br>\n",
    "- $ a^{[0]}_{i} := x_i $   \n",
    "\n",
    "**Transformation**\n",
    "- At <font color=\"green\"><b>layer</b></font> $l \\in \\{1,2,\\ldots,L\\}$ the vector associated with example $i$ is transformed as follows:<br>\n",
    "    $\\begin{eqnarray}\n",
    "      z^{[l]}_{i,k} &= & \\sum_{j=1}^{n^{[l-1]}} a^{[l-1]}_{i,j} w^{[l]}_{j,k} + b^{[l]}_k \\; \\;,\\; k \\in \\{1,2, \\dots,n^{[l]}\\} \\\\\n",
    "      a^{[l]}_{i,k} &= & \\widehat{h}^{[l]}(z^{[l]}_{i,k}) \\; \\;,\\; k \\in \\{1,2, \\dots,n^{[l]}\\}\n",
    "    \\end{eqnarray}$<br>\n",
    "  i.e.:\n",
    "    + at **each** node $k$ of layer $l$ the input vector $a^{[l-1]}_i$ is <font color=\"green\"><b>weighted</b></font> and a <font color=\"green\"><b>bias</b></font> is added ($ \\Rightarrow z^{[l]}_{i,k}$)\n",
    "    + subsequently, at **each** node $k$ of layer $l$ the corresponding value $z^{[l]}_{i,k}$ is <font color=\"green\"><b>transformed</b></font> using a **non-linear** activation function\n",
    "      $\\widehat{h}^{[l]}$.<br> The output of the activation function for example $i$ at node $k$ of layer $l$ is labeled $a^{[l]}_{i,k}$.<br>\n",
    "    + given that the $l$-th layer contains $n^{[l]}$\n",
    "      nodes, the transformation of example $i$ at layer $l$ results into a vector of dimension $n^{[l]}$.\n",
    "\n",
    "**Vectorized Form**   \n",
    "- This procedure can easily be <font color=\"green\"><b>vectorized</b></font>:<br> $m$ example vectors can be transformed simultaneously/in batch at the same layer .\n",
    "\n",
    "- <font color=\"green\"><b>Algorithm</b></font>:\n",
    "  + Let $X$ be an input matrix of $m$ examples (one example per row) and $n^{[0]}$ columns.\n",
    "  + $A^{[0]}:=X$ $,\\hspace{0.5in}A{[0]} \\in \\mathbb{R}^{m\\times n^{[0]}}$<br>\n",
    "    $\\texttt{for(l=1,..,L)}$<br>\n",
    "    $\\texttt{do}$<br>\n",
    "    $\\begin{eqnarray}\n",
    "      \\hspace{0.35in}Z^{[l]} &= & A^{[l-1]}W^{[l]} + b^{[l]} \\\\\n",
    "      A^{[l]} &= &\\widehat{h}^{[l]}(Z^{[l]})\n",
    "     \\end{eqnarray}$<br>\n",
    "    $\\texttt{done}$\n",
    "  + Calculate $\\mathcal{C}$ using $A^{[L]}$ and $Y$.\n",
    "\n",
    "**Summary**   \n",
    "- In order to learn, the network requires weight, biases matrices and activation functions for each layer $l$:  \n",
    "  * <font color=\"green\"><b>Weight matrices</b></font>: $W^{[l]} \\in \\mathbb{R}^{n^{[l-1]}\\times n^{[l]}}$\n",
    "  * <font color=\"green\"><b>Bias vectors</b></font>: $b^{[l]} \\in \\mathbb{R}^{ 1 \\times n^{[l]}}$\n",
    "  * <font color=\"green\"><b>Non-linear activation functions</b></font>: $\\widehat{h}^{[l]}$\n",
    "- The weight matrices and the bias vectors are known as the <font color=\"green\"><b>PARAMETERS</b></font> of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cc295e-a36c-4274-a8f7-48e1caf0739a",
   "metadata": {},
   "source": [
    "# 7.Back propagation\n",
    "\n",
    "The <font color=\"green\"><b>back propagation</b></font> operates in the <font color=\"green\"><b> opposite</b></font> direction of the forward propagation:\n",
    "  - it starts with the cost function $\\mathcal{C}$ and recedes all the way back to $l=1$.\n",
    "  - during the back propagation, the <font color=\"green\"><b>gradients</b></font> of the parameters $W^{[l]}, b^{[l]}$ are calculated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e599aaa-f7d5-4b0f-8fbf-b09f54b59843",
   "metadata": {},
   "source": [
    "<font color=\"teal\"><b>Note (for sake of completeness)</b></font><br>\n",
    "<details>\n",
    "  <summary>Click here to expand!</summary>\n",
    "\n",
    "In these section we will describe the backpropagation in more detail.<br>The expressions below can easily\n",
    "be derived by the application of the chain rule.\n",
    "\n",
    "- We start at the <font color=\"green\"><b>output</b></font> layer ($l=L$):\n",
    "  * $\\begin{eqnarray}\n",
    "     \\mathcal{C} & := & \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}^{(i)}\n",
    "     \\end{eqnarray}$\n",
    "  * Let:<br>\n",
    "    $\\begin{eqnarray}\n",
    "     (dA^{[L]})_{ij} &:=& \\frac{\\partial \\mathcal{L}^{(i)}} {\\partial a^{[L]}_{i,j}} \\\\\n",
    "     (dZ^{[L]})_{ij} &:=& \\frac{\\partial \\mathcal{L}^{(i)}} {\\partial z^{[L]}_{i,j}} &= &\\sum_{k=1}^{n^{[L]}}\\frac{\\partial \\mathcal{L}^{(i)}} {\\partial a^{[L]}_{i,k}} \\frac{\\partial a^{[L]}_{i,k}}\n",
    "     {\\partial z^{[L]}_{i,j}}\\\\\n",
    "     (dW^{[L]})_{i,j} & := &  \\frac{\\partial \\mathcal{C}}{\\partial w^{[L]}_{i, j}} \\\\\n",
    "     (db^{[L]})_{j} & := &  \\frac{\\partial \\mathcal{C}}{\\partial b^{[L]}_{j}} \n",
    "    \\end{eqnarray}$\n",
    "  * The application of the <font color=\"green\"><b>chain rule</b></font> results into:<br>\n",
    "    $\\begin{eqnarray}\n",
    "     dW^{[L]} & = & \\frac{1}{m}(A^{[L-1]})^T.dZ^{[L]} \\\\\n",
    "     db^{[L]} & = & \\frac{1}{m}\\sum_{i=1}^m(dZ^{[L]})_i \n",
    "    \\end{eqnarray}$\n",
    "  \n",
    "- Using the relation $Z^{[l]} = A^{[l-1]}.W^{[l]} + b^{[l]}$<br>\n",
    "  we can determine $dW^{[l]}, db^{[l]}$ for the <font color=\"green\"><b>other layers</b></font> $ l \\in \\{L-1,\\ldots,2,1\\}$.\n",
    "\n",
    "  Therefore,<br>\n",
    "  $\\texttt{for(l=L,...,2)}$<br>\n",
    "  $\\texttt{do}$<br>\n",
    "  $\\begin{eqnarray}\n",
    "      \\hspace{0.35in}dA^{[l-1]} &= & dZ^{[l]}.(W^{[l]})^T \\\\\n",
    "                     dZ^{[l-1]} &= & dA^{[l-1]} \\odot d\\widehat{H}^{[l-1]} \\\\\n",
    "                     dW^{[l-1]} & =& \\frac{1}{m}(A^{[l-2]})^T.dZ^{[l-1]} \\\\\n",
    "                     db^{[l-1]} & =& \\frac{1}{m}\\sum_{i=1}^m(dZ^{[l-1]})_i\n",
    "  \\end{eqnarray}$<br>\n",
    "  $\\texttt{done}$\n",
    "\n",
    "  where:\n",
    "  + $\\odot$ stands for the <a href=\"https://en.wikipedia.org/wiki/Hadamard_product_(matrices)\"><b>Hadamard product</b></a> (element-wise matrix multiplication)\n",
    "  + $(d\\widehat{H}^{[l-1]})_{i,j} := (\\widehat{h}^{\\prime[l-1]})_{i,j} $<br>\n",
    "  \n",
    "  <font color=\"brown\"><b>Remark:</b></font>\n",
    "    \n",
    "  + $dZ^{[l-1]} =  dA^{[l-1]} \\odot d\\widehat{H}^{[l-1]}$ <br>$\\texttt{iff}$<br>\n",
    "    $\\begin{eqnarray}\n",
    "      \\frac{\\partial a^{[l]}_{ik}}{\\partial z^{[l]}_{ij}} &= &\\widehat{h}^{[l]}(z^{[l]}_{ij})\n",
    "      \\end{eqnarray}$ which is indeed the case for `ReLU`\n",
    "\n",
    "</details>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c331abce-373a-413a-bcd5-c5b25ade6fcd",
   "metadata": {},
   "source": [
    "# 8.Updating the parameters\n",
    "\n",
    "In order to update the parameters, we used in <a href=\"lecture1.ipynb\"><b>Lecture 1</b></a>:\n",
    "+ <font color=\"green\"><b>simple gradient descent</b></font>\n",
    "+ the complete <font color=\"green\"><b>training data at once</b></font><br>\n",
    "\n",
    "For the general case, we need to add a few important <font color=\"red\"><b>additions/modifications</b></font>:<br>\n",
    "\n",
    "- **Epochs & the batch size $m$**\n",
    "  * In <font color=\"green\"><b>regular/batch gradient descent</b></font> we consider **all examples at once**:\n",
    "    + to calculate the gradients\n",
    "    + and update the parameters.<br>\n",
    "    In the case of DL, this approach in <font color=\"red\"><b>generally impossile</b></font> to\n",
    "    the sheer size of the data.\n",
    "\n",
    "  * <font color=\"green\"><b>In praxi</b></font>, the following approach is followed in DL:\n",
    "    + the complete dataset ($N$ examples) is split up into `mini-batches` of size $m$. \n",
    "    + it takes $\\kappa:= \\frac{N}{m}$ `mini-batches` to proceed through the dataset (assuming $m | N$).\n",
    "    + in each `mini-batch`, the gradients will be calculated and the parameters updated.\n",
    "    + A <font color=\"green\"><b>single pass</b></font> through the <font color=\"green\"><b>ENTIRE data set</b></font> is called an <font color=\"green\"><b>epoch</b></font>.\n",
    "    + When $m=1$, the gradient procedure is known as <font color=\"green\"><b>stochastic gradient descent</b></font>.<br>\n",
    "      (This is <font color=\"red\"><b>NOT a great approach</b></font> - <font color=\"red\"><b>volatility</b></font> of the loss)\n",
    "    + $m$ is a <font color=\"orangered\"><b>hyper parameter</b></font>.<br>\n",
    "      It is often set to be a power of $2$ (data alignment with the memory architecture) e.g. $64,128,256,512,1024$.\n",
    "\n",
    "  * So, we end up with the following <font color=\"green\"><b>training algorithm</b></font>:\n",
    "\n",
    "    $\\texttt{for(iepoch=1,...,num\\_epochs)}$<br>\n",
    "    $\\texttt{do}$<br>\n",
    "    $\\hspace{0.35in}\\texttt{for(ibatch=1,...,num\\_batches)}$<br>\n",
    "    $\\hspace{0.35in}\\texttt{do}$<br>\n",
    "    $\\hspace{0.7in}\\texttt{Calculate Cost (forward)}$<br>\n",
    "    $\\hspace{0.7in}\\texttt{Calculate gradients (backward)}$<br>\n",
    "    $\\hspace{0.7in}\\texttt{Update parameters}$<br>\n",
    "    $\\hspace{0.35in}\\texttt{done}$<br>\n",
    "    $\\texttt{done}$<br>\n",
    "\n",
    "- **Normalize the input**\n",
    "  * In order to speed up the optimization it is recommended to normalize the inputs.<br>\n",
    "    There are different approaches (depending on the problem) to do so, e.g.:\n",
    "    - Standardization: calculate the <font color=\"green\"><b>mean</b></font> and the <font color=\"green\"><b>variance</b></font> of the (mini-)batch for each \"feature\" ($j$):\n",
    "      \n",
    "      $\\begin{eqnarray}\n",
    "        \\overline{x_j}     & := &\\frac{1}{m} \\sum_{i=1}^m x_{i,j}  \\\\\n",
    "        s^2_j              & := & \\frac{1}{m} \\sum_{i=1}^m \\Big( x_{i,j} - \\overline{x_j} \\Big)^2 \\\\\n",
    "        \\widetilde{x_{i,j}} & := & \\frac{ x_{i,j} - \\overline{x_j}}{ \\sqrt{s^2_j+\\epsilon}}\n",
    "       \\end{eqnarray}$\n",
    "\n",
    "      1. After $1$ epoch: the mean and variance of the <b>complete</b> training set are known.\n",
    "      2. The mean and variance of the global training set should be used:\n",
    "         - for the cross-validation and testing set\n",
    "         - during inference\n",
    "         \n",
    "    - For images, the following approach is often used:<br>\n",
    "      $\\begin{eqnarray}\n",
    "         \\widetilde{x_{i,j}} & = & \\displaystyle \\frac{ x_{i,j} - \\min_{i}(x_{i,j})}{\\max_{i} (x_{i,j})- \\min_i(x_{i,j})}\n",
    "      \\end{eqnarray}$\n",
    "\n",
    "- **Non-convexity of the objective function**<br>\n",
    "  * In <a href=\"./lecture1.ipynb\"><b>Lecture 1</b></a>, our objective/loss function was <font color=\"green\"><b>convex</b></font>:\n",
    "    when we reached the <br>\n",
    "    <font color=\"green\"><b>local minimum</b></font> we were assured to obtain the <font color=\"green\"><b>global minimum</b></font> of the objective/loss function.<br>\n",
    "  \n",
    "  * In general, the cost function for deep neural nets is <font color=\"red\"><b>NOT convex</b></font>:<br>we can never be certain          that we have reached the global minimum of this highly dimensional surface.\n",
    "\n",
    "\n",
    "- **Use a more advanced version of gradient descent**<br>\n",
    "  * The <font color=\"green\"><b>rate of convergence</b></font> for simple gradient descent (<font color=\"teal\"><b>first-order method</b></font>) is very slow.\n",
    "  * There are <font color=\"teal\"><b>second-order</b></font> optimization methods:\n",
    "    + <font color=\"green\"><b>pro</b></font>: <font color=\"green\"><b>speed up</b></font> the optimization process.\n",
    "    + <font color=\"red\"><b>con</b></font>: very <font color=\"red\"><b>expensive</b></font> (require the\n",
    "      calculation of the Hessian for the parameters).\n",
    "  * Use a <font color=\"green\"><b>more performant version</b></font> of gradient descent to <font color=\"green\"><b>enhance   the rate of convergence</b></font>.<br>\n",
    "    The most commonly used version is called <a href=\"https://arxiv.org/pdf/1412.6980\"><b>Adam</b></a>. (See <a href=\"./ewma.ipynb\"><b>ewma.ipynb</b></a> for more details)\n",
    "\n",
    "- **Use a non-constant learning rate**<br>\n",
    "  Its rationale:<br>\n",
    "  * Far from minimum, take <font color=\"green\"><b>larger</b></font> steps\n",
    "  * Close to minimum, take <font color=\"green\"><b>smaller</b></font> steps (avoid oscillations around the minimum)\n",
    "  * Therefore, schedule the learning rate ($\\alpha$) e.g.<br>\n",
    "      $\\begin{eqnarray}\n",
    "        \\alpha & = & \\displaystyle \\alpha_s \\kappa^{\\texttt{iepoch}}\n",
    "       \\end{eqnarray}$<br>\n",
    "       where $\\alpha_s:= \\texttt{const}$, $0 < \\kappa < 1$.\n",
    "          \n",
    "- **Initialization of the weights**<br>\n",
    "  * + $W^{[l]}$ should be initialized (randomly) and <font color=\"red\"><b>NOT</b></font> be set to $0.0$ (<font color=\"green\"><b>get rid of symmetry</b></font>)<br>\n",
    "       $b^{[l]}$ can be set to $0.0$.\n",
    "    + However, random initialization of $W^{[l]}$ can lead to:<br>\n",
    "      1. <font color=\"red\"><b>slow convergence/different results</b></font> (due to the non-convexity of the loss function).\n",
    "      2. for large networks: <font color=\"red\"><b>vanishing/exploding</b></font> gradients.\n",
    "       \n",
    "  * A note on <font color=\"green\"><b>vanishing/exploding</b></font> gradients<br>\n",
    "    Let's consider a simple $\\texttt{Gedankenexperiment}$:\n",
    "    + Let $b^{[l]}:= 0$, for $l \\in \\{1,2, \\ldots, L\\}$\n",
    "    + Let $h^{[l]}:= 1$, for $l \\in \\{1,2, \\ldots, L\\}$ <br>\n",
    "      Then we get:<br>\n",
    "      $\\begin{eqnarray}\n",
    "        A^{[L]} = X^{[0]}W^{[1]} W^{[2]} \\ldots W^{[L]}\n",
    "      \\end{eqnarray}$\n",
    "    + If $W^{[l]}$ is $\\texttt{diagonal}$, and<br>\n",
    "      $W^{[l]}=\\varrho \\widehat{\\mathbb{1}}$, for $l \\in \\{1,2,\\ldots,L\\}$,<br>\n",
    "      then:<br>\n",
    "      $\\begin{eqnarray}\n",
    "          \\lim_{L \\to \\infty} A^{[L]}_{ii} &=& 0 , \\;\\;\\textrm{if}\\;\\; 0<=\\varrho < 1 \\\\\n",
    "          \\lim_{L \\to \\infty} A^{[L]}_{ii} &=& \\infty ,   \\;\\;\\textrm{if}\\;\\; \\varrho > 1\n",
    "       \\end{eqnarray}$<br>\n",
    "      the former case will lead to <font color=\"red\"><b>vanishing gradients</b></font>.<br>\n",
    "      the latter case will lead to <font color=\"red\"><b>exploding gradients</b></font>. \n",
    "  * In order to circumvent these issues, the following (heuristic) <font color=\"green\"><b>weight initializations</b></font> were introduced:<br>    \n",
    "    + <a href=\"https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\"><b>Glorot/Xavier</b></a>:\n",
    "      - $b^{[l]} = 0.0$\n",
    "      - $W^{[l]} = \\texttt{Unif}\\Big ( \\displaystyle - \\sqrt{\\frac{6}{n^{[l-1]} +n^{[l]}}} , \\sqrt{\\frac{6}{n^{[l-1]} +n^{[l]}}} \\Big )$\n",
    "    * <a href=\"https://arxiv.org/pdf/1502.01852\"><b>He</b></a>:\n",
    "      - $b^{[l]} = 0.0$\n",
    "      - $W^{[l]} = \\texttt{Norm}\\Big ( \\mu =0 , \\sigma^2 =  \\displaystyle \\frac{2}{n^{[l-1]}}\\Big )$ for $a^{[l]} \\in \\texttt{ReLu}$ family.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000fdf94-d373-4c45-b176-d34596f83130",
   "metadata": {},
   "source": [
    "### <font color=\"blue\"><b>Exercise 2:</b></font>\n",
    "- Flatten the training, cross-validation sets & test sets into:\n",
    "  + training: $(50000, 784)$\n",
    "  + cross-validation: $(10000, 784)$\n",
    "  + test: $(10000, 784)$\n",
    "- Normalize each of the input sets by mapping them to [0,1]\n",
    "- Create a $\\texttt{keras}$ model with the following structure:<br>\n",
    "  + layer 0 : input\n",
    "  + layer 1 : dense with 128 neurons; $\\texttt{ReLU}$\n",
    "  + layer 2 : dense with 10 neurons; $\\texttt{softmax}$ -> why $\\texttt{softmax}$?\n",
    "- How many trainable parameters do you expect? Does it match the output of the `summary` function?\n",
    "- What loss function should one consider?\n",
    "- `compile` the model using `adam`, `sparse_categorical_crossentropy` and `accuracy`.\n",
    "- Train the model using the `fit` function.\n",
    "- `evaluate` the model.\n",
    "- What is the default batch size?\n",
    "- What is the default weight initialization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6ec27b-8433-4a79-83ea-356cbd808900",
   "metadata": {},
   "source": [
    "# 9.Practical considerations\n",
    "\n",
    "## 9.1.Training/Cross-Validation/Testing\n",
    "* The <font color=\"green\"><b>data</b></font> are split up into three <font color=\"green\"><b>partitions</b></font>:\n",
    "  - <font color=\"green\"><b>training</b></font> set: data to train a model\n",
    "  - <font color=\"green\"><b>cross-validation/dev</b></font> set: data to cross-validate the different models<br>\n",
    "                           (select a model based on the hyperparameters)\n",
    "  - <font color=\"green\"><b>testing</b></font>: data to test the final model.<br>\n",
    "    <font color=\"red\"><b>NEVER</b></font> to be used for either training or cross-validation purposes.\n",
    "    \n",
    "  <font color=\"orangered\"><b>Fundamental assumption</b></font>:<br>\n",
    "  The data for each of these partitions <font color=\"green\"><b>must</b></font> belong to the same (statistical) distribution.\n",
    "\n",
    "* How to split the data set?\n",
    "  - For data sets ($<=60,000$ entries), the following ratios are often chosen:\n",
    "    * $60$/$20$/$20$\n",
    "    * $70$/$15$/$15$\n",
    "  - For large data sets (>$1,000,000$), the training set may take up $99\\%$ or $99.5\\%$ of the data. \n",
    "\n",
    "* *Mention folding*\n",
    "  \n",
    "## 9.2.Bias/Variance:\n",
    "\n",
    "* In <font color=\"orangered\"><b>ML/DEEP LEARNING</b></font>:\n",
    "  - the concepts of <font color=\"green\"><b>bias</b></font> and\n",
    "    <font color=\"green\"><b>variance</b></font> are borrowed from statistics.\n",
    "  - though similar, they are <font color=\"red\"><b>NOT</b></font> the same (see <a href=\"./bias-variance.ipynb\"><b>Bias/Variance in statistics</b></a>).\n",
    "* <font color=\"green\"><b>BIAS</b></font>: the distance between the raw training data and its corresponding estimates/predictions by our model.<br>\n",
    "  - $\\texttt{Bias} \\sim \\displaystyle \\frac{1}{n} \\sum_{i=1}^n (\\widehat{y}_i - y_i)$, for $i \\in$ training data.\n",
    "  - The <font color=\"red\"><b>LARGER THE BIAS</b></font>, the larger the inadequacy of our model. ($\\Rightarrow$ <font color=\"green\"><b>UNDERFITTING</b></font>)\n",
    "* <font color=\"green\"><b>VARIANCE</b></font>: the difference in error between the predicted training data and the\n",
    "    predicted data of a modified data set.<br>Both predictions are based on a **SAME** model.<br>\n",
    "  - In essence, the variance is a measure\n",
    "    of the **sensitivity to fluctuations** in the data set.\n",
    "  - The <font color=\"red\"><b>LARGER THE VARIANCE</b></font>, the larger the gap/error between the predicted data from the training set<br> and the data from\n",
    "    the cross-validation set ($\\Rightarrow$ <font color=\"green\"><b>OVERFITTING</b></font>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a22c51-1d5e-4408-b79f-b818c1716d0e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SZ=21\n",
    "rnd.seed(13)\n",
    "noise = 0.05*rnd.normal(size=SZ)\n",
    "ind_train = [2,8,9,10,11,12,18]\n",
    "ind_test = np.setdiff1d(np.arange(SZ), ind_train)\n",
    "\n",
    "x = np.linspace(0.,2.0,21)\n",
    "y = x**2 -2*x + 1 + noise\n",
    "# Create training and testing set\n",
    "x_train = x[ind_train]\n",
    "y_train = x_train**2 -2*x_train +1 + noise[ind_train]\n",
    "x_test = x[ind_test]\n",
    "y_test = x_test**2 -2*x_test +1 + noise[ind_test]\n",
    "\n",
    "# Fit sel. points to polynomials of a order 1,2 and 4\n",
    "coeffs1 = np.polyfit(x_train, y_train, 1)\n",
    "p1 = np.poly1d(coeffs1)\n",
    "y1 = p1(x)\n",
    "\n",
    "coeffs2 = np.polyfit(x_train, y_train, 2)\n",
    "p2 = np.poly1d(coeffs2)\n",
    "y2 = p2(x)\n",
    "\n",
    "coeffs4 = np.polyfit(x_train, y_train, 4)\n",
    "p4 = np.poly1d(coeffs4)\n",
    "y4 = p4(x)\n",
    "\n",
    "plt.plot(x_train, y_train, '.', label=\"Training data\")\n",
    "plt.plot(x_test, y_test, '+', label=\"Test data\")\n",
    "plt.plot(x, y1, '--', label=\"Poly. deg=1 (\"+r\"$\\mathtt{Underfitting}$\"+\")\")\n",
    "plt.plot(x, y2, '--', label=\"Poly. deg=2\")\n",
    "plt.plot(x, y4, '--', label=\"Poly. deg=4 (\"+r\"$\\mathtt{Overfitting}$\"+\")\" )\n",
    "plt.title(r\"Example of $\\mathtt{Underfitting}$/$\\mathtt{Overfitting}$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93369900-353b-40b6-87e0-53bc8c7c29dd",
   "metadata": {},
   "source": [
    "## 9.3.Hints to develop neural nets \n",
    "\n",
    "* <font color=\"green\"><b>Algorithm</b></font>:<br>\n",
    "  $\\texttt{Start with simple model}$<br>\n",
    "  \n",
    "  $\\texttt{while(Bias == High)}$<br>\n",
    "  $\\texttt{\\{}$<br>\n",
    "     $\\hspace{0.35in}\\texttt{Reduce High Bias}$<br>\n",
    "  $\\texttt{\\}}$<br>\n",
    "\n",
    "  $\\texttt{while(Variance==High)}$<br>\n",
    "  $\\texttt{\\{}$<br>\n",
    "     $\\hspace{0.35in}\\texttt{Reduce High Variance}$<br>\n",
    "  $\\texttt{\\}}$<br>\n",
    "    \n",
    "* How to deal with High Bias and Variance?\n",
    "  - <font color=\"red\"><b>High Bias</b></font> ($\\Rightarrow$ training model needs improvement)\n",
    "    * increase the number of <font color=\"green\"><b>hidden layers</b></font>\n",
    "    * increase the number of <font color=\"green\"><b>units per layer</b></font>\n",
    "    * increase the number of <font color=\"green\"><b>epochs</b></font> \n",
    "    * may have to change <font color=\"green\"><b>NN architecture</b></font>\n",
    "  - <font color=\"red\"><b>High Variance</b></font> (your model does not work well at crosss-validation)\n",
    "    * use <font color=\"green\"><b>regularization</b></font> to reduce overfitting.\n",
    "    * may have to <font color=\"green\"><b>increase the training data</b></font> to have a more versatile model.<br>\n",
    "      <font color=\"orangered\"><b>Note:</b></font><br>\n",
    "      The technique of <a href=\"https://en.wikipedia.org/wiki/Data_augmentation\"><b>data augmentation</b></a> may be an option.<br>\n",
    "      + images: rotation, flipping, cropping, filtering, ...\n",
    "      + audio: adding noise, shifting pitch, (in/de)crease speed, ...\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b95ecc4-4210-4256-a048-05c5719bf8c8",
   "metadata": {},
   "source": [
    "## 9.4.Hyper parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2378ffaf-7616-4082-b5e5-e99e86ac34c3",
   "metadata": {},
   "source": [
    "* Deep Learning has a <font color=\"green\"><b>lot of hyperparameters</b></font>\n",
    "  + learning rate ($\\alpha$), learning rate decay\n",
    "  + momentum terms: $\\beta_1$, $\\beta_2$\n",
    "  + $L$, $n^{[l]}$ for $l \\in \\{1,2, \\ldots, L\\}$\n",
    "  + $m$ (Size of the mini-batch)\n",
    "  + ...\n",
    "* <font color=\"green\"><b>Importance</b></font>: (in descending order)<br>\n",
    "  $\\alpha$ > $\\beta_1$, $\\beta_2$ > $m$ > $n^{[l]}$ > $L$\n",
    "* How to <font color=\"green\"><b>optimize</b></font> the hyper parameters:\n",
    "  - Avoid a grid (curse of combinatorics)\n",
    "  - Choose first random values (cfr. Monte-Carlo vs. trapezoid integration)\n",
    "  - Then focus on smaller areas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96d2fd7-2417-49d8-8088-218a0b97384a",
   "metadata": {},
   "source": [
    "### <font color=\"blue\"><b>Exercise 3:</b></font>\n",
    "- Tinker with the learning rate\n",
    "- Increase the number of layers to 4 and 6 and compare the results.\n",
    "- Change the batch size to $64$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217b73d6-7832-448a-a988-a0546872886b",
   "metadata": {},
   "source": [
    "# 10.Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc4f414-3eff-4871-9f82-59a0527096eb",
   "metadata": {},
   "source": [
    "- **Regularization**<br>\n",
    "  Aim: to deal with <font color=\"red\"><b>overfitting</b></font>/<font color=\"red\"><b>high variance</b></font><br>\n",
    "  There are several approaches:\n",
    "\n",
    "  * **Using an additional regularization term**<br>\n",
    "    Append a <font color=\"green\"><b>regularization term</b></font> ($\\mathcal{R}$) to the Cost function, i.e.<br>\n",
    "    $\\begin{eqnarray}\n",
    "    \\mathcal{C} & := & \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}^{(i)} + \\mathcal{R}\n",
    " \\end{eqnarray}$\n",
    "    - $\\texttt{L}^2$ regularization ($\\texttt{Ridge}$ or $\\texttt{Tikhonov}$ regularization):<br>\n",
    "      $\\begin{eqnarray}\n",
    "        \\mathcal{R} &:= & \\displaystyle \\frac{\\lambda_2}{2m} \\sum_{l=1}^L \\sum_{i=1}^{n^{[l-1]}} \\sum_{j=1}^{n^{[l]}} \\Big(W^{[l]}_{i,j} \\Big)^2 \\nonumber \\\\\n",
    "                    & = & \\displaystyle \\frac{\\lambda_2}{2m} \\sum_{l=1}^L  \\left\\| W^{[l]}\\right\\|^2_F \n",
    "       \\end{eqnarray}$<br>\n",
    "      where $\\left\\| W^{[l]}\\right\\|^2_F$ stands for the <a href=\"https://mathworld.wolfram.com/FrobeniusNorm.html\">$\\texttt{Frobenius}$</a> norm of the matrix $W^{[l]}$.<br>\n",
    "      <font color=\"orangered\"><b>Effect</b></font>: <font color=\"teal\"><b>discourages large weights</b></font><br>\n",
    "      Original paper by *Krogh and Hertz (1991)*:<br>\n",
    "      <a href=\"https://proceedings.neurips.cc/paper_files/paper/1991/file/8eefcfdf5990e441f0fb6f3fad709e21-Paper.pdf\"><b>A Simple Weight Decay Can Improve Generalization</b></a>\n",
    "    - $\\texttt{L}^1$ regularization ($\\texttt{Lasso}$ regularization):<br>\n",
    "      $\\begin{eqnarray}\n",
    "        \\mathcal{R} &:= & \\frac{\\lambda_1}{m} \\sum_{l=1}^L \\sum_{i=1}^{n^{[l-1]}} \\sum_{j=1}^{n^{[l]}} |W^{[l]}_{i,j}|\n",
    "       \\end{eqnarray}$<br>\n",
    "      <font color=\"orangered\"><b>Effect</b></font>: <font color=\"teal\"><b>drives less important weights to zero</b></font> i.e. induces sparsity/feature selection.<br>\n",
    "      Original paper by *Tibshirani (1996)*:<br>\n",
    "      <a href=\"https://arindam.cs.illinois.edu/courses/s14cs598/paper/lasso.pdf\"><b>Regression Shrinkage and Selection via the Lasso</b></a>\n",
    "    - $\\texttt{Elastic}$ net: combination of $\\texttt{L}^1$ and $\\texttt{L}^2$ regularization (see <a href=\"https://www.stat.purdue.edu/~tlzhang/mathstat/ElasticNet.pdf\"><b>here</b></a> for the details).<br>\n",
    "      $\\begin{eqnarray}\n",
    "        \\mathcal{R} &:= & \\frac{\\lambda_1}{m} \\sum_{l=1}^L \\sum_{i=1}^{n^{[l-1]}} \\sum_{j=1}^{n^{[l]}}|W^{[l]}_{i,j}| + \\frac{\\lambda_2}{2m}  \\sum_{l=1}^L \\sum_{i=1}^{n^{[l-1]}} \\sum_{j=1}^{n^{[l]}}(W^{[l]}_{i,j})^2 \n",
    "       \\end{eqnarray}$\n",
    "\n",
    "  * **Dropout**\n",
    "    - How: <font color=\"green\"><b>turn off</b></font> activations <font color=\"green\"><b>randomly</b></font>.\n",
    "    - Original paper by *Srivastava, Hinton, et al.*:<br> <a href=\"https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf\"><b>Dropout: A Simple Way to Prevent Neural Networks from Overfitting</b></font>\n",
    "    - In essence an <font color=\"green\"><b>adaptive</b></font> version of $\\texttt{L}^2$ regularization (reduce weights) \n",
    "    - Algorithm ($\\texttt{Inverted drop-out}$)\n",
    "      + For each layer $l$:\n",
    "        + Set $p^{[l]}$: probability to <font color=\"green\"><b>keep</b></font> activation for layer $l$\n",
    "        + Calculate $A^{[l]}$ (as in the regular/non-dropout case)\n",
    "        + Generate a <font color=\"green\"><b>masking matrix</b></font> $M^{[l]} \\in \\mathbb{R}^{m \\times n^{[l]}}$\n",
    "          * Sample the elements of $M^{[l]}$ from $U(0,1)$ \n",
    "          * $\n",
    "                M^{[l]}_{i,j} = \\begin{cases}\n",
    "                             1, \\hspace{0.25in}\\texttt{for}\\;\\;\\; M^{[l]}_{i,j} \\leq p^{[l]} \\\\\n",
    "                             0, \\hspace{0.25in}\\texttt{for}\\;\\;\\; p^{[l]} \\lt M^{[l]}_{i,j} \n",
    "                \\end{cases}$\n",
    "        + Use $M^{[l]}$ to <font color=\"green\"><b>discard</b></font> some of the activations in $A^{[l]}$:<br>\n",
    "          $\\begin{eqnarray}\n",
    "            A^{[l]} &= & A^{[l]} \\odot M^{[l]}\n",
    "           \\end{eqnarray}$\n",
    "        + <font color=\"green\"><b>Rescale</b></font> $A^{[l]}$ (to correct for the missing activations):<br>\n",
    "          $\\begin{eqnarray}\n",
    "             A^{[l]} & = & \\frac{ A^{[l]}}{p^{[l]}}\n",
    "           \\end{eqnarray}$\n",
    "    - Very popular in the field of computer vision (always a shortage of data $\\Rightarrow$ <font color=\"teal\"><b>overfitting).\n",
    "\n",
    "                \n",
    "- **Batch Normalization**\n",
    "\n",
    "  Above we described the <font color=\"green\"><b>standardization/normalization</b></font> of the inputs to speed up the optimization.<br>    The application of the normalization method to the hidden layers is known as <font color=\"green\"><b>batch normalization</b></font>.\n",
    "\n",
    "  <font color=\"orangered\"><b>Still to be worked further out in the coming days.</b></font>\n",
    "\n",
    "  Source:<br>\n",
    "  <a href=\"https://arxiv.org/pdf/1502.03167\"><b><it>Sergey Ioffe and Christian Szegedy (2015)</it>.<br>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</b></a>\n",
    "  \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3e789b-44c8-4454-a561-a828c855fae1",
   "metadata": {},
   "source": [
    "<font color=\"teal\"><b>Note (for sake of completeness)</b></font><br>\n",
    " <details>\n",
    "   <summary>Click here to expand!</summary>\n",
    "   - If $\\texttt{L}^{1}$, $\\texttt{L}^{2}$ or $\\texttt{elastic net}$ regularization is to be used,<br> \n",
    "     the gradient term needs to be <font color=\"green\"><b>altered</b></font>: <br>\n",
    "\n",
    "     * $\\texttt{L}^{2}$ regularization:<br>\n",
    "       $\\begin{eqnarray}\n",
    "         dW^{[l]} & = & dW^{[l]} \\,+\\,\\frac{\\lambda_2}{m} W^{[l]},\\;\\;\\;\\texttt{for}\\,\\,\\,l \\in \\{1,2,\\ldots,L\\}\n",
    "         \\end{eqnarray}$\n",
    "            \n",
    "     * $\\texttt{L}^{1}$ regularization:<br>\n",
    "           $\\begin{eqnarray}\n",
    "            dW^{[l]} & = & dW^{[l]} \\,+ \\,\\frac{\\lambda_1}{m} \\texttt{sgn}(W^{[l]}),\\;\\;\\;\\texttt{for}\\,\\,\\,l \\in                 \\{1,2,\\ldots,L\\}\n",
    "            \\end{eqnarray}$\n",
    "\n",
    "     * $\\texttt{Elastic}$ net regularization:<br>\n",
    "           $\\begin{eqnarray}\n",
    "             dW^{[l]} & = & dW^{[l]} \\,+ \\,\\frac{1}{m} \\Big [ \\lambda_1\\,\\texttt{sgn}(W^{[l]})\\, + \\,\\lambda_2  W^{[l]}\\Big],\\;\\;\\;\\texttt{for}\\,\\,\\,l \\in                 \\{1,2,\\ldots,L\\}\n",
    "            \\end{eqnarray}\n",
    "           $\n",
    "\n",
    "   - If $\\texttt{dropout}$ is used, the backpropagation step needs<br> to be <font color=\"green\"><b>altered</b></font> in the following way:<br>\n",
    "     * <font color=\"green\"><b>Switch off</b></font> those gradients of which the activations\n",
    "       were switched off during the forward propagation step.<br>\n",
    "       $\\begin{eqnarray}  \n",
    "        dA^{[l]} & = &  dA^{[l]} \\odot M^{[l]},\\;\\;\\;\\texttt{for}\\,\\,\\,l \\in \\{1,2,\\ldots,L\\}\n",
    "        \\end{eqnarray}$\n",
    "     * <font color=\"green\"><b>Rescale</b></font> the gradients<br>\n",
    "       $\\begin{eqnarray}  \n",
    "        dA^{[l]} & = &  \\frac{dA^{[l]}}{p^{[l]}},\\;\\;\\;\\texttt{for}\\,\\,\\,l \\in \\{1,2,\\ldots,L\\}\n",
    "        \\end{eqnarray}$\n",
    "   \n",
    " </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6482ece-ce1f-468a-8a0b-1cd518dd3607",
   "metadata": {},
   "source": [
    "# 11.Debugging\n",
    "The <font color=\"green\"><b>following algorithm</b></font> (based on the <font color=\"teal\"><b>numerical calculation</b></font> of the partial derivatives<br>\n",
    "of the weights and biases w.r.t. the cost function ($\\mathcal{C}$)) should be used iff:<br>\n",
    "- you write your own code\n",
    "- and want to check/debug it.\n",
    "\n",
    "It should <font color=\"red\"><b>NOT</b></font> be used to perform regular training (<font color=\"red\"><b>too expensive</b></font>).<br>\n",
    "For some background on numerical differentiation, please have a look <a href=\"./finitediff.ipynb\"><b>here</b></a>.\n",
    "  \n",
    "\n",
    "$\\texttt{Algorithm}$:\n",
    "- $\\texttt{Flatten your weight and bias vectors into one large vector }$ $\\Xi$:<br>\n",
    "  $\\begin{eqnarray}\n",
    "    \\Xi & := \\begin{pmatrix} w^{[1]}_{1,1} \\\\\n",
    "                            w^{[1]}_{2,1} \\\\\n",
    "                             \\vdots \\\\\n",
    "                            w^{[L]}_{n^{[L-1]},n^{[L]}} \\\\\n",
    "                            b^{[1]}_1 \\\\\n",
    "                            b^{[1]}_2 \\\\\n",
    "                             \\vdots \\\\\n",
    "                             b^{[L]}_{n^{[L]}}\n",
    "            \\end{pmatrix}                           \n",
    "  \\end{eqnarray}$\n",
    "- Let:\n",
    "  * $\\epsilon$ be very small ($10^{-8}, 10^{-7}$)\n",
    "  * $\\texttt{num\\_param} := |\\Xi|$ (length of the vector $|\\Xi|$)\n",
    "- \n",
    "$\\texttt{for(i=1, ..., num\\_param)}$<br>\n",
    "    $\\texttt{do}$<br>\n",
    "       $\\begin{eqnarray}\n",
    "       \\;\\;\\;\\;\\;d\\xi_i & = & \\displaystyle \\frac{\\mathcal{C}(\\xi_1,\\xi_2,\\ldots,\\xi_i+ \\epsilon,\\ldots, \\xi_{|\\Xi|-1},  \\xi_{|\\Xi|}) - \n",
    "                              \\mathcal{C}(\\xi_1,\\xi_2,\\ldots,\\xi_i- \\epsilon,\\ldots, \\xi_{|\\Xi|-1}, \\xi_{|\\Xi|})}{2\\epsilon} \\\\\n",
    "        \\;\\;\\;\\;\\;\\xi_i  & = &   \\xi_i \\, - \\, \\alpha \\, d\\xi_i                    \n",
    "        \\end{eqnarray}$\n",
    "         \n",
    "    $\\texttt{done}$<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723977a6-62c4-4196-8523-3c21e6b70170",
   "metadata": {},
   "source": [
    "# 12.Conclusion\n",
    "\n",
    "* You now have been exposed to the fundamental techniques to treat dense neural nets.<br>\n",
    "  Note that we have neither treated CNNs nor RNNs. To be added in the next stage.\n",
    "* You should be able to implement your own dense neural nets from scratch (I provided you even\n",
    "  with the mathematical background to do so).\n",
    "* If you pursue this route, I recommend to start with NumPy.<br>In the subsequent phase you can go down the\n",
    "  following list:$\\texttt{CuPy}$, $\\texttt{nvmath}$, $\\ldots$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d90fae5-e4c9-4d6e-bf1e-bc8d70731cdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
